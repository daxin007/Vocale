{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = os.getcwd()\n",
    "\n",
    "def getDatafile(file_dir):\n",
    "\n",
    "    test_list = []\n",
    "    test_label = [] \n",
    "    valide_list = []\n",
    "    valide_label = []\n",
    "    train_list = []\n",
    "    train_label = []\n",
    "    #定义存放各类别数据和对应标签的列表，列表名对应你所需要分类的列别名\n",
    "\n",
    "    file_list = []   #liste des files de wav\n",
    "    dic_list = [] # dictionnaire de base des données\n",
    "    path_list=os.listdir(file_dir)\n",
    "\n",
    "    for wav_dir in path_list:\n",
    "        name = wav_dir.split(sep='.')\n",
    "        if(len(name) == 1) :\n",
    "            for wav in os.listdir(file_dir+'/'+wav_dir): \n",
    "                name_wav = wav.split(sep='.')\n",
    "                if(len(name_wav) == 2 and name_wav[1] == 'wav' ):\n",
    "                    file_list.append(wav_dir+'/'+wav)\n",
    "                    if not(wav_dir in dic_list):\n",
    "                        dic_list.append(wav_dir)\n",
    "\n",
    "    print(len(file_list))\n",
    "    print(dic_list)\n",
    "\n",
    "    index = {}\n",
    "    for i in range(len(dic_list)):\n",
    "        index[dic_list[i]] = i\n",
    "    #Creates the reverse table that maps labels names to their index\n",
    "\n",
    "    print(index)\n",
    "    \n",
    "    \n",
    "    #Creates the test_list\n",
    "    for line in open('testing_list.txt','r'):\n",
    "        test_list.append(line.strip('\\n'))\n",
    "\n",
    "    perm = np.random.permutation(len(test_list))\n",
    "    test = list()\n",
    "    for i in range(len(perm)):\n",
    "        test.append(test_list[perm[i]])\n",
    "        label = test_list[perm[i]].strip('\\n').split(sep='/')[0]\n",
    "        test_label.append(index[label])\n",
    "\n",
    "\n",
    "    #Creates the valide_list\n",
    "    for line in open('validation_list.txt','r'):\n",
    "        valide_list.append(line.strip('\\n'))\n",
    "    perm = np.random.permutation(len(valide_list))\n",
    "    valide = list()\n",
    "    for i in range(len(perm)):\n",
    "        valide.append(valide_list[perm[i]])\n",
    "        label = valide_list[perm[i]].strip('\\n').split(sep='/')[0]\n",
    "        valide_label.append(index[label])\n",
    "\n",
    "\n",
    "    #Creates the train_list\n",
    "    for line in file_list:\n",
    "        if not (line in test_list ):\n",
    "            if not (line in valide_list):\n",
    "                train_list.append(line)\n",
    "    perm = np.random.permutation(len(train_list))\n",
    "    train = list()\n",
    "    for i in range(len(perm)):\n",
    "        train.append(train_list[perm[i]])\n",
    "        label = train_list[perm[i]].split(sep='/')[0]\n",
    "        train_label.append(index[label])\n",
    "        \n",
    "        \n",
    "    return index, train, train_label, valide, valide_label, test, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105829\n",
      "['right', 'eight', 'cat', 'tree', 'backward', 'learn', 'bed', 'happy', 'go', 'dog', 'no', 'wow', 'follow', 'nine', 'left', 'stop', 'three', 'sheila', 'one', 'bird', 'zero', 'seven', 'up', 'visual', 'marvin', 'two', 'house', 'down', 'six', 'yes', 'on', 'five', 'forward', 'off', 'four']\n",
      "{'right': 0, 'eight': 1, 'cat': 2, 'tree': 3, 'backward': 4, 'learn': 5, 'bed': 6, 'happy': 7, 'go': 8, 'dog': 9, 'no': 10, 'wow': 11, 'follow': 12, 'nine': 13, 'left': 14, 'stop': 15, 'three': 16, 'sheila': 17, 'one': 18, 'bird': 19, 'zero': 20, 'seven': 21, 'up': 22, 'visual': 23, 'marvin': 24, 'two': 25, 'house': 26, 'down': 27, 'six': 28, 'yes': 29, 'on': 30, 'five': 31, 'forward': 32, 'off': 33, 'four': 34}\n"
     ]
    }
   ],
   "source": [
    "index, train, train_label, valide, valide_label, test, test_label = getDatafile(file_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 happy/890cc926_nohash_0.wav\n",
      "7\n",
      "1000 five/728be664_nohash_0.wav\n",
      "31\n",
      "2000 dog/d98f6043_nohash_1.wav\n",
      "9\n",
      "3000 stop/b959cd0c_nohash_2.wav\n",
      "15\n",
      "4000 two/83c9e7e6_nohash_0.wav\n",
      "25\n",
      "5000 stop/a8688b67_nohash_0.wav\n",
      "15\n",
      "6000 backward/13dce503_nohash_2.wav\n",
      "4\n",
      "7000 one/3402e488_nohash_2.wav\n",
      "18\n",
      "8000 eight/8e05039f_nohash_1.wav\n",
      "1\n",
      "9000 tree/7106d229_nohash_0.wav\n",
      "3\n",
      "10000 zero/332d33b1_nohash_0.wav\n",
      "20\n",
      "11000 bed/ec989d6d_nohash_0.wav\n",
      "6\n",
      "12000 two/2039b9c1_nohash_1.wav\n",
      "25\n",
      "13000 left/3589bc72_nohash_0.wav\n",
      "14\n",
      "14000 learn/02ade946_nohash_1.wav\n",
      "5\n",
      "15000 cat/31601aad_nohash_0.wav\n",
      "2\n",
      "16000 five/617de221_nohash_4.wav\n",
      "31\n",
      "17000 three/499be02e_nohash_1.wav\n",
      "16\n",
      "18000 one/8aa35b0c_nohash_0.wav\n",
      "18\n",
      "19000 five/87d5e978_nohash_1.wav\n",
      "31\n",
      "20000 nine/9e42ae25_nohash_2.wav\n",
      "13\n",
      "21000 down/627c0bec_nohash_2.wav\n",
      "27\n",
      "22000 stop/f42e234b_nohash_0.wav\n",
      "15\n",
      "23000 two/b84f83d2_nohash_0.wav\n",
      "25\n",
      "24000 one/1c84a139_nohash_0.wav\n",
      "18\n",
      "25000 sheila/c50f55b8_nohash_4.wav\n",
      "17\n",
      "26000 six/dae01802_nohash_3.wav\n",
      "28\n",
      "27000 two/dae01802_nohash_2.wav\n",
      "25\n",
      "28000 stop/256c0a05_nohash_0.wav\n",
      "15\n",
      "29000 bird/fad7a69a_nohash_0.wav\n",
      "19\n",
      "30000 zero/31d31fa0_nohash_0.wav\n",
      "20\n",
      "31000 bird/8eb4a1bf_nohash_0.wav\n",
      "19\n",
      "32000 two/f5626af6_nohash_1.wav\n",
      "25\n",
      "33000 cat/f21893dc_nohash_0.wav\n",
      "2\n",
      "34000 sheila/24a3e589_nohash_0.wav\n",
      "17\n",
      "35000 off/a24582a0_nohash_0.wav\n",
      "33\n",
      "36000 down/8a0457c9_nohash_0.wav\n",
      "27\n",
      "37000 bird/80c17118_nohash_0.wav\n",
      "19\n",
      "38000 two/edd8bfe3_nohash_0.wav\n",
      "25\n",
      "39000 five/94403ad3_nohash_0.wav\n",
      "31\n",
      "40000 right/c205e625_nohash_0.wav\n",
      "0\n",
      "41000 bed/8c7c9168_nohash_0.wav\n",
      "6\n",
      "42000 off/6889b21f_nohash_0.wav\n",
      "33\n",
      "43000 up/f174517e_nohash_1.wav\n",
      "22\n",
      "44000 three/da93deb7_nohash_0.wav\n",
      "16\n",
      "45000 two/5769c5ab_nohash_2.wav\n",
      "25\n",
      "46000 zero/3bb68054_nohash_4.wav\n",
      "20\n",
      "47000 one/c86d4fd4_nohash_1.wav\n",
      "18\n",
      "48000 two/bbd0bbd0_nohash_1.wav\n",
      "25\n",
      "49000 follow/235b444f_nohash_0.wav\n",
      "12\n",
      "50000 house/3c1e12cf_nohash_0.wav\n",
      "26\n",
      "51000 up/e9bc5cc2_nohash_4.wav\n",
      "22\n",
      "52000 backward/e4be0cf6_nohash_0.wav\n",
      "4\n",
      "53000 stop/28ed6bc9_nohash_0.wav\n",
      "15\n",
      "54000 two/190821dc_nohash_2.wav\n",
      "25\n",
      "55000 down/3a69f765_nohash_0.wav\n",
      "27\n",
      "56000 eight/b308773d_nohash_0.wav\n",
      "1\n",
      "57000 wow/3f6ba067_nohash_0.wav\n",
      "11\n",
      "58000 off/b69002d4_nohash_3.wav\n",
      "33\n",
      "59000 cat/64220627_nohash_0.wav\n",
      "2\n",
      "60000 one/1bb574f9_nohash_2.wav\n",
      "18\n",
      "61000 bed/7cf14c54_nohash_0.wav\n",
      "6\n",
      "62000 eight/21832144_nohash_4.wav\n",
      "1\n",
      "63000 go/106a6183_nohash_3.wav\n",
      "8\n",
      "64000 go/b2ae3928_nohash_0.wav\n",
      "8\n",
      "65000 seven/f06190c1_nohash_0.wav\n",
      "21\n",
      "66000 up/29229c21_nohash_1.wav\n",
      "22\n",
      "67000 one/aff582a1_nohash_0.wav\n",
      "18\n",
      "68000 no/3f45a0cf_nohash_2.wav\n",
      "10\n",
      "69000 yes/5a5721f8_nohash_3.wav\n",
      "29\n",
      "70000 five/35d1b6ee_nohash_1.wav\n",
      "31\n",
      "71000 off/7d6b4b10_nohash_1.wav\n",
      "33\n",
      "72000 three/9151f184_nohash_0.wav\n",
      "16\n",
      "73000 zero/520b2c17_nohash_0.wav\n",
      "20\n",
      "74000 visual/f0522ff4_nohash_0.wav\n",
      "23\n",
      "75000 up/a16013b7_nohash_0.wav\n",
      "22\n",
      "76000 go/a243fcc2_nohash_0.wav\n",
      "8\n",
      "77000 house/7d86b703_nohash_0.wav\n",
      "26\n",
      "78000 no/66cbe2b3_nohash_1.wav\n",
      "10\n",
      "79000 on/af790082_nohash_2.wav\n",
      "30\n",
      "80000 bed/da76aa58_nohash_0.wav\n",
      "6\n",
      "81000 right/bfb10243_nohash_0.wav\n",
      "0\n",
      "82000 right/b414c653_nohash_0.wav\n",
      "0\n",
      "83000 no/c39703ec_nohash_1.wav\n",
      "10\n",
      "84000 five/3d9bbe2d_nohash_1.wav\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "nn_train = len(train)\n",
    "nx = 128\n",
    "ny = 32\n",
    "dataset  = np.zeros((nn_train,1,nx,ny))\n",
    "\n",
    "i = 0 \n",
    "for file in train :\n",
    "    y, sr = librosa.load(file, sr=None)\n",
    "    melspec = librosa.feature.melspectrogram(y, sr, n_fft=1024, hop_length=512, n_mels=128)\n",
    "    logmelspec = librosa.power_to_db(melspec)\n",
    "    if logmelspec.size < 4096:\n",
    "        t=4096-logmelspec.size\n",
    "        a=np.mean(logmelspec)\n",
    "        z=np.zeros(t)\n",
    "        z.fill(a)\n",
    "#         print(tmp.shape)\n",
    "        logmelspec=(np.append(logmelspec,z))\n",
    "    elif logmelspec.size > 4096:\n",
    "        logmelspec=logmelspec[:4096]\n",
    "        \n",
    "    #print(len(logmelspec))\n",
    "        \n",
    "    if(len(logmelspec) != 128):    \n",
    "        logmelspec = np.reshape(logmelspec, (128, 32)) \n",
    "    \n",
    "    logmelspec = (logmelspec - np.min(logmelspec) )/(np.max(logmelspec)  - np.min(logmelspec))\n",
    "    \n",
    "    logmelspec = logmelspec[newaxis, :, :]\n",
    "    #dataset.append(logmelspec)\n",
    "    dataset[i] = logmelspec\n",
    "    if( i%1000 == 0):\n",
    "            print(i,file)\n",
    "            print(train_label[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 one/c518d1b1_nohash_1.wav\n",
      "18\n",
      "1000 three/94de6a6a_nohash_1.wav\n",
      "16\n",
      "2000 off/91b03183_nohash_3.wav\n",
      "33\n",
      "3000 nine/ea356919_nohash_2.wav\n",
      "13\n",
      "4000 bed/fa446c16_nohash_0.wav\n",
      "6\n",
      "5000 two/893705bb_nohash_7.wav\n",
      "25\n",
      "6000 seven/9a69672b_nohash_2.wav\n",
      "21\n",
      "7000 one/bfd26d6b_nohash_0.wav\n",
      "18\n",
      "8000 six/bfd26d6b_nohash_0.wav\n",
      "28\n",
      "9000 no/bb05582b_nohash_3.wav\n",
      "10\n",
      "10000 nine/1cb788bc_nohash_0.wav\n",
      "13\n",
      "11000 house/370844f7_nohash_1.wav\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "testset = []\n",
    "\n",
    "nn_test = len(test)\n",
    "nx = 128\n",
    "ny = 32\n",
    "testset  = np.zeros((nn_test,1,nx,ny))\n",
    "\n",
    "i = 0 \n",
    "for file in test :\n",
    "    y, sr = librosa.load(file, sr=None)\n",
    "    melspec = librosa.feature.melspectrogram(y, sr, n_fft=1024, hop_length=512, n_mels=128)\n",
    "    logmelspec = librosa.power_to_db(melspec)\n",
    "    if logmelspec.size < 4096:\n",
    "        t=4096-logmelspec.size\n",
    "        a=np.mean(logmelspec)\n",
    "        z=np.zeros(t)\n",
    "        z.fill(a)\n",
    "#         print(tmp.shape)\n",
    "        logmelspec=(np.append(logmelspec,z))\n",
    "    elif logmelspec.size > 4096:\n",
    "        logmelspec=logmelspec[:4096]\n",
    "        \n",
    "    if(len(logmelspec) != 128):    \n",
    "          logmelspec = np.reshape(logmelspec, (128, 32)) \n",
    "    \n",
    "    logmelspec = (logmelspec - np.min(logmelspec) )/(np.max(logmelspec)  - np.min(logmelspec))\n",
    "    #print(logmelspec)\n",
    "    logmelspec = logmelspec[newaxis, :, :]\n",
    "    #dataset.append(logmelspec)\n",
    "    testset[i] = logmelspec\n",
    "    if( i%1000 == 0):\n",
    "            print(i,file)\n",
    "            print(test_label[i])\n",
    "    i+=1\n",
    "    \n",
    "    #if(i>11000): print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable # torch 中 Variable 模块\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "#train_tensor = transform1(dataset)\n",
    "train_tensor = torch.tensor(dataset).float()\n",
    "\n",
    "target_tensor = torch.tensor(train_label)\n",
    "\n",
    "print(type(train_tensor))\n",
    "\n",
    "#print(target_tensor[0])\n",
    "torch_dataset = torch.utils.data.TensorDataset(train_tensor, target_tensor)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(torch_dataset, batch_size=2,\n",
    "                                          shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x1cd9311978>\n"
     ]
    }
   ],
   "source": [
    "test_tensor = torch.tensor(testset).float()\n",
    "target_test = torch.tensor(test_label)\n",
    "torch_testset = torch.utils.data.TensorDataset(test_tensor, target_test)\n",
    "testloader = torch.utils.data.DataLoader(torch_testset, batch_size=2,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "print(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,16,7)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(64 * 13 * 1, 35)\n",
    "        #self.fc2 = nn.Linear(256, 256)\n",
    "        #self.fc3 = nn.Linear(256, 35)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        #print(x[0])\n",
    "        x = x.view(-1, 64 * 13 * 1)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.00001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 20000] loss: 0.731\n",
      "[1, 40000] loss: 0.728\n",
      "[2, 20000] loss: 0.723\n",
      "[2, 40000] loss: 0.720\n",
      "[3, 20000] loss: 0.716\n",
      "[3, 40000] loss: 0.712\n",
      "[4, 20000] loss: 0.710\n",
      "[4, 40000] loss: 0.706\n",
      "[5, 20000] loss: 0.697\n",
      "[5, 40000] loss: 0.708\n",
      "[6, 20000] loss: 0.690\n",
      "[6, 40000] loss: 0.705\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable # torch 中 Variable 模块\n",
    "\n",
    "for epoch in range(6):  # loop over the dataset multiple times  36\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs = Variable(data[0])\n",
    "        labels = Variable(data[1])\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print(outputs)\n",
    "        # _,predicted = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20000 == 19999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20, 25])\n",
      "tensor([20, 25])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "\n",
    "inputs, labels = dataiter.next()\n",
    "\n",
    "print(labels)\n",
    "\n",
    "outputs = net(inputs)\n",
    "\n",
    "_,predicted = torch.max(outputs, 1)\n",
    "\n",
    "print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train --> correct 68364 : \t81%\n"
     ]
    }
   ],
   "source": [
    "corret = 0\n",
    "summ = 0\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    inputs = Variable(data[0])\n",
    "    labels = Variable(data[1])\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    _,predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    if (predicted[0] == labels[0] ) : corret += 1\n",
    "    if(len(predicted) == 2):\n",
    "        if (predicted[1] == labels[1] ) : corret += 1\n",
    "        \n",
    "    summ += 2\n",
    "    \n",
    "print(f\" train --> correct {corret} : \\t{round(corret/summ*100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test --> correct 8527 : \t77%\n",
      "test --> loss  : \t1\n"
     ]
    }
   ],
   "source": [
    "corret = 0\n",
    "summ = 0\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    inputs = Variable(data[0])\n",
    "    labels = Variable(data[1])\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    _,predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    loss = criterion(outputs, labels)\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    if (predicted[0] == labels[0] ) : corret += 1\n",
    "    if(len(predicted) == 2):\n",
    "        if (predicted[1] == labels[1] ) : corret += 1  \n",
    "        \n",
    "    summ += 2\n",
    "    \n",
    "print(f\"test --> correct {corret} : \\t{round(corret/summ*100)}%\")\n",
    "print(f\"test --> loss  : \\t{round(running_loss/i)}\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to model_30.pth. You can run `python evaluate.py --model model_30.pth` to generate the Kaggle formatted csv file\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/python3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model_file = 'model_30.pth'\n",
    "torch.save(Net, model_file)\n",
    "print('Saved model to ' + model_file + '. You can run `python evaluate.py --model ' + model_file + '` to generate the Kaggle formatted csv file\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
