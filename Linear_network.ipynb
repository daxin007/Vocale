{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatafile(file_dir):\n",
    "\n",
    "\ttest_list = []\n",
    "\ttest_label = [] \n",
    "\tvalide_list = []\n",
    "\tvalide_label = []\n",
    "\ttrain_list = []\n",
    "\ttrain_label = []\n",
    "\n",
    "\n",
    "\tfile_list = []   #liste des files de wav\n",
    "\tdic_list = [] # dictionnaire de base des données\n",
    "\tpath_list=os.listdir(file_dir)\n",
    "\n",
    "\tfor wav_dir in path_list:\n",
    "\t\tname = wav_dir.split(sep='.')\n",
    "\t\tif(len(name) == 1) :\n",
    "\t\t\tfor wav in os.listdir(file_dir+'/'+wav_dir): \n",
    "\t\t\t\tname_wav = wav.split(sep='.')\n",
    "\t\t\t\tif(len(name_wav) == 2 and name_wav[1] == 'wav' ):\n",
    "\t\t\t\t\tfile_list.append(wav_dir+'/'+wav)\n",
    "\t\t\t\t\tif not(wav_dir in dic_list):\n",
    "\t\t\t\t\t\tdic_list.append(wav_dir)\n",
    "\n",
    "\tprint(len(file_list))\n",
    "\tprint(dic_list)\n",
    "\n",
    "\tindex = {}\n",
    "\tfor i in range(len(dic_list)):\n",
    "\t\tindex[dic_list[i]] = i\n",
    "\t#Creates the reverse table that maps labels names to their index\t\n",
    "\t\t\n",
    "\tprint(index)\n",
    "\tf=open(\"/home/gjx/Desktop/vocale/data_speech_commands_v0.02/testing_list.txt\",'r')\n",
    "\tfor line in f:\n",
    "\t\ttest_list.append(line.strip('\\n'))\n",
    "\t\tlabel = line.strip('\\n').split(sep='/')[0]\n",
    "\t\ttest_label.append(index[label])\n",
    "\n",
    "\n",
    "\tfor line in open(\"/home/gjx/Desktop/vocale/data_speech_commands_v0.02/validation_list.txt\",'r'):\n",
    "\t\tvalide_list.append(line.strip('\\n'))\n",
    "\t\tlabel = line.strip('\\n').split(sep='/')[0]\n",
    "\t\tvalide_label.append(index[label])\n",
    "\n",
    "\n",
    "\tfor line in file_list:\n",
    "\t\tif not (line in test_list ):\n",
    "\t\t\tif not (line in valide_list):\n",
    "\t\t\t\ttrain_list.append(line)\n",
    "\t\t\t\ttrain_label.append(index[label])\n",
    "\n",
    "\treturn index, train_list, train_label, valide_list, valide_label, test_list, test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105835\n",
      "['down', 'learn', 'left', 'forward', 'on', 'marvin', 'one', 'six', 'visual', 'right', 'sheila', 'yes', 'backward', 'eight', 'up', 'bird', 'go', 'wow', 'no', 'bed', 'seven', 'tree', 'cat', 'nine', 'zero', 'four', 'stop', 'follow', '_background_noise_', 'five', 'off', 'dog', 'house', 'two', 'three', 'happy']\n",
      "{'down': 0, 'learn': 1, 'left': 2, 'forward': 3, 'on': 4, 'marvin': 5, 'one': 6, 'six': 7, 'visual': 8, 'right': 9, 'sheila': 10, 'yes': 11, 'backward': 12, 'eight': 13, 'up': 14, 'bird': 15, 'go': 16, 'wow': 17, 'no': 18, 'bed': 19, 'seven': 20, 'tree': 21, 'cat': 22, 'nine': 23, 'zero': 24, 'four': 25, 'stop': 26, 'follow': 27, '_background_noise_': 28, 'five': 29, 'off': 30, 'dog': 31, 'house': 32, 'two': 33, 'three': 34, 'happy': 35}\n"
     ]
    }
   ],
   "source": [
    "file_dir=\"/home/gjx/Desktop/vocale/data_speech_commands_v0.02\"\n",
    "index, train_list, train_label, valide_list, valide_label, test_list, test_label=getDatafile(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# train_set=[]\n",
    "# start=time.clock()\n",
    "# for i in tqdm(range(len(train_list))):\n",
    "#     y, sr = librosa.load(\"/home/gjx/Desktop/vocale/data_speech_commands_v0.02/\"+train_list[i], sr=None)\n",
    "#     melspec = librosa.feature.melspectrogram(y, sr, n_fft=1024, hop_length=512, n_mels=128)\n",
    "#     logmelspec = librosa.power_to_db(melspec)\n",
    "#     train_set.append(logmelspec)\n",
    "# end=time.clock()\n",
    "# print(end-start)\n",
    "# for i in range(len(train_set)):\n",
    "#     train_set[i]=train_set[i].reshape(-1)\n",
    "\n",
    "# import numpy as np\n",
    "# for i in range(len(train_set)):\n",
    "#     if train_set[i].size < 4096:\n",
    "#         t=4096-train_set[i].size\n",
    "#         a=np.mean(train_set[i])\n",
    "#         z=np.zeros(t)\n",
    "#         z.fill(a)\n",
    "# #         print(tmp.shape)\n",
    "#         train_set[i]=(np.append(train_set[i],z))\n",
    "#     elif train_set[i].size > 4096:\n",
    "#         train_set[i]=train_set[i][:4096]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9981/9981 [03:06<00:00, 53.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1322.801362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "valide_set=[]\n",
    "start=time.clock()\n",
    "for i in tqdm(range(len(valide_list))):\n",
    "    y, sr = librosa.load(\"/home/gjx/Desktop/vocale/data_speech_commands_v0.02/\"+valide_list[i], sr=None)\n",
    "    melspec = librosa.feature.melspectrogram(y, sr, n_fft=1024, hop_length=512, n_mels=128)\n",
    "    logmelspec = librosa.power_to_db(melspec)\n",
    "    valide_set.append(logmelspec)\n",
    "end=time.clock()\n",
    "print(end-start)\n",
    "\n",
    "for i in range(len(valide_set)):\n",
    "    valide_set[i]=valide_set[i].reshape(-1)\n",
    "\n",
    "import numpy as np\n",
    "for i in range(len(valide_set)):\n",
    "    if valide_set[i].size < 4096:\n",
    "        t=4096-valide_set[i].size\n",
    "        a=np.mean(valide_set[i])\n",
    "        z=np.zeros(t)\n",
    "        z.fill(a)\n",
    "#         print(tmp.shape)\n",
    "        valide_set[i]=(np.append(valide_set[i],z))\n",
    "    elif valide_set[i].size > 4096:\n",
    "        valide_set[i]=valide_set[i][:4096]\n",
    "\n",
    "\n",
    "# pca = PCA(n_components=2000)\n",
    "# train = pca.fit_transform(valide_set)\n",
    "# print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11005/11005 [02:45<00:00, 66.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201.566248\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "test_set=[]\n",
    "start=time.clock()\n",
    "for i in tqdm(range(len(test_list))):\n",
    "    y, sr = librosa.load(\"/home/gjx/Desktop/vocale/data_speech_commands_v0.02/\"+test_list[i], sr=None)\n",
    "    melspec = librosa.feature.melspectrogram(y, sr, n_fft=1024, hop_length=512, n_mels=128)\n",
    "    logmelspec = librosa.power_to_db(melspec)\n",
    "    test_set.append(logmelspec)\n",
    "end=time.clock()\n",
    "print(end-start)\n",
    "\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    test_set[i]=test_set[i].reshape(-1)\n",
    "\n",
    "import numpy as np\n",
    "for i in range(len(test_set)):\n",
    "    if test_set[i].size < 4096:\n",
    "        t=4096-test_set[i].size\n",
    "        a=np.mean(test_set[i])\n",
    "        z=np.zeros(t)\n",
    "        z.fill(a)\n",
    "#         print(tmp.shape)\n",
    "        test_set[i]=(np.append(test_set[i],z))\n",
    "    elif test_set[i].size > 4096:\n",
    "        test_set[i]=test_set[i][:4096]\n",
    "\n",
    "# test = pca.fit_transform(test_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valide_set.extend(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=400)\n",
    "data = pca.fit_transform(valide_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:9981]\n",
    "\n",
    "test = data[9981:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "x = torch.Tensor(train)\n",
    "y = torch.Tensor(valide_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.Tensor(test)\n",
    "y_test = torch.Tensor(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim\n",
    "x = Variable(x)\n",
    "y = Variable(y)\n",
    " \n",
    "class Net(nn.Module):\n",
    "    def __init__(self,n_feature,n_hidden1,n_hidden2,n_hidden3,n_hidden4, n_hidden5,n_out):\n",
    "        super(Net,self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_feature,n_hidden1)\n",
    "        self.hidden2 = nn.Linear(n_hidden1,n_hidden2)\n",
    "        self.hidden3 = nn.Linear(n_hidden2,n_hidden3)\n",
    "        self.hidden4 = nn.Linear(n_hidden3,n_hidden4)\n",
    "        self.hidden5 = nn.Linear(n_hidden4,n_hidden5)\n",
    "        self.out = nn.Linear(n_hidden5,n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = F.relu(self.hidden4(x))\n",
    "        x = F.relu(self.hidden5(x))\n",
    "        x = self.out(x)\n",
    "        out = F.log_softmax(x,dim=1)\n",
    "        return out\n",
    " \n",
    " \n",
    "net = Net(n_feature=400,n_hidden1=1500,n_hidden2=1000, n_hidden3=500, n_hidden4=200,n_hidden5=100,n_out=36)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjx/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/gjx/.local/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: tensor(4.4346)\n",
      "train : 0.027752730187355978\n",
      "test : 0.029622898682417083\n",
      "1 loss: tensor(4.3876)\n",
      "train : 0.034365294058711554\n",
      "test : 0.03434802362562472\n",
      "2 loss: tensor(4.3606)\n",
      "train : 0.038272718164512574\n",
      "test : 0.03861880963198546\n",
      "3 loss: tensor(4.3426)\n",
      "train : 0.041078048291754335\n",
      "test : 0.04079963652885052\n",
      "4 loss: tensor(4.3292)\n",
      "train : 0.042180142270313595\n",
      "test : 0.04352567014993185\n",
      "5 loss: tensor(4.3187)\n",
      "train : 0.04288147480212404\n",
      "test : 0.046160835983643796\n",
      "6 loss: tensor(4.3098)\n",
      "train : 0.04528604348261697\n",
      "test : 0.04797819173103135\n",
      "7 loss: tensor(4.3020)\n",
      "train : 0.04819156397154594\n",
      "test : 0.04779645615629259\n",
      "8 loss: tensor(4.2950)\n",
      "train : 0.05159803626891093\n",
      "test : 0.0497955474784189\n",
      "9 loss: tensor(4.2885)\n",
      "train : 0.053702033864342254\n",
      "test : 0.05170377101317583\n",
      "10 loss: tensor(4.2824)\n",
      "train : 0.0545035567578399\n",
      "test : 0.053430258973194006\n",
      "11 loss: tensor(4.2767)\n",
      "train : 0.056607554353271215\n",
      "test : 0.05370286233530214\n",
      "12 loss: tensor(4.2713)\n",
      "train : 0.05841098086364092\n",
      "test : 0.05497501135847342\n",
      "13 loss: tensor(4.2660)\n",
      "train : 0.060414788097385035\n",
      "test : 0.05688323489323035\n",
      "14 loss: tensor(4.2610)\n",
      "train : 0.06231840496944194\n",
      "test : 0.05770104497955475\n",
      "15 loss: tensor(4.2560)\n",
      "train : 0.06362087967137561\n",
      "test : 0.05988187187641981\n",
      "16 loss: tensor(4.2511)\n",
      "train : 0.06712754233042781\n",
      "test : 0.06024534302589732\n",
      "17 loss: tensor(4.2464)\n",
      "train : 0.06963230137260795\n",
      "test : 0.06133575647432985\n",
      "18 loss: tensor(4.2416)\n",
      "train : 0.07113515679791604\n",
      "test : 0.06378918673330304\n",
      "19 loss: tensor(4.2369)\n",
      "train : 0.07363991584009619\n",
      "test : 0.06497046796910495\n",
      "20 loss: tensor(4.2323)\n",
      "train : 0.07504258090371706\n",
      "test : 0.0676056338028169\n",
      "21 loss: tensor(4.2276)\n",
      "train : 0.07744714958420999\n",
      "test : 0.07014993184915948\n",
      "22 loss: tensor(4.2229)\n",
      "train : 0.07874962428614367\n",
      "test : 0.07142208087233076\n",
      "23 loss: tensor(4.2182)\n",
      "train : 0.08155495441338544\n",
      "test : 0.0740572467060427\n",
      "24 loss: tensor(4.2135)\n",
      "train : 0.08335838092375514\n",
      "test : 0.07641980917764653\n",
      "25 loss: tensor(4.2088)\n",
      "train : 0.08506161707243763\n",
      "test : 0.07869150386188097\n",
      "26 loss: tensor(4.2040)\n",
      "train : 0.08716561466786896\n",
      "test : 0.08214447978191732\n",
      "27 loss: tensor(4.1993)\n",
      "train : 0.09017132551848513\n",
      "test : 0.08405270331667423\n",
      "28 loss: tensor(4.1945)\n",
      "train : 0.09177437130548041\n",
      "test : 0.08577919127669241\n",
      "29 loss: tensor(4.1896)\n",
      "train : 0.09437932070934776\n",
      "test : 0.08741481144934121\n",
      "30 loss: tensor(4.1847)\n",
      "train : 0.09778579300671275\n",
      "test : 0.0878691503861881\n",
      "31 loss: tensor(4.1799)\n",
      "train : 0.10079150385732892\n",
      "test : 0.08959563834620626\n",
      "32 loss: tensor(4.1750)\n",
      "train : 0.10419797615469392\n",
      "test : 0.09204906860517946\n",
      "33 loss: tensor(4.1701)\n",
      "train : 0.1075042580903717\n",
      "test : 0.09477510222626079\n",
      "34 loss: tensor(4.1651)\n",
      "train : 0.11000901713255185\n",
      "test : 0.09750113584734212\n",
      "35 loss: tensor(4.1600)\n",
      "train : 0.1129145376214808\n",
      "test : 0.1012267151294866\n",
      "36 loss: tensor(4.1550)\n",
      "train : 0.11612062919547139\n",
      "test : 0.1036801453884598\n",
      "37 loss: tensor(4.1498)\n",
      "train : 0.12052900510970845\n",
      "test : 0.10731485688323489\n",
      "38 loss: tensor(4.1446)\n",
      "train : 0.1231339545135758\n",
      "test : 0.10967741935483871\n",
      "39 loss: tensor(4.1394)\n",
      "train : 0.12603947500250476\n",
      "test : 0.11285779191276693\n",
      "40 loss: tensor(4.1341)\n",
      "train : 0.12914537621480812\n",
      "test : 0.11422080872330759\n",
      "41 loss: tensor(4.1287)\n",
      "train : 0.13305280032060915\n",
      "test : 0.11549295774647887\n",
      "42 loss: tensor(4.1233)\n",
      "train : 0.1372607955114718\n",
      "test : 0.11721944570649705\n",
      "43 loss: tensor(4.1178)\n",
      "train : 0.1409678388938984\n",
      "test : 0.12003634711494775\n",
      "44 loss: tensor(4.1122)\n",
      "train : 0.14407374010620178\n",
      "test : 0.12221717401181281\n",
      "45 loss: tensor(4.1065)\n",
      "train : 0.14758040276525397\n",
      "test : 0.12485233984552477\n",
      "46 loss: tensor(4.1008)\n",
      "train : 0.15048592325418295\n",
      "test : 0.128032712403453\n",
      "47 loss: tensor(4.0950)\n",
      "train : 0.15258992084961426\n",
      "test : 0.13012267151294865\n",
      "48 loss: tensor(4.0891)\n",
      "train : 0.15699829676385132\n",
      "test : 0.1323943661971831\n",
      "49 loss: tensor(4.0831)\n",
      "train : 0.16090572086965235\n",
      "test : 0.1348477964561563\n",
      "50 loss: tensor(4.0770)\n",
      "train : 0.16451257389039176\n",
      "test : 0.13730122671512948\n",
      "51 loss: tensor(4.0708)\n",
      "train : 0.16641619076244865\n",
      "test : 0.13830077237619265\n",
      "52 loss: tensor(4.0646)\n",
      "train : 0.16932171125137763\n",
      "test : 0.14002726033621082\n",
      "53 loss: tensor(4.0582)\n",
      "train : 0.17122532812343452\n",
      "test : 0.14139027714675148\n",
      "54 loss: tensor(4.0518)\n",
      "train : 0.17212704137861937\n",
      "test : 0.14293502953203088\n",
      "55 loss: tensor(4.0452)\n",
      "train : 0.17383027752730187\n",
      "test : 0.1452975920036347\n",
      "56 loss: tensor(4.0385)\n",
      "train : 0.17573389439935877\n",
      "test : 0.14684234438891414\n",
      "57 loss: tensor(4.0317)\n",
      "train : 0.17583408476104598\n",
      "test : 0.14911403907314857\n",
      "58 loss: tensor(4.0248)\n",
      "train : 0.17703636910129245\n",
      "test : 0.15047705588368923\n",
      "59 loss: tensor(4.0179)\n",
      "train : 0.18094379320709347\n",
      "test : 0.1514766015447524\n",
      "60 loss: tensor(4.0108)\n",
      "train : 0.18364893297264803\n",
      "test : 0.15393003180372558\n",
      "61 loss: tensor(4.0036)\n",
      "train : 0.18595331129145376\n",
      "test : 0.1570195365742844\n",
      "62 loss: tensor(3.9963)\n",
      "train : 0.18775673780182348\n",
      "test : 0.15838255338482507\n",
      "63 loss: tensor(3.9890)\n",
      "train : 0.18885883178038274\n",
      "test : 0.1587460245343026\n",
      "64 loss: tensor(3.9815)\n",
      "train : 0.190461877567378\n",
      "test : 0.1599273057701045\n",
      "65 loss: tensor(3.9739)\n",
      "train : 0.1919647329926861\n",
      "test : 0.16129032258064516\n",
      "66 loss: tensor(3.9662)\n",
      "train : 0.1935677787796814\n",
      "test : 0.16165379373012267\n",
      "67 loss: tensor(3.9584)\n",
      "train : 0.19497044384330228\n",
      "test : 0.16238073602907768\n",
      "68 loss: tensor(3.9504)\n",
      "train : 0.19657348963029755\n",
      "test : 0.16319854611540208\n",
      "69 loss: tensor(3.9424)\n",
      "train : 0.19787596433223123\n",
      "test : 0.1648341662880509\n",
      "70 loss: tensor(3.9342)\n",
      "train : 0.19977958120428815\n",
      "test : 0.16646978646069968\n",
      "71 loss: tensor(3.9260)\n",
      "train : 0.20178338843803226\n",
      "test : 0.1675601999091322\n",
      "72 loss: tensor(3.9176)\n",
      "train : 0.20288548241659152\n",
      "test : 0.16928668786915038\n",
      "73 loss: tensor(3.9092)\n",
      "train : 0.2041879571185252\n",
      "test : 0.17101317582916856\n",
      "74 loss: tensor(3.9007)\n",
      "train : 0.20539024145877166\n",
      "test : 0.17310313493866425\n",
      "75 loss: tensor(3.8922)\n",
      "train : 0.20699328724576696\n",
      "test : 0.17419354838709677\n",
      "76 loss: tensor(3.8835)\n",
      "train : 0.20809538122432622\n",
      "test : 0.17555656519763743\n",
      "77 loss: tensor(3.8747)\n",
      "train : 0.20969842701132152\n",
      "test : 0.17837346660608813\n",
      "78 loss: tensor(3.8658)\n",
      "train : 0.21180242460675283\n",
      "test : 0.17946388005452069\n",
      "79 loss: tensor(3.8569)\n",
      "train : 0.21440737401062018\n",
      "test : 0.181462971376647\n",
      "80 loss: tensor(3.8479)\n",
      "train : 0.21601041979761548\n",
      "test : 0.18200817810086325\n",
      "81 loss: tensor(3.8388)\n",
      "train : 0.21691213305280033\n",
      "test : 0.18328032712403453\n",
      "82 loss: tensor(3.8296)\n",
      "train : 0.21881574992485722\n",
      "test : 0.18409813721035892\n",
      "83 loss: tensor(3.8203)\n",
      "train : 0.22071936679691415\n",
      "test : 0.18555202180826896\n",
      "84 loss: tensor(3.8109)\n",
      "train : 0.22282336439234546\n",
      "test : 0.18718764198091775\n",
      "85 loss: tensor(3.8015)\n",
      "train : 0.22452660054102797\n",
      "test : 0.1878237164925034\n",
      "86 loss: tensor(3.7920)\n",
      "train : 0.22663059813645928\n",
      "test : 0.18964107223989096\n",
      "87 loss: tensor(3.7824)\n",
      "train : 0.22833383428514176\n",
      "test : 0.19127669241253975\n",
      "88 loss: tensor(3.7728)\n",
      "train : 0.2308385933273219\n",
      "test : 0.19236710586097228\n",
      "89 loss: tensor(3.7631)\n",
      "train : 0.23294259092275324\n",
      "test : 0.19318491594729667\n",
      "90 loss: tensor(3.7534)\n",
      "train : 0.23484620779481014\n",
      "test : 0.1948205361199455\n",
      "91 loss: tensor(3.7435)\n",
      "train : 0.23674982466686706\n",
      "test : 0.19691049522944115\n",
      "92 loss: tensor(3.7337)\n",
      "train : 0.23855325117723675\n",
      "test : 0.1983643798273512\n",
      "93 loss: tensor(3.7238)\n",
      "train : 0.24135858130447851\n",
      "test : 0.20109041344843254\n",
      "94 loss: tensor(3.7138)\n",
      "train : 0.24336238853822262\n",
      "test : 0.2031803725579282\n",
      "95 loss: tensor(3.7038)\n",
      "train : 0.24616771866546439\n",
      "test : 0.20536119945479328\n",
      "96 loss: tensor(3.6938)\n",
      "train : 0.24797114517583407\n",
      "test : 0.20781462971376646\n",
      "97 loss: tensor(3.6837)\n",
      "train : 0.249874762047891\n",
      "test : 0.2100863243980009\n",
      "98 loss: tensor(3.6736)\n",
      "train : 0.2518785692816351\n",
      "test : 0.21181281235801908\n",
      "99 loss: tensor(3.6634)\n",
      "train : 0.25398256687706644\n",
      "test : 0.21281235801908224\n",
      "100 loss: tensor(3.6532)\n",
      "train : 0.2558861837491233\n",
      "test : 0.2153566560654248\n",
      "101 loss: tensor(3.6430)\n",
      "train : 0.25829075242961624\n",
      "test : 0.21671967287596547\n",
      "102 loss: tensor(3.6327)\n",
      "train : 0.260094178939986\n",
      "test : 0.2189913675601999\n",
      "103 loss: tensor(3.6223)\n",
      "train : 0.2623985572587917\n",
      "test : 0.22153566560654248\n",
      "104 loss: tensor(3.6120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 0.2648031259392846\n",
      "test : 0.22317128577919126\n",
      "105 loss: tensor(3.6016)\n",
      "train : 0.2680092175132752\n",
      "test : 0.225988187187642\n",
      "106 loss: tensor(3.5912)\n",
      "train : 0.2712153090872658\n",
      "test : 0.22844161744661517\n",
      "107 loss: tensor(3.5807)\n",
      "train : 0.2750225428313796\n",
      "test : 0.23053157655611087\n",
      "108 loss: tensor(3.5702)\n",
      "train : 0.2775273018735598\n",
      "test : 0.23307587460245344\n",
      "109 loss: tensor(3.5597)\n",
      "train : 0.28013225127742714\n",
      "test : 0.2351658337119491\n",
      "110 loss: tensor(3.5492)\n",
      "train : 0.28293758140466885\n",
      "test : 0.23680145388459792\n",
      "111 loss: tensor(3.5387)\n",
      "train : 0.28604348261697227\n",
      "test : 0.23743752839618354\n",
      "112 loss: tensor(3.5282)\n",
      "train : 0.2882476705740908\n",
      "test : 0.24161744661517492\n",
      "113 loss: tensor(3.5180)\n",
      "train : 0.28784690912734195\n",
      "test : 0.23961835529304862\n",
      "114 loss: tensor(3.5088)\n",
      "train : 0.28804728985071637\n",
      "test : 0.24361653793730123\n",
      "115 loss: tensor(3.5030)\n",
      "train : 0.2733193066826971\n",
      "test : 0.22644252612448887\n",
      "116 loss: tensor(3.5086)\n",
      "train : 0.23504658851818455\n",
      "test : 0.2100863243980009\n",
      "117 loss: tensor(3.5534)\n",
      "train : 0.18074341248371906\n",
      "test : 0.1530213539300318\n",
      "118 loss: tensor(3.6683)\n",
      "train : 0.15609658350866645\n",
      "test : 0.14420717855520218\n",
      "119 loss: tensor(3.8224)\n",
      "train : 0.2309387836890091\n",
      "test : 0.19045888232621536\n",
      "120 loss: tensor(3.6497)\n",
      "train : 0.2511772367498247\n",
      "test : 0.21435711040436164\n",
      "121 loss: tensor(3.5341)\n",
      "train : 0.28814748021240355\n",
      "test : 0.23343934575193093\n",
      "122 loss: tensor(3.4859)\n",
      "train : 0.2764252078950005\n",
      "test : 0.23961835529304862\n",
      "123 loss: tensor(3.4769)\n",
      "train : 0.2746217813846308\n",
      "test : 0.22407996365288504\n",
      "124 loss: tensor(3.4769)\n",
      "train : 0.25107704638813744\n",
      "test : 0.22280781462971377\n",
      "125 loss: tensor(3.4964)\n",
      "train : 0.249874762047891\n",
      "test : 0.20127214902317128\n",
      "126 loss: tensor(3.5082)\n",
      "train : 0.2253281234345256\n",
      "test : 0.20427078600636076\n",
      "127 loss: tensor(3.5442)\n",
      "train : 0.2602945596633604\n",
      "test : 0.20908677873693776\n",
      "128 loss: tensor(3.4972)\n",
      "train : 0.24787095481414687\n",
      "test : 0.2194457064970468\n",
      "129 loss: tensor(3.4992)\n",
      "train : 0.28444043682997694\n",
      "test : 0.22853248523398456\n",
      "130 loss: tensor(3.4424)\n",
      "train : 0.2712153090872658\n",
      "test : 0.23589277601090414\n",
      "131 loss: tensor(3.4481)\n",
      "train : 0.29385833082857427\n",
      "test : 0.23207632894139027\n",
      "132 loss: tensor(3.4191)\n",
      "train : 0.27011321510870656\n",
      "test : 0.23389368468877783\n",
      "133 loss: tensor(3.4412)\n",
      "train : 0.2921550946798918\n",
      "test : 0.22853248523398456\n",
      "134 loss: tensor(3.4113)\n",
      "train : 0.26640617172627995\n",
      "test : 0.23134938664243526\n",
      "135 loss: tensor(3.4392)\n",
      "train : 0.2937581404668871\n",
      "test : 0.23062244434348023\n",
      "136 loss: tensor(3.3958)\n",
      "train : 0.2732191163210099\n",
      "test : 0.2372557928214448\n",
      "137 loss: tensor(3.4171)\n",
      "train : 0.3028754633804228\n",
      "test : 0.23598364379827352\n",
      "138 loss: tensor(3.3711)\n",
      "train : 0.28313796212804326\n",
      "test : 0.2432530667878237\n",
      "139 loss: tensor(3.3894)\n",
      "train : 0.3078849814647831\n",
      "test : 0.2393457519309405\n",
      "140 loss: tensor(3.3499)\n",
      "train : 0.28914938382927563\n",
      "test : 0.24761472058155384\n",
      "141 loss: tensor(3.3704)\n",
      "train : 0.3096884079751528\n",
      "test : 0.24143571104043615\n",
      "142 loss: tensor(3.3337)\n",
      "train : 0.29185452359483016\n",
      "test : 0.2506133575647433\n",
      "143 loss: tensor(3.3556)\n",
      "train : 0.3113916441238353\n",
      "test : 0.24279872785097684\n",
      "144 loss: tensor(3.3179)\n",
      "train : 0.2937581404668871\n",
      "test : 0.2517946388005452\n",
      "145 loss: tensor(3.3382)\n",
      "train : 0.31399659352770265\n",
      "test : 0.24661517492049068\n",
      "146 loss: tensor(3.3018)\n",
      "train : 0.29866746818956014\n",
      "test : 0.2547932757837347\n",
      "147 loss: tensor(3.3193)\n",
      "train : 0.312694118825769\n",
      "test : 0.24861426624261698\n",
      "148 loss: tensor(3.2901)\n",
      "train : 0.2974651838493137\n",
      "test : 0.25533848250795094\n",
      "149 loss: tensor(3.3061)\n",
      "train : 0.30708345857128544\n",
      "test : 0.2469786460699682\n",
      "150 loss: tensor(3.2877)\n",
      "train : 0.2957619477006312\n",
      "test : 0.2530667878237165\n",
      "151 loss: tensor(3.3006)\n",
      "train : 0.2975653742110009\n",
      "test : 0.24107223989095866\n",
      "152 loss: tensor(3.2927)\n",
      "train : 0.29666366095581603\n",
      "test : 0.24888686960472511\n",
      "153 loss: tensor(3.3012)\n",
      "train : 0.3036769862739204\n",
      "test : 0.2462517037710132\n",
      "154 loss: tensor(3.2833)\n",
      "train : 0.29806632601943694\n",
      "test : 0.24834166288050885\n",
      "155 loss: tensor(3.2938)\n",
      "train : 0.31830477908025245\n",
      "test : 0.2571558382553385\n",
      "156 loss: tensor(3.2549)\n",
      "train : 0.306382126039475\n",
      "test : 0.2518855065879146\n",
      "157 loss: tensor(3.2665)\n",
      "train : 0.32642019837691616\n",
      "test : 0.26533393911858244\n",
      "158 loss: tensor(3.2427)\n",
      "train : 0.31419697425107707\n",
      "test : 0.2587914584279873\n",
      "159 loss: tensor(3.2480)\n",
      "train : 0.3051798416992285\n",
      "test : 0.2517946388005452\n",
      "160 loss: tensor(3.2998)\n",
      "train : 0.28293758140466885\n",
      "test : 0.23144025442980462\n",
      "161 loss: tensor(3.3643)\n",
      "train : 0.2305380222422603\n",
      "test : 0.19127669241253975\n",
      "162 loss: tensor(3.5349)\n",
      "train : 0.2462679090271516\n",
      "test : 0.20345297592003636\n",
      "163 loss: tensor(3.5928)\n",
      "train : 0.31199278629395855\n",
      "test : 0.25642889595638346\n",
      "164 loss: tensor(3.3371)\n",
      "train : 0.3416491333533714\n",
      "test : 0.2811449341208542\n",
      "165 loss: tensor(3.1883)\n",
      "train : 0.35667768760645224\n",
      "test : 0.28577919127669243\n",
      "166 loss: tensor(3.1549)\n",
      "train : 0.35808035267007315\n",
      "test : 0.29695592912312585\n",
      "167 loss: tensor(3.1470)\n",
      "train : 0.3469592225227933\n",
      "test : 0.2726033621081327\n",
      "168 loss: tensor(3.1524)\n",
      "train : 0.3338342851417694\n",
      "test : 0.28032712403452975\n",
      "169 loss: tensor(3.1793)\n",
      "train : 0.3253181043983569\n",
      "test : 0.2599727396637892\n",
      "170 loss: tensor(3.1798)\n",
      "train : 0.32201182246267906\n",
      "test : 0.2685143116765107\n",
      "171 loss: tensor(3.2158)\n",
      "train : 0.3313295260995892\n",
      "test : 0.2611540208995911\n",
      "172 loss: tensor(3.1628)\n",
      "train : 0.3393447550345657\n",
      "test : 0.2793275783734666\n",
      "173 loss: tensor(3.1828)\n",
      "train : 0.34786093577797816\n",
      "test : 0.27596547024079965\n",
      "174 loss: tensor(3.1254)\n",
      "train : 0.3547740707343954\n",
      "test : 0.29095865515674696\n",
      "175 loss: tensor(3.1507)\n",
      "train : 0.3517683598837792\n",
      "test : 0.2813266696955929\n",
      "176 loss: tensor(3.1130)\n",
      "train : 0.34956417192666067\n",
      "test : 0.2806905951840073\n",
      "177 loss: tensor(3.1595)\n",
      "train : 0.3484620779481014\n",
      "test : 0.2796001817355747\n",
      "178 loss: tensor(3.1204)\n",
      "train : 0.3373409478008216\n",
      "test : 0.2670604270786006\n",
      "179 loss: tensor(3.1777)\n",
      "train : 0.3412483719066226\n",
      "test : 0.2766015447523853\n",
      "180 loss: tensor(3.1227)\n",
      "train : 0.34074742009818654\n",
      "test : 0.26215356656065425\n",
      "181 loss: tensor(3.1569)\n",
      "train : 0.33884380322612967\n",
      "test : 0.2754202635165834\n",
      "182 loss: tensor(3.1232)\n",
      "train : 0.3527702635006512\n",
      "test : 0.27314856883234895\n",
      "183 loss: tensor(3.1157)\n",
      "train : 0.3462578899909829\n",
      "test : 0.28123580190822356\n",
      "184 loss: tensor(3.1064)\n",
      "train : 0.3700030057108506\n",
      "test : 0.2838709677419355\n",
      "185 loss: tensor(3.0695)\n",
      "train : 0.3639915840096183\n",
      "test : 0.29659245797364836\n",
      "186 loss: tensor(3.0670)\n",
      "train : 0.3826269912834385\n",
      "test : 0.2905951840072694\n",
      "187 loss: tensor(3.0283)\n",
      "train : 0.37561366596533413\n",
      "test : 0.3076783280327124\n",
      "188 loss: tensor(3.0390)\n",
      "train : 0.3854323214106803\n",
      "test : 0.29577464788732394\n",
      "189 loss: tensor(3.0117)\n",
      "train : 0.3727081454764052\n",
      "test : 0.3048614266242617\n",
      "190 loss: tensor(3.0460)\n",
      "train : 0.37050395751928666\n",
      "test : 0.28841435711040436\n",
      "191 loss: tensor(3.0406)\n",
      "train : 0.34816150686303976\n",
      "test : 0.28641526578827803\n",
      "192 loss: tensor(3.0982)\n",
      "train : 0.3321310489930869\n",
      "test : 0.26778736937755565\n",
      "193 loss: tensor(3.1249)\n",
      "train : 0.33072838392946596\n",
      "test : 0.2626987732848705\n",
      "194 loss: tensor(3.1501)\n",
      "train : 0.34715960324616774\n",
      "test : 0.28305315765561107\n",
      "195 loss: tensor(3.0938)\n",
      "train : 0.37200681294459476\n",
      "test : 0.2917764652430713\n",
      "196 loss: tensor(3.0427)\n",
      "train : 0.38272718164512576\n",
      "test : 0.3072239890958655\n",
      "197 loss: tensor(3.0007)\n",
      "train : 0.37531309488027254\n",
      "test : 0.29141299409359384\n",
      "198 loss: tensor(3.0243)\n",
      "train : 0.3484620779481014\n",
      "test : 0.2865970013630168\n",
      "199 loss: tensor(3.0752)\n",
      "train : 0.3315299068229636\n",
      "test : 0.2598818718764198\n",
      "200 loss: tensor(3.1726)\n",
      "train : 0.33894399358781685\n",
      "test : 0.2796001817355747\n",
      "201 loss: tensor(3.0941)\n",
      "train : 0.36178739605249977\n",
      "test : 0.28359836437982733\n",
      "202 loss: tensor(3.0566)\n",
      "train : 0.4063721070033063\n",
      "test : 0.32748750567923673\n",
      "203 loss: tensor(2.9456)\n",
      "train : 0.4118825768961026\n",
      "test : 0.32085415720127214\n",
      "204 loss: tensor(2.9093)\n",
      "train : 0.4314196974251077\n",
      "test : 0.34547932757837346\n",
      "205 loss: tensor(2.8751)\n",
      "train : 0.4204989480012023\n",
      "test : 0.32866878691503865\n",
      "206 loss: tensor(2.8753)\n",
      "train : 0.4300170323614868\n",
      "test : 0.3462971376646979\n",
      "207 loss: tensor(2.8668)\n",
      "train : 0.41609057208696526\n",
      "test : 0.3214902317128578\n",
      "208 loss: tensor(2.8860)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 0.41959723474601746\n",
      "test : 0.3355747387551113\n",
      "209 loss: tensor(2.8841)\n",
      "train : 0.40446849013124936\n",
      "test : 0.3145842798727851\n",
      "210 loss: tensor(2.9074)\n",
      "train : 0.4142871455765955\n",
      "test : 0.3306678782371649\n",
      "211 loss: tensor(2.8952)\n",
      "train : 0.4083759142370504\n",
      "test : 0.31385733757383005\n",
      "212 loss: tensor(2.8973)\n",
      "train : 0.4206993287245767\n",
      "test : 0.3323034984098137\n",
      "213 loss: tensor(2.8780)\n",
      "train : 0.4175934275122733\n",
      "test : 0.32085415720127214\n",
      "214 loss: tensor(2.8736)\n",
      "train : 0.41458771666165717\n",
      "test : 0.3283961835529305\n",
      "215 loss: tensor(2.8811)\n",
      "train : 0.40416791904618776\n",
      "test : 0.3144025442980463\n",
      "216 loss: tensor(2.8991)\n",
      "train : 0.37110509968940986\n",
      "test : 0.2964107223989096\n",
      "217 loss: tensor(2.9845)\n",
      "train : 0.3631900611161206\n",
      "test : 0.2876874148114493\n",
      "218 loss: tensor(3.0167)\n",
      "train : 0.35337140567077446\n",
      "test : 0.2745115856428896\n",
      "219 loss: tensor(3.0385)\n",
      "train : 0.37601442741208296\n",
      "test : 0.3064970467969105\n",
      "220 loss: tensor(2.9728)\n",
      "train : 0.42490732391543934\n",
      "test : 0.32757837346660607\n",
      "221 loss: tensor(2.8579)\n",
      "train : 0.43061817453161005\n",
      "test : 0.33830077237619266\n",
      "222 loss: tensor(2.8131)\n",
      "train : 0.4417393046788899\n",
      "test : 0.3457519309404816\n",
      "223 loss: tensor(2.7998)\n",
      "train : 0.42320408776675683\n",
      "test : 0.3315765561108587\n",
      "224 loss: tensor(2.8131)\n",
      "train : 0.4017633503656948\n",
      "test : 0.32457973648341665\n",
      "225 loss: tensor(2.8717)\n",
      "train : 0.38352870453862337\n",
      "test : 0.29986369831894594\n",
      "226 loss: tensor(2.9030)\n",
      "train : 0.3729085261997796\n",
      "test : 0.3087687414811449\n",
      "227 loss: tensor(2.9492)\n",
      "train : 0.3941488828774672\n",
      "test : 0.3000454338936847\n",
      "228 loss: tensor(2.8973)\n",
      "train : 0.41809437932070936\n",
      "test : 0.34066333484779643\n",
      "229 loss: tensor(2.8347)\n",
      "train : 0.42891493838292755\n",
      "test : 0.3243980009086779\n",
      "230 loss: tensor(2.7979)\n",
      "train : 0.4362288347860936\n",
      "test : 0.347296683325761\n",
      "231 loss: tensor(2.7877)\n",
      "train : 0.4317202685101693\n",
      "test : 0.3265788278055429\n",
      "232 loss: tensor(2.7789)\n",
      "train : 0.43462578899909826\n",
      "test : 0.35038618809631983\n",
      "233 loss: tensor(2.7701)\n",
      "train : 0.43632902514778077\n",
      "test : 0.3312130849613812\n",
      "234 loss: tensor(2.7610)\n",
      "train : 0.4366295962328424\n",
      "test : 0.35474784189005\n",
      "235 loss: tensor(2.7433)\n",
      "train : 0.43783188057308886\n",
      "test : 0.33421172194457066\n",
      "236 loss: tensor(2.7511)\n",
      "train : 0.4275122733193067\n",
      "test : 0.34711494775102225\n",
      "237 loss: tensor(2.7390)\n",
      "train : 0.4226029455966336\n",
      "test : 0.3213993639254884\n",
      "238 loss: tensor(2.7939)\n",
      "train : 0.41568981064021643\n",
      "test : 0.3323943661971831\n",
      "239 loss: tensor(2.7807)\n",
      "train : 0.4050696323013726\n",
      "test : 0.30795093139482055\n",
      "240 loss: tensor(2.8659)\n",
      "train : 0.4075743913435528\n",
      "test : 0.3224897773739209\n",
      "241 loss: tensor(2.8392)\n",
      "train : 0.41809437932070936\n",
      "test : 0.3213993639254884\n",
      "242 loss: tensor(2.8349)\n",
      "train : 0.4171926660655245\n",
      "test : 0.32321671967287596\n",
      "243 loss: tensor(2.8292)\n",
      "train : 0.44825167818855827\n",
      "test : 0.346569741026806\n",
      "244 loss: tensor(2.7648)\n",
      "train : 0.4456467287846909\n",
      "test : 0.3416628805088596\n",
      "245 loss: tensor(2.7379)\n",
      "train : 0.47079450956817953\n",
      "test : 0.3651976374375284\n",
      "246 loss: tensor(2.7021)\n",
      "train : 0.45706843001703235\n",
      "test : 0.34438891412994094\n",
      "247 loss: tensor(2.6969)\n",
      "train : 0.4670874661857529\n",
      "test : 0.3654702407996365\n",
      "248 loss: tensor(2.7004)\n",
      "train : 0.4494539625288047\n",
      "test : 0.34020899591094955\n",
      "249 loss: tensor(2.7080)\n",
      "train : 0.4522592926560465\n",
      "test : 0.3545661063153112\n",
      "250 loss: tensor(2.7361)\n",
      "train : 0.4379320709347761\n",
      "test : 0.33166742389822806\n",
      "251 loss: tensor(2.7476)\n",
      "train : 0.43612864442440635\n",
      "test : 0.34393457519309406\n",
      "252 loss: tensor(2.7809)\n",
      "train : 0.42340446849013125\n",
      "test : 0.325397546569741\n",
      "253 loss: tensor(2.7997)\n",
      "train : 0.42160104197976156\n",
      "test : 0.3365742844161745\n",
      "254 loss: tensor(2.8154)\n",
      "train : 0.4439434926360084\n",
      "test : 0.3393003180372558\n",
      "255 loss: tensor(2.7594)\n",
      "train : 0.44604749023143975\n",
      "test : 0.34847796456156294\n",
      "256 loss: tensor(2.7377)\n",
      "train : 0.4683899408876866\n",
      "test : 0.3545661063153112\n",
      "257 loss: tensor(2.6887)\n",
      "train : 0.46027452159102294\n",
      "test : 0.3550204452521581\n",
      "258 loss: tensor(2.6904)\n",
      "train : 0.4626790902715159\n",
      "test : 0.35274875056792365\n",
      "259 loss: tensor(2.6825)\n",
      "train : 0.45306081554954414\n",
      "test : 0.3518400726942299\n",
      "260 loss: tensor(2.6853)\n",
      "train : 0.46247870954814146\n",
      "test : 0.35847342117219444\n",
      "261 loss: tensor(2.6695)\n",
      "train : 0.4722973649934876\n",
      "test : 0.364379827351204\n",
      "262 loss: tensor(2.6330)\n",
      "train : 0.4811141168219617\n",
      "test : 0.3739209450249886\n",
      "263 loss: tensor(2.6126)\n",
      "train : 0.4853221120128244\n",
      "test : 0.37319400272603365\n",
      "264 loss: tensor(2.5898)\n",
      "train : 0.48371906622582905\n",
      "test : 0.37264879600181733\n",
      "265 loss: tensor(2.5967)\n",
      "train : 0.4773068830778479\n",
      "test : 0.36910495229441165\n",
      "266 loss: tensor(2.5906)\n",
      "train : 0.46558461076044483\n",
      "test : 0.3627442071785552\n",
      "267 loss: tensor(2.6258)\n",
      "train : 0.46408175533513674\n",
      "test : 0.3611085870059064\n",
      "268 loss: tensor(2.6253)\n",
      "train : 0.4484520589119327\n",
      "test : 0.34875056792367104\n",
      "269 loss: tensor(2.6605)\n",
      "train : 0.45977356978258693\n",
      "test : 0.3600181735574739\n",
      "270 loss: tensor(2.6576)\n",
      "train : 0.46057509267608454\n",
      "test : 0.35538391640163564\n",
      "271 loss: tensor(2.6336)\n",
      "train : 0.4716962228233644\n",
      "test : 0.36619718309859156\n",
      "272 loss: tensor(2.6507)\n",
      "train : 0.4753030758441038\n",
      "test : 0.36365288505224896\n",
      "273 loss: tensor(2.5937)\n",
      "train : 0.48391944694920347\n",
      "test : 0.3757383007723762\n",
      "274 loss: tensor(2.6139)\n",
      "train : 0.48502154092776273\n",
      "test : 0.3689232167196729\n",
      "275 loss: tensor(2.5734)\n",
      "train : 0.49063220118224626\n",
      "test : 0.38173557473875513\n",
      "276 loss: tensor(2.5819)\n",
      "train : 0.48672477707644524\n",
      "test : 0.3657428441617447\n",
      "277 loss: tensor(2.5720)\n",
      "train : 0.4841198276725779\n",
      "test : 0.3810995002271695\n",
      "278 loss: tensor(2.5768)\n",
      "train : 0.4781084059713456\n",
      "test : 0.3531122217174012\n",
      "279 loss: tensor(2.6019)\n",
      "train : 0.46548442039875765\n",
      "test : 0.3709223080417992\n",
      "280 loss: tensor(2.5986)\n",
      "train : 0.46207794810139263\n",
      "test : 0.3436619718309859\n",
      "281 loss: tensor(2.6595)\n",
      "train : 0.457369001102094\n",
      "test : 0.3654702407996365\n",
      "282 loss: tensor(2.6133)\n",
      "train : 0.4718966035467388\n",
      "test : 0.352112676056338\n",
      "283 loss: tensor(2.6598)\n",
      "train : 0.46558461076044483\n",
      "test : 0.37219445706497045\n",
      "284 loss: tensor(2.5953)\n",
      "train : 0.48351868550245464\n",
      "test : 0.3579282144479782\n",
      "285 loss: tensor(2.6041)\n",
      "train : 0.468590321611061\n",
      "test : 0.37383007723761924\n",
      "286 loss: tensor(2.6010)\n",
      "train : 0.4715960324616772\n",
      "test : 0.3489323034984098\n",
      "287 loss: tensor(2.6280)\n",
      "train : 0.469091273419497\n",
      "test : 0.3672875965470241\n",
      "288 loss: tensor(2.6045)\n",
      "train : 0.49594229035166815\n",
      "test : 0.3676510676965016\n",
      "289 loss: tensor(2.5733)\n",
      "train : 0.5067628494138864\n",
      "test : 0.39373012267151297\n",
      "290 loss: tensor(2.5013)\n",
      "train : 0.5318104398356878\n",
      "test : 0.3924579736483417\n",
      "291 loss: tensor(2.4676)\n",
      "train : 0.5322112012824366\n",
      "test : 0.4097228532485234\n",
      "292 loss: tensor(2.4328)\n",
      "train : 0.5369201482817353\n",
      "test : 0.39454793275783734\n",
      "293 loss: tensor(2.4348)\n",
      "train : 0.5279030157298867\n",
      "test : 0.4032712403452976\n",
      "294 loss: tensor(2.4297)\n",
      "train : 0.5200881675182848\n",
      "test : 0.38855065879145845\n",
      "295 loss: tensor(2.4573)\n",
      "train : 0.5108706542430618\n",
      "test : 0.39036801453884595\n",
      "296 loss: tensor(2.4637)\n",
      "train : 0.4975453361386635\n",
      "test : 0.37346660608814175\n",
      "297 loss: tensor(2.5047)\n",
      "train : 0.49944895301072034\n",
      "test : 0.3741935483870968\n",
      "298 loss: tensor(2.4980)\n",
      "train : 0.4920348662458672\n",
      "test : 0.3730122671512949\n",
      "299 loss: tensor(2.5241)\n",
      "train : 0.49243562769261595\n",
      "test : 0.3687414811449341\n",
      "300 loss: tensor(2.5095)\n",
      "train : 0.49413886384129846\n",
      "test : 0.3794638800545207\n",
      "301 loss: tensor(2.5300)\n",
      "train : 0.4937381023945496\n",
      "test : 0.36746933212176286\n",
      "302 loss: tensor(2.5215)\n",
      "train : 0.5030558060314598\n",
      "test : 0.391094956837801\n",
      "303 loss: tensor(2.5194)\n",
      "train : 0.5033563771165214\n",
      "test : 0.3771013175829169\n",
      "304 loss: tensor(2.4931)\n",
      "train : 0.5122733193066827\n",
      "test : 0.39854611540208995\n",
      "305 loss: tensor(2.4890)\n",
      "train : 0.5000500951808436\n",
      "test : 0.3717401181281236\n",
      "306 loss: tensor(2.5067)\n",
      "train : 0.5040577096483319\n",
      "test : 0.3898228078146297\n",
      "307 loss: tensor(2.5144)\n",
      "train : 0.4982466686704739\n",
      "test : 0.37146751476601547\n",
      "308 loss: tensor(2.5180)\n",
      "train : 0.5126740807534316\n",
      "test : 0.39327578373466604\n",
      "309 loss: tensor(2.4989)\n",
      "train : 0.525097685602645\n",
      "test : 0.38973194002726036\n",
      "310 loss: tensor(2.4393)\n",
      "train : 0.5388237651537922\n",
      "test : 0.4105406633348478\n",
      "311 loss: tensor(2.4243)\n",
      "train : 0.544534615769963\n",
      "test : 0.40536119945479326\n",
      "312 loss: tensor(2.3847)\n",
      "train : 0.5417292856427212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.40917764652430716\n",
      "313 loss: tensor(2.3975)\n",
      "train : 0.5289049193467589\n",
      "test : 0.3983643798273512\n",
      "314 loss: tensor(2.4082)\n",
      "train : 0.5096683699028154\n",
      "test : 0.39154929577464787\n",
      "315 loss: tensor(2.4462)\n",
      "train : 0.4950405770964833\n",
      "test : 0.3749204906860518\n",
      "316 loss: tensor(2.5145)\n",
      "train : 0.5107704638813746\n",
      "test : 0.38691503861880966\n",
      "317 loss: tensor(2.4626)\n",
      "train : 0.5223925458370905\n",
      "test : 0.39945479327578376\n",
      "318 loss: tensor(2.4377)\n",
      "train : 0.5469391844504559\n",
      "test : 0.41090413448432533\n",
      "319 loss: tensor(2.3773)\n",
      "train : 0.559663360384731\n",
      "test : 0.4228078146297138\n",
      "320 loss: tensor(2.3358)\n",
      "train : 0.569882777276826\n",
      "test : 0.421172194457065\n",
      "321 loss: tensor(2.3207)\n",
      "train : 0.5648732591924657\n",
      "test : 0.4253521126760563\n",
      "322 loss: tensor(2.3197)\n",
      "train : 0.5600641218314798\n",
      "test : 0.41890049977283056\n",
      "323 loss: tensor(2.3308)\n",
      "train : 0.5531509868750626\n",
      "test : 0.41490231712857795\n",
      "324 loss: tensor(2.3576)\n",
      "train : 0.5355174832181144\n",
      "test : 0.4093593820990459\n",
      "325 loss: tensor(2.3779)\n",
      "train : 0.5344153892395551\n",
      "test : 0.4028169014084507\n",
      "326 loss: tensor(2.4183)\n",
      "train : 0.521190261496844\n",
      "test : 0.40118128123580193\n",
      "327 loss: tensor(2.4124)\n",
      "train : 0.5336138663460576\n",
      "test : 0.40145388459791004\n",
      "328 loss: tensor(2.4431)\n",
      "train : 0.5324115820058111\n",
      "test : 0.4057246706042708\n",
      "329 loss: tensor(2.3951)\n",
      "train : 0.5476405169822663\n",
      "test : 0.4045433893684689\n",
      "330 loss: tensor(2.4222)\n",
      "train : 0.5412283338342851\n",
      "test : 0.4118128123580191\n",
      "331 loss: tensor(2.3681)\n",
      "train : 0.5551547941088067\n",
      "test : 0.41117673784643344\n",
      "332 loss: tensor(2.3823)\n",
      "train : 0.5464382326420198\n",
      "test : 0.4189913675601999\n",
      "333 loss: tensor(2.3552)\n",
      "train : 0.5495441338543232\n",
      "test : 0.4088141753748296\n",
      "334 loss: tensor(2.3785)\n",
      "train : 0.5282035868149484\n",
      "test : 0.4006360745115856\n",
      "335 loss: tensor(2.4024)\n",
      "train : 0.5105700831580002\n",
      "test : 0.3841890049977283\n",
      "336 loss: tensor(2.4557)\n",
      "train : 0.4804127842901513\n",
      "test : 0.3551113130395275\n",
      "337 loss: tensor(2.5578)\n",
      "train : 0.46197775773970545\n",
      "test : 0.3598364379827351\n",
      "338 loss: tensor(2.6041)\n",
      "train : 0.47720669271616073\n",
      "test : 0.3538391640163562\n",
      "339 loss: tensor(2.6101)\n",
      "train : 0.5121731289449954\n",
      "test : 0.39691049522944116\n",
      "340 loss: tensor(2.4287)\n",
      "train : 0.5506462278328825\n",
      "test : 0.4049068605179464\n",
      "341 loss: tensor(2.3564)\n",
      "train : 0.5597635507464181\n",
      "test : 0.4272603362108133\n",
      "342 loss: tensor(2.2865)\n",
      "train : 0.5633704037671576\n",
      "test : 0.410358927760109\n",
      "343 loss: tensor(2.3101)\n",
      "train : 0.5587616471295461\n",
      "test : 0.42362562471603815\n",
      "344 loss: tensor(2.2850)\n",
      "train : 0.5566576495341148\n",
      "test : 0.4076328941390277\n",
      "345 loss: tensor(2.3199)\n",
      "train : 0.5569582206191764\n",
      "test : 0.4256247160381645\n",
      "346 loss: tensor(2.2878)\n",
      "train : 0.5673780182346458\n",
      "test : 0.41508405270331666\n",
      "347 loss: tensor(2.2962)\n",
      "train : 0.5718865845105701\n",
      "test : 0.43425715583825536\n",
      "348 loss: tensor(2.2535)\n",
      "train : 0.5818054303176035\n",
      "test : 0.4268059972739664\n",
      "349 loss: tensor(2.2514)\n",
      "train : 0.5803025748922953\n",
      "test : 0.4419809177646524\n",
      "350 loss: tensor(2.2274)\n",
      "train : 0.5866145676785893\n",
      "test : 0.4283507496592458\n",
      "351 loss: tensor(2.2356)\n",
      "train : 0.5782987676585513\n",
      "test : 0.4441617446615175\n",
      "352 loss: tensor(2.2239)\n",
      "train : 0.583508666466286\n",
      "test : 0.425988187187642\n",
      "353 loss: tensor(2.2377)\n",
      "train : 0.5747921049994991\n",
      "test : 0.4440708768741481\n",
      "354 loss: tensor(2.2277)\n",
      "train : 0.5787997194669873\n",
      "test : 0.4241708314402544\n",
      "355 loss: tensor(2.2457)\n",
      "train : 0.5715860134255085\n",
      "test : 0.4392548841435711\n",
      "356 loss: tensor(2.2371)\n",
      "train : 0.5670774471495842\n",
      "test : 0.418537028623353\n",
      "357 loss: tensor(2.2680)\n",
      "train : 0.5554553651938684\n",
      "test : 0.41771921853702865\n",
      "358 loss: tensor(2.2788)\n",
      "train : 0.5366195771966736\n",
      "test : 0.40835983643798274\n",
      "359 loss: tensor(2.3534)\n",
      "train : 0.5408275723875363\n",
      "test : 0.3950022716946842\n",
      "360 loss: tensor(2.3334)\n",
      "train : 0.5302073940486925\n",
      "test : 0.407451158564289\n",
      "361 loss: tensor(2.3896)\n",
      "train : 0.5694820158300772\n",
      "test : 0.41244888686960474\n",
      "362 loss: tensor(2.2680)\n",
      "train : 0.5861136158701533\n",
      "test : 0.4422535211267606\n",
      "363 loss: tensor(2.2225)\n",
      "train : 0.6047490231439735\n",
      "test : 0.43543843707405727\n",
      "364 loss: tensor(2.1750)\n",
      "train : 0.6065524496543433\n",
      "test : 0.4547932757837347\n",
      "365 loss: tensor(2.1710)\n",
      "train : 0.5993387436128644\n",
      "test : 0.4352567014993185\n",
      "366 loss: tensor(2.1689)\n",
      "train : 0.5925257990181344\n",
      "test : 0.4449795547478419\n",
      "367 loss: tensor(2.1952)\n",
      "train : 0.5677787796813947\n",
      "test : 0.41653793730122673\n",
      "368 loss: tensor(2.2297)\n",
      "train : 0.5677787796813947\n",
      "test : 0.4275329395729214\n",
      "369 loss: tensor(2.2651)\n",
      "train : 0.5457369001102094\n",
      "test : 0.40099954566106316\n",
      "370 loss: tensor(2.3061)\n",
      "train : 0.5682797314898307\n",
      "test : 0.4258064516129032\n",
      "371 loss: tensor(2.2867)\n",
      "train : 0.5614667868951007\n",
      "test : 0.41272149023171284\n",
      "372 loss: tensor(2.2657)\n",
      "train : 0.5863139965935277\n",
      "test : 0.4388914129940936\n",
      "373 loss: tensor(2.2425)\n",
      "train : 0.5917242761246368\n",
      "test : 0.4238073602907769\n",
      "374 loss: tensor(2.2093)\n",
      "train : 0.6016431219316701\n",
      "test : 0.44834166288050886\n",
      "375 loss: tensor(2.2026)\n",
      "train : 0.6051497845907223\n",
      "test : 0.4324398000908678\n",
      "376 loss: tensor(2.1758)\n",
      "train : 0.6069532111010921\n",
      "test : 0.4508859609268514\n",
      "377 loss: tensor(2.1805)\n",
      "train : 0.6086564472497745\n",
      "test : 0.43734666060881416\n",
      "378 loss: tensor(2.1620)\n",
      "train : 0.6028454062719166\n",
      "test : 0.44679691049522946\n",
      "379 loss: tensor(2.1754)\n",
      "train : 0.6019436930167318\n",
      "test : 0.43716492503407545\n",
      "380 loss: tensor(2.1688)\n",
      "train : 0.5862138062318405\n",
      "test : 0.4309859154929577\n",
      "381 loss: tensor(2.2040)\n",
      "train : 0.5723875363190061\n",
      "test : 0.42344388914129943\n",
      "382 loss: tensor(2.2306)\n",
      "train : 0.5435327121530908\n",
      "test : 0.40018173557473874\n",
      "383 loss: tensor(2.3081)\n",
      "train : 0.5351167217713656\n",
      "test : 0.403362108132667\n",
      "384 loss: tensor(2.3431)\n",
      "train : 0.5287045386233844\n",
      "test : 0.387005906406179\n",
      "385 loss: tensor(2.3687)\n",
      "train : 0.5517483218114417\n",
      "test : 0.41381190368014537\n",
      "386 loss: tensor(2.2869)\n",
      "train : 0.5670774471495842\n",
      "test : 0.4182644252612449\n",
      "387 loss: tensor(2.2655)\n",
      "train : 0.5859132351467788\n",
      "test : 0.4407087687414811\n",
      "388 loss: tensor(2.1761)\n",
      "train : 0.6058511171225328\n",
      "test : 0.4477055883689232\n",
      "389 loss: tensor(2.1606)\n",
      "train : 0.6170724376314999\n",
      "test : 0.453430258973194\n",
      "390 loss: tensor(2.1158)\n",
      "train : 0.608856827973149\n",
      "test : 0.4460699681962744\n",
      "391 loss: tensor(2.1406)\n",
      "train : 0.6070534014627793\n",
      "test : 0.44434348023625625\n",
      "392 loss: tensor(2.1304)\n",
      "train : 0.5976355074641819\n",
      "test : 0.43543843707405727\n",
      "393 loss: tensor(2.1625)\n",
      "train : 0.5982366496343052\n",
      "test : 0.4407996365288505\n",
      "394 loss: tensor(2.1594)\n",
      "train : 0.6035467388037271\n",
      "test : 0.43898228078146295\n",
      "395 loss: tensor(2.1445)\n",
      "train : 0.6186754834184951\n",
      "test : 0.45288505224897774\n",
      "396 loss: tensor(2.1115)\n",
      "train : 0.6269912834385332\n",
      "test : 0.4550658791458428\n",
      "397 loss: tensor(2.0826)\n",
      "train : 0.6294960424807133\n",
      "test : 0.4602453430258973\n",
      "398 loss: tensor(2.0784)\n",
      "train : 0.6233844304177938\n",
      "test : 0.45606542480690593\n",
      "399 loss: tensor(2.0794)\n",
      "train : 0.5997395050596133\n",
      "test : 0.44116310767832806\n",
      "400 loss: tensor(2.1401)\n",
      "train : 0.5775974351267408\n",
      "test : 0.4296228986824171\n",
      "401 loss: tensor(2.1870)\n",
      "train : 0.5495441338543232\n",
      "test : 0.4076328941390277\n",
      "402 loss: tensor(2.3075)\n",
      "train : 0.5649734495541529\n",
      "test : 0.4225352112676056\n",
      "403 loss: tensor(2.2467)\n",
      "train : 0.5964332231239354\n",
      "test : 0.434166288050886\n",
      "404 loss: tensor(2.1780)\n",
      "train : 0.6317002304378319\n",
      "test : 0.46260790549750114\n",
      "405 loss: tensor(2.0758)\n",
      "train : 0.6464282136058511\n",
      "test : 0.4692412539754657\n",
      "406 loss: tensor(2.0336)\n",
      "train : 0.6588518184550646\n",
      "test : 0.479418446160836\n",
      "407 loss: tensor(2.0106)\n",
      "train : 0.6501352569882777\n",
      "test : 0.471240345297592\n",
      "408 loss: tensor(2.0157)\n",
      "train : 0.6544434425408275\n",
      "test : 0.47260336210813264\n",
      "409 loss: tensor(2.0189)\n",
      "train : 0.6291954713956517\n",
      "test : 0.46224443434802365\n",
      "410 loss: tensor(2.0599)\n",
      "train : 0.6240857629496043\n",
      "test : 0.4522489777373921\n",
      "411 loss: tensor(2.0839)\n",
      "train : 0.5942290351668169\n",
      "test : 0.4480690595184007\n",
      "412 loss: tensor(2.1542)\n",
      "train : 0.5995391243362389\n",
      "test : 0.4295320308950477\n",
      "413 loss: tensor(2.1744)\n",
      "train : 0.5758941989780583\n",
      "test : 0.438255338482508\n",
      "414 loss: tensor(2.2268)\n",
      "train : 0.5471395651738302\n",
      "test : 0.3853702862335302\n",
      "415 loss: tensor(2.3356)\n",
      "train : 0.5179841699228535\n",
      "test : 0.4052703316674239\n",
      "416 loss: tensor(2.4330)\n",
      "train : 0.5345155796012424\n",
      "test : 0.3780099954566106\n",
      "417 loss: tensor(2.4283)\n",
      "train : 0.5906221821460775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.44579736483416627\n",
      "418 loss: tensor(2.2037)\n",
      "train : 0.6514377316902114\n",
      "test : 0.45288505224897774\n",
      "419 loss: tensor(2.0645)\n",
      "train : 0.6681695220919748\n",
      "test : 0.4981372103589278\n",
      "420 loss: tensor(1.9944)\n",
      "train : 0.6787897004308185\n",
      "test : 0.4788732394366197\n",
      "421 loss: tensor(1.9800)\n",
      "train : 0.6767858931970745\n",
      "test : 0.497955474784189\n",
      "422 loss: tensor(1.9649)\n",
      "train : 0.6730788498146478\n",
      "test : 0.47878237164925036\n",
      "423 loss: tensor(1.9771)\n",
      "train : 0.6646628594329226\n",
      "test : 0.49086778736937753\n",
      "424 loss: tensor(1.9785)\n",
      "train : 0.6525398256687707\n",
      "test : 0.46542480690595184\n",
      "425 loss: tensor(2.0145)\n",
      "train : 0.6394148882877467\n",
      "test : 0.47669241253975464\n",
      "426 loss: tensor(2.0262)\n",
      "train : 0.6242861436729786\n",
      "test : 0.44870513402998635\n",
      "427 loss: tensor(2.0780)\n",
      "train : 0.6200781484821161\n",
      "test : 0.4655156746933212\n",
      "428 loss: tensor(2.0667)\n",
      "train : 0.6177737701633103\n",
      "test : 0.4447978191731031\n",
      "429 loss: tensor(2.0932)\n",
      "train : 0.6325017533313295\n",
      "test : 0.46742389822807817\n",
      "430 loss: tensor(2.0395)\n",
      "train : 0.6426209798617373\n",
      "test : 0.4607905497501136\n",
      "431 loss: tensor(2.0322)\n",
      "train : 0.6563470594128845\n",
      "test : 0.47950931394820534\n",
      "432 loss: tensor(1.9847)\n",
      "train : 0.6591523895401262\n",
      "test : 0.4704225352112676\n",
      "433 loss: tensor(1.9833)\n",
      "train : 0.6538423003707043\n",
      "test : 0.47641980917764654\n",
      "434 loss: tensor(1.9802)\n",
      "train : 0.6354072738202585\n",
      "test : 0.46124488868696045\n",
      "435 loss: tensor(2.0165)\n",
      "train : 0.6019436930167318\n",
      "test : 0.440436165379373\n",
      "436 loss: tensor(2.0993)\n",
      "train : 0.5755936278929967\n",
      "test : 0.4238982280781463\n",
      "437 loss: tensor(2.1895)\n",
      "train : 0.5374211000901713\n",
      "test : 0.3924579736483417\n",
      "438 loss: tensor(2.3169)\n",
      "train : 0.5652740206392145\n",
      "test : 0.41653793730122673\n",
      "439 loss: tensor(2.2428)\n",
      "train : 0.5885181845506462\n",
      "test : 0.4225352112676056\n",
      "440 loss: tensor(2.1561)\n",
      "train : 0.613766155695822\n",
      "test : 0.45570195365742844\n",
      "441 loss: tensor(2.0756)\n",
      "train : 0.6665664763049794\n",
      "test : 0.47360290776919584\n",
      "442 loss: tensor(1.9785)\n",
      "train : 0.6709748522192165\n",
      "test : 0.48605179463880055\n",
      "443 loss: tensor(1.9375)\n",
      "train : 0.6837992185151789\n",
      "test : 0.4899591094956838\n",
      "444 loss: tensor(1.9198)\n",
      "train : 0.6772868450055105\n",
      "test : 0.48605179463880055\n",
      "445 loss: tensor(1.9236)\n",
      "train : 0.6674681895601643\n",
      "test : 0.48032712403452976\n",
      "446 loss: tensor(1.9424)\n",
      "train : 0.6511371606051498\n",
      "test : 0.46251703771013175\n",
      "447 loss: tensor(1.9909)\n",
      "train : 0.6204789099288649\n",
      "test : 0.45134029986369834\n",
      "448 loss: tensor(2.0546)\n",
      "train : 0.604448452058912\n",
      "test : 0.4288050885960927\n",
      "449 loss: tensor(2.1217)\n",
      "train : 0.594729986975253\n",
      "test : 0.4410722398909587\n",
      "450 loss: tensor(2.1444)\n",
      "train : 0.6207794810139264\n",
      "test : 0.43989095865515676\n",
      "451 loss: tensor(2.0793)\n",
      "train : 0.6463280232441639\n",
      "test : 0.471240345297592\n",
      "452 loss: tensor(1.9862)\n",
      "train : 0.6792906522392546\n",
      "test : 0.4765106769650159\n",
      "453 loss: tensor(1.9334)\n",
      "train : 0.6866045486424206\n",
      "test : 0.4984098137210359\n",
      "454 loss: tensor(1.8908)\n",
      "train : 0.6956216811942691\n",
      "test : 0.48768741481144934\n",
      "455 loss: tensor(1.8843)\n",
      "train : 0.6934174932371506\n",
      "test : 0.5014993184915947\n",
      "456 loss: tensor(1.8694)\n",
      "train : 0.6949203486624587\n",
      "test : 0.4830531576556111\n",
      "457 loss: tensor(1.8848)\n",
      "train : 0.6816952209197475\n",
      "test : 0.4978646069968196\n",
      "458 loss: tensor(1.8857)\n",
      "train : 0.676084560665264\n",
      "test : 0.4716946842344389\n",
      "459 loss: tensor(1.9245)\n",
      "train : 0.6568480112213205\n",
      "test : 0.48596092685143116\n",
      "460 loss: tensor(1.9382)\n",
      "train : 0.6460274521591023\n",
      "test : 0.45524761472058156\n",
      "461 loss: tensor(1.9882)\n",
      "train : 0.633704037671576\n",
      "test : 0.4700590640617901\n",
      "462 loss: tensor(1.9997)\n",
      "train : 0.6410179340747421\n",
      "test : 0.4489777373920945\n",
      "463 loss: tensor(2.0023)\n",
      "train : 0.643923454563671\n",
      "test : 0.476056338028169\n",
      "464 loss: tensor(1.9781)\n",
      "train : 0.6623584811141168\n",
      "test : 0.46215356656065426\n",
      "465 loss: tensor(1.9447)\n",
      "train : 0.6696723775172828\n",
      "test : 0.4919582008178101\n",
      "466 loss: tensor(1.9052)\n",
      "train : 0.68339845706843\n",
      "test : 0.478691503861881\n",
      "467 loss: tensor(1.8922)\n",
      "train : 0.6823965534515579\n",
      "test : 0.49568378009995456\n",
      "468 loss: tensor(1.8751)\n",
      "train : 0.6830978859833684\n",
      "test : 0.4786006360745116\n",
      "469 loss: tensor(1.8838)\n",
      "train : 0.6725778980062118\n",
      "test : 0.48732394366197185\n",
      "470 loss: tensor(1.8963)\n",
      "train : 0.6603546738803727\n",
      "test : 0.46715129486597\n",
      "471 loss: tensor(1.9253)\n",
      "train : 0.6423204087766757\n",
      "test : 0.46506133575647435\n",
      "472 loss: tensor(1.9826)\n",
      "train : 0.6243863340346658\n",
      "test : 0.44870513402998635\n",
      "473 loss: tensor(2.0240)\n",
      "train : 0.6002404568680493\n",
      "test : 0.4388914129940936\n",
      "474 loss: tensor(2.1059)\n",
      "train : 0.6091573990582106\n",
      "test : 0.4355293048614266\n",
      "475 loss: tensor(2.1129)\n",
      "train : 0.6333032762248272\n",
      "test : 0.45651976374375286\n",
      "476 loss: tensor(2.0407)\n",
      "train : 0.6594529606251879\n",
      "test : 0.47342117219445706\n",
      "477 loss: tensor(1.9450)\n",
      "train : 0.6771866546438232\n",
      "test : 0.4855974557019537\n",
      "478 loss: tensor(1.9181)\n",
      "train : 0.6601542931569983\n",
      "test : 0.475329395729214\n",
      "479 loss: tensor(1.9381)\n",
      "train : 0.6495341148181545\n",
      "test : 0.461608359836438\n",
      "480 loss: tensor(1.9762)\n",
      "train : 0.6075543532712153\n",
      "test : 0.44716038164470695\n",
      "481 loss: tensor(2.0863)\n",
      "train : 0.6077547339945897\n",
      "test : 0.43471149477510224\n",
      "482 loss: tensor(2.1087)\n",
      "train : 0.5931269411882577\n",
      "test : 0.4368923216719673\n",
      "483 loss: tensor(2.1293)\n",
      "train : 0.6218815749924858\n",
      "test : 0.4453430258973194\n",
      "484 loss: tensor(2.0685)\n",
      "train : 0.6345055605650737\n",
      "test : 0.45533848250795095\n",
      "485 loss: tensor(2.0148)\n",
      "train : 0.6547440136258892\n",
      "test : 0.46669695592912314\n",
      "486 loss: tensor(1.9930)\n",
      "train : 0.682897505259994\n",
      "test : 0.4822353475692867\n",
      "487 loss: tensor(1.8921)\n",
      "train : 0.6943192064923355\n",
      "test : 0.4981372103589278\n",
      "488 loss: tensor(1.8698)\n",
      "train : 0.7124536619577196\n",
      "test : 0.5017719218537029\n",
      "489 loss: tensor(1.8161)\n",
      "train : 0.7157599438933975\n",
      "test : 0.5094048159927306\n",
      "490 loss: tensor(1.8075)\n",
      "train : 0.7232742210199379\n",
      "test : 0.5091322126306225\n",
      "491 loss: tensor(1.7841)\n",
      "train : 0.7216711752329426\n",
      "test : 0.5128577919127669\n",
      "492 loss: tensor(1.7899)\n",
      "train : 0.722873459573189\n",
      "test : 0.5063153112221718\n",
      "493 loss: tensor(1.7805)\n",
      "train : 0.7096483318304779\n",
      "test : 0.5036801453884598\n",
      "494 loss: tensor(1.8074)\n",
      "train : 0.7076445245967338\n",
      "test : 0.496501590186279\n",
      "495 loss: tensor(1.8112)\n",
      "train : 0.6845005510469893\n",
      "test : 0.48159927305770106\n",
      "496 loss: tensor(1.8794)\n",
      "train : 0.6714758040276525\n",
      "test : 0.4716946842344389\n",
      "497 loss: tensor(1.9011)\n",
      "train : 0.6345055605650737\n",
      "test : 0.44461608359836435\n",
      "498 loss: tensor(2.0187)\n",
      "train : 0.6289950906722773\n",
      "test : 0.44752385279418444\n",
      "499 loss: tensor(2.0143)\n",
      "train : 0.6139665364191965\n",
      "test : 0.4319854611540209\n",
      "500 loss: tensor(2.0719)\n",
      "train : 0.647830878669472\n",
      "test : 0.45851885506587914\n",
      "501 loss: tensor(1.9768)\n",
      "train : 0.6752830377717663\n",
      "test : 0.472694229895502\n",
      "502 loss: tensor(1.9041)\n",
      "train : 0.7027351968740607\n",
      "test : 0.49704679691049525\n",
      "503 loss: tensor(1.8312)\n",
      "train : 0.7292856427211702\n",
      "test : 0.5082235347569287\n",
      "504 loss: tensor(1.7751)\n",
      "train : 0.732090972848412\n",
      "test : 0.5190368014538846\n",
      "505 loss: tensor(1.7556)\n",
      "train : 0.7466185752930569\n",
      "test : 0.5169468423443889\n",
      "506 loss: tensor(1.7343)\n",
      "train : 0.7387035367197675\n",
      "test : 0.5264879600181736\n",
      "507 loss: tensor(1.7377)\n",
      "train : 0.7441138162508767\n",
      "test : 0.5110404361653794\n",
      "508 loss: tensor(1.7370)\n",
      "train : 0.7243763149984972\n",
      "test : 0.5219445706497047\n",
      "509 loss: tensor(1.7611)\n",
      "train : 0.7197675583608857\n",
      "test : 0.4934120854157201\n",
      "510 loss: tensor(1.7871)\n",
      "train : 0.6881074040677286\n",
      "test : 0.5006815084052704\n",
      "511 loss: tensor(1.8459)\n",
      "train : 0.6628594329225529\n",
      "test : 0.46133575647432984\n",
      "512 loss: tensor(1.9239)\n",
      "train : 0.628794709948903\n",
      "test : 0.4655156746933212\n",
      "513 loss: tensor(1.9918)\n",
      "train : 0.616170724376315\n",
      "test : 0.43343934575193094\n",
      "514 loss: tensor(2.0828)\n",
      "train : 0.6341047991183248\n",
      "test : 0.46569741026806\n",
      "515 loss: tensor(1.9747)\n",
      "train : 0.6620579100290552\n",
      "test : 0.4599727396637892\n",
      "516 loss: tensor(1.9228)\n",
      "train : 0.7005310089169422\n",
      "test : 0.5019536574284417\n",
      "517 loss: tensor(1.8123)\n",
      "train : 0.7264803125939284\n",
      "test : 0.5007723761926397\n",
      "518 loss: tensor(1.7591)\n",
      "train : 0.7417092475703837\n",
      "test : 0.520581553839164\n",
      "519 loss: tensor(1.7185)\n",
      "train : 0.7502254283137962\n",
      "test : 0.5174011812812358\n",
      "520 loss: tensor(1.7021)\n",
      "train : 0.7465183849313696\n",
      "test : 0.5206724216265334\n",
      "521 loss: tensor(1.7055)\n",
      "train : 0.7337942089970945\n",
      "test : 0.5095865515674693\n",
      "522 loss: tensor(1.7222)\n",
      "train : 0.6988277727682597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.49050431621990004\n",
      "523 loss: tensor(1.7997)\n",
      "train : 0.6630598136459273\n",
      "test : 0.4606996819627442\n",
      "524 loss: tensor(1.8824)\n",
      "train : 0.5841098086364092\n",
      "test : 0.42353475692866877\n",
      "525 loss: tensor(2.1396)\n",
      "train : 0.6095581605049594\n",
      "test : 0.4198091776465243\n",
      "526 loss: tensor(2.0740)\n",
      "train : 0.5932271315499449\n",
      "test : 0.4269877328487051\n",
      "527 loss: tensor(2.1171)\n",
      "train : 0.6572487726680694\n",
      "test : 0.461608359836438\n",
      "528 loss: tensor(1.9252)\n",
      "train : 0.6894098787696623\n",
      "test : 0.4870513402998637\n",
      "529 loss: tensor(1.8546)\n",
      "train : 0.7021340547039375\n",
      "test : 0.4953203089504771\n",
      "530 loss: tensor(1.7971)\n",
      "train : 0.7153591824466486\n",
      "test : 0.5030440708768742\n",
      "531 loss: tensor(1.7854)\n",
      "train : 0.7049393848311792\n",
      "test : 0.4995002271694684\n",
      "532 loss: tensor(1.7906)\n",
      "train : 0.7101492836389139\n",
      "test : 0.49713766469786463\n",
      "533 loss: tensor(1.7876)\n",
      "train : 0.6891093076846008\n",
      "test : 0.4907769195820082\n",
      "534 loss: tensor(1.8286)\n",
      "train : 0.6940186354072738\n",
      "test : 0.4877782825988187\n",
      "535 loss: tensor(1.8186)\n",
      "train : 0.6754834184951408\n",
      "test : 0.48723307587460246\n",
      "536 loss: tensor(1.8705)\n",
      "train : 0.7062418595331129\n",
      "test : 0.49332121762835074\n",
      "537 loss: tensor(1.7958)\n",
      "train : 0.7081454764051698\n",
      "test : 0.5023171285779191\n",
      "538 loss: tensor(1.7921)\n",
      "train : 0.7346959222522793\n",
      "test : 0.5101317582916856\n",
      "539 loss: tensor(1.7321)\n",
      "train : 0.7352970644224026\n",
      "test : 0.5151294865970014\n",
      "540 loss: tensor(1.7233)\n",
      "train : 0.7497244765053602\n",
      "test : 0.5179463880054521\n",
      "541 loss: tensor(1.6935)\n",
      "train : 0.7476204789099289\n",
      "test : 0.5213993639254885\n",
      "542 loss: tensor(1.6937)\n",
      "train : 0.7550345656747821\n",
      "test : 0.5173103134938665\n",
      "543 loss: tensor(1.6832)\n",
      "train : 0.7442140066125639\n",
      "test : 0.5213993639254885\n",
      "544 loss: tensor(1.6944)\n",
      "train : 0.7505259993988578\n",
      "test : 0.5111313039527488\n",
      "545 loss: tensor(1.6938)\n",
      "train : 0.7358982065925258\n",
      "test : 0.5183098591549296\n",
      "546 loss: tensor(1.7149)\n",
      "train : 0.7414086764853222\n",
      "test : 0.5050431621990005\n",
      "547 loss: tensor(1.7181)\n",
      "train : 0.7236749824666867\n",
      "test : 0.5149477510222626\n",
      "548 loss: tensor(1.7450)\n",
      "train : 0.7272818354874261\n",
      "test : 0.49568378009995456\n",
      "549 loss: tensor(1.7454)\n",
      "train : 0.714056707744715\n",
      "test : 0.5120399818264425\n",
      "550 loss: tensor(1.7663)\n",
      "train : 0.7181645125738904\n",
      "test : 0.488323489323035\n",
      "551 loss: tensor(1.7667)\n",
      "train : 0.7115519487025348\n",
      "test : 0.5081326669695593\n",
      "552 loss: tensor(1.7668)\n",
      "train : 0.7091473800220419\n",
      "test : 0.4840527033166742\n",
      "553 loss: tensor(1.7925)\n",
      "train : 0.7030357679591224\n",
      "test : 0.5046796910495229\n",
      "554 loss: tensor(1.7720)\n",
      "train : 0.6854022643021741\n",
      "test : 0.46742389822807817\n",
      "555 loss: tensor(1.8683)\n",
      "train : 0.6560464883278229\n",
      "test : 0.4781462971376647\n",
      "556 loss: tensor(1.8839)\n",
      "train : 0.6256888087365995\n",
      "test : 0.4298046342571558\n",
      "557 loss: tensor(2.0417)\n",
      "train : 0.6056507363991585\n",
      "test : 0.4409813721035893\n",
      "558 loss: tensor(2.0557)\n",
      "train : 0.6333032762248272\n",
      "test : 0.43189459336665154\n",
      "559 loss: tensor(2.0620)\n",
      "train : 0.6847009317703637\n",
      "test : 0.4814175374829623\n",
      "560 loss: tensor(1.8352)\n",
      "train : 0.7271816451257389\n",
      "test : 0.4955020445252158\n",
      "561 loss: tensor(1.7529)\n",
      "train : 0.7396052499749524\n",
      "test : 0.5187641980917764\n",
      "562 loss: tensor(1.6994)\n",
      "train : 0.7525298066326019\n",
      "test : 0.5114039073148569\n",
      "563 loss: tensor(1.6792)\n",
      "train : 0.7508265704839194\n",
      "test : 0.5196728759654703\n",
      "564 loss: tensor(1.6678)\n",
      "train : 0.7465183849313696\n",
      "test : 0.5099500227169469\n",
      "565 loss: tensor(1.6747)\n",
      "train : 0.734996493337341\n",
      "test : 0.5117673784643344\n",
      "566 loss: tensor(1.6888)\n",
      "train : 0.7225728884881274\n",
      "test : 0.49977283053157656\n",
      "567 loss: tensor(1.7150)\n",
      "train : 0.7042380522993688\n",
      "test : 0.49613811903680144\n",
      "568 loss: tensor(1.7490)\n",
      "train : 0.6958220619176435\n",
      "test : 0.4841435711040436\n",
      "569 loss: tensor(1.7727)\n",
      "train : 0.6895100691313496\n",
      "test : 0.4828714220808723\n",
      "570 loss: tensor(1.7927)\n",
      "train : 0.7029355775974351\n",
      "test : 0.4855065879145843\n",
      "571 loss: tensor(1.7616)\n",
      "train : 0.7102494740006011\n",
      "test : 0.497955474784189\n",
      "572 loss: tensor(1.7403)\n",
      "train : 0.7376014427412083\n",
      "test : 0.5033166742389823\n",
      "573 loss: tensor(1.6813)\n",
      "train : 0.7487225728884881\n",
      "test : 0.5185824625170377\n",
      "574 loss: tensor(1.6561)\n",
      "train : 0.7692615970343653\n",
      "test : 0.5176737846433439\n",
      "575 loss: tensor(1.6173)\n",
      "train : 0.7735697825869151\n",
      "test : 0.5319400272603362\n",
      "576 loss: tensor(1.6052)\n",
      "train : 0.7831880573088869\n",
      "test : 0.528759654702408\n",
      "577 loss: tensor(1.5852)\n",
      "train : 0.7838893898406973\n",
      "test : 0.5370286233530214\n",
      "578 loss: tensor(1.5833)\n",
      "train : 0.7854924356276927\n",
      "test : 0.5316674238982281\n",
      "579 loss: tensor(1.5751)\n",
      "train : 0.7805831079050195\n",
      "test : 0.536210813266697\n",
      "580 loss: tensor(1.5835)\n",
      "train : 0.777877968139465\n",
      "test : 0.5290322580645161\n",
      "581 loss: tensor(1.5889)\n",
      "train : 0.7648532211201282\n",
      "test : 0.5263062244434348\n",
      "582 loss: tensor(1.6155)\n",
      "train : 0.7516280933774171\n",
      "test : 0.5127669241253976\n",
      "583 loss: tensor(1.6541)\n",
      "train : 0.7247770764452459\n",
      "test : 0.4949568378009995\n",
      "584 loss: tensor(1.7198)\n",
      "train : 0.6864041679190462\n",
      "test : 0.47641980917764654\n",
      "585 loss: tensor(1.8360)\n",
      "train : 0.6650636208796714\n",
      "test : 0.4579736483416629\n",
      "586 loss: tensor(1.8965)\n",
      "train : 0.6491333533714057\n",
      "test : 0.4533393911858246\n",
      "587 loss: tensor(1.9567)\n",
      "train : 0.6656647630497946\n",
      "test : 0.4587005906406179\n",
      "588 loss: tensor(1.8832)\n",
      "train : 0.6966235848111412\n",
      "test : 0.4741481144934121\n",
      "589 loss: tensor(1.7790)\n",
      "train : 0.7049393848311792\n",
      "test : 0.4841435711040436\n",
      "590 loss: tensor(1.7634)\n",
      "train : 0.7280833583809238\n",
      "test : 0.4947751022262608\n",
      "591 loss: tensor(1.7026)\n",
      "train : 0.720869652339445\n",
      "test : 0.49541117673784646\n",
      "592 loss: tensor(1.7122)\n",
      "train : 0.7262799318705541\n",
      "test : 0.49850068150840526\n",
      "593 loss: tensor(1.7061)\n",
      "train : 0.7186654643823264\n",
      "test : 0.49695592912312586\n",
      "594 loss: tensor(1.7186)\n",
      "train : 0.7176635607654543\n",
      "test : 0.49604725124943205\n",
      "595 loss: tensor(1.7344)\n",
      "train : 0.713054804127843\n",
      "test : 0.4898682417083144\n",
      "596 loss: tensor(1.7370)\n",
      "train : 0.7161607053401463\n",
      "test : 0.4983189459336665\n",
      "597 loss: tensor(1.7321)\n",
      "train : 0.7255785993387436\n",
      "test : 0.49286687869150386\n",
      "598 loss: tensor(1.7090)\n",
      "train : 0.7372006812944595\n",
      "test : 0.5064970467969105\n",
      "599 loss: tensor(1.6889)\n",
      "train : 0.7444143873359382\n",
      "test : 0.5002271694684235\n",
      "600 loss: tensor(1.6635)\n",
      "train : 0.7561366596533413\n",
      "test : 0.5179463880054521\n",
      "601 loss: tensor(1.6378)\n",
      "train : 0.7656547440136259\n",
      "test : 0.509041344843253\n",
      "602 loss: tensor(1.6157)\n",
      "train : 0.7714657849914838\n",
      "test : 0.529486597001363\n",
      "603 loss: tensor(1.5975)\n",
      "train : 0.7800821560965835\n",
      "test : 0.5162199000454339\n",
      "604 loss: tensor(1.5846)\n",
      "train : 0.7799819657348963\n",
      "test : 0.5339391185824626\n",
      "605 loss: tensor(1.5774)\n",
      "train : 0.780783488628394\n",
      "test : 0.517219445706497\n",
      "606 loss: tensor(1.5738)\n",
      "train : 0.7775773970544033\n",
      "test : 0.5350295320308951\n",
      "607 loss: tensor(1.5765)\n",
      "train : 0.7744714958421\n",
      "test : 0.5147660154475239\n",
      "608 loss: tensor(1.5819)\n",
      "train : 0.7693617873960525\n",
      "test : 0.5306678782371649\n",
      "609 loss: tensor(1.5929)\n",
      "train : 0.761847510269512\n",
      "test : 0.5088596092685143\n",
      "610 loss: tensor(1.6079)\n",
      "train : 0.7539324716962228\n",
      "test : 0.5240345297592004\n",
      "611 loss: tensor(1.6232)\n",
      "train : 0.746217813846308\n",
      "test : 0.49986369831894595\n",
      "612 loss: tensor(1.6503)\n",
      "train : 0.7301873559763551\n",
      "test : 0.5113130395274875\n",
      "613 loss: tensor(1.6755)\n",
      "train : 0.720869652339445\n",
      "test : 0.4834166288050886\n",
      "614 loss: tensor(1.7085)\n",
      "train : 0.6956216811942691\n",
      "test : 0.496501590186279\n",
      "615 loss: tensor(1.7806)\n",
      "train : 0.7034365294058712\n",
      "test : 0.47342117219445706\n",
      "616 loss: tensor(1.7651)\n",
      "train : 0.6761847510269512\n",
      "test : 0.48450704225352115\n",
      "617 loss: tensor(1.8413)\n",
      "train : 0.6875062618976054\n",
      "test : 0.4757837346660609\n",
      "618 loss: tensor(1.8230)\n",
      "train : 0.6847009317703637\n",
      "test : 0.47796456156292594\n",
      "619 loss: tensor(1.8083)\n",
      "train : 0.6852018835787997\n",
      "test : 0.4772376192639709\n",
      "620 loss: tensor(1.8143)\n",
      "train : 0.6912133052800321\n",
      "test : 0.47714675147660157\n",
      "621 loss: tensor(1.8082)\n",
      "train : 0.7286845005510469\n",
      "test : 0.5019536574284417\n",
      "622 loss: tensor(1.7104)\n",
      "train : 0.7448151487826871\n",
      "test : 0.5014084507042254\n",
      "623 loss: tensor(1.6704)\n",
      "train : 0.7804829175433323\n",
      "test : 0.5340299863698319\n",
      "624 loss: tensor(1.5908)\n",
      "train : 0.7848912934575694\n",
      "test : 0.5242162653339392\n",
      "625 loss: tensor(1.5681)\n",
      "train : 0.7970143272217213\n",
      "test : 0.5412085415720127\n",
      "626 loss: tensor(1.5416)\n",
      "train : 0.7986173730087166\n",
      "test : 0.5318491594729668\n",
      "627 loss: tensor(1.5360)\n",
      "train : 0.7951107103496644\n",
      "test : 0.5402089959109496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628 loss: tensor(1.5331)\n",
      "train : 0.7989179440937781\n",
      "test : 0.5300318037255792\n",
      "629 loss: tensor(1.5368)\n",
      "train : 0.788598336839996\n",
      "test : 0.5358473421172194\n",
      "630 loss: tensor(1.5468)\n",
      "train : 0.7895000500951809\n",
      "test : 0.5228532485233984\n",
      "631 loss: tensor(1.5580)\n",
      "train : 0.7766756837992185\n",
      "test : 0.5310313493866424\n",
      "632 loss: tensor(1.5763)\n",
      "train : 0.7735697825869151\n",
      "test : 0.5154020899591095\n",
      "633 loss: tensor(1.5929)\n",
      "train : 0.7589419897805831\n",
      "test : 0.5255792821444798\n",
      "634 loss: tensor(1.6082)\n",
      "train : 0.7628494138863842\n",
      "test : 0.5060427078600636\n",
      "635 loss: tensor(1.6198)\n",
      "train : 0.7576395150786495\n",
      "test : 0.5237619263970922\n",
      "636 loss: tensor(1.6147)\n",
      "train : 0.7660555054603747\n",
      "test : 0.5084052703316674\n",
      "637 loss: tensor(1.6124)\n",
      "train : 0.7666566476304979\n",
      "test : 0.5267605633802817\n",
      "638 loss: tensor(1.5926)\n",
      "train : 0.7668570283538724\n",
      "test : 0.515765561108587\n",
      "639 loss: tensor(1.5939)\n",
      "train : 0.7584410379721471\n",
      "test : 0.5250340754202635\n",
      "640 loss: tensor(1.5986)\n",
      "train : 0.7521290451858531\n",
      "test : 0.5091322126306225\n",
      "641 loss: tensor(1.6166)\n",
      "train : 0.7334936379120328\n",
      "test : 0.509041344843253\n",
      "642 loss: tensor(1.6642)\n",
      "train : 0.7310890692315399\n",
      "test : 0.5007723761926397\n",
      "643 loss: tensor(1.6504)\n",
      "train : 0.7295862138062318\n",
      "test : 0.506133575647433\n",
      "644 loss: tensor(1.6734)\n",
      "train : 0.7513275222923554\n",
      "test : 0.5096774193548387\n",
      "645 loss: tensor(1.6070)\n",
      "train : 0.7601442741208295\n",
      "test : 0.5188550658791459\n",
      "646 loss: tensor(1.5932)\n",
      "train : 0.7818855826069532\n",
      "test : 0.5235801908223535\n",
      "647 loss: tensor(1.5415)\n",
      "train : 0.7895000500951809\n",
      "test : 0.5308496138119037\n",
      "648 loss: tensor(1.5282)\n",
      "train : 0.7987175633704038\n",
      "test : 0.5342117219445707\n",
      "649 loss: tensor(1.5015)\n",
      "train : 0.8021240356677688\n",
      "test : 0.5348477964561563\n",
      "650 loss: tensor(1.4968)\n",
      "train : 0.8068329826670674\n",
      "test : 0.5374829622898682\n",
      "651 loss: tensor(1.4846)\n",
      "train : 0.8053301272417593\n",
      "test : 0.5373920945024989\n",
      "652 loss: tensor(1.4899)\n",
      "train : 0.8033263200080152\n",
      "test : 0.5368468877782826\n",
      "653 loss: tensor(1.4898)\n",
      "train : 0.7956116621581004\n",
      "test : 0.5276692412539755\n",
      "654 loss: tensor(1.5060)\n",
      "train : 0.7901011922653041\n",
      "test : 0.5317582916855974\n",
      "655 loss: tensor(1.5195)\n",
      "train : 0.7824867247770765\n",
      "test : 0.5186733303044071\n",
      "656 loss: tensor(1.5405)\n",
      "train : 0.7703636910129246\n",
      "test : 0.5211267605633803\n",
      "657 loss: tensor(1.5674)\n",
      "train : 0.7685602645025549\n",
      "test : 0.5084052703316674\n",
      "658 loss: tensor(1.5710)\n",
      "train : 0.7615469391844505\n",
      "test : 0.5189459336665152\n",
      "659 loss: tensor(1.6014)\n",
      "train : 0.771065023544735\n",
      "test : 0.51094956837801\n",
      "660 loss: tensor(1.5701)\n",
      "train : 0.7664562669071235\n",
      "test : 0.5240345297592004\n",
      "661 loss: tensor(1.5929)\n",
      "train : 0.7656547440136259\n",
      "test : 0.5133121308496138\n",
      "662 loss: tensor(1.5908)\n",
      "train : 0.7641518885883178\n",
      "test : 0.524670604270786\n",
      "663 loss: tensor(1.5874)\n",
      "train : 0.7539324716962228\n",
      "test : 0.5059518400726942\n",
      "664 loss: tensor(1.6360)\n",
      "train : 0.7520288548241659\n",
      "test : 0.5140390731485688\n",
      "665 loss: tensor(1.6036)\n",
      "train : 0.7478208596333032\n",
      "test : 0.4991367560199909\n",
      "666 loss: tensor(1.6420)\n",
      "train : 0.7424105801021942\n",
      "test : 0.5074965924579736\n",
      "667 loss: tensor(1.6192)\n",
      "train : 0.7401062017833885\n",
      "test : 0.49450249886415265\n",
      "668 loss: tensor(1.6464)\n",
      "train : 0.7416090572086965\n",
      "test : 0.5021353930031803\n",
      "669 loss: tensor(1.6303)\n",
      "train : 0.7373008716561467\n",
      "test : 0.4926851431167651\n",
      "670 loss: tensor(1.6309)\n",
      "train : 0.7520288548241659\n",
      "test : 0.5028623353021354\n",
      "671 loss: tensor(1.6217)\n",
      "train : 0.7583408476104598\n",
      "test : 0.5046796910495229\n",
      "672 loss: tensor(1.5779)\n",
      "train : 0.7867949103296263\n",
      "test : 0.5253066787823717\n",
      "673 loss: tensor(1.5332)\n",
      "train : 0.7960124236048493\n",
      "test : 0.5265788278055429\n",
      "674 loss: tensor(1.4919)\n",
      "train : 0.8117423103897405\n",
      "test : 0.5418446160835984\n",
      "675 loss: tensor(1.4716)\n",
      "train : 0.8047289850716361\n",
      "test : 0.5338482507950931\n",
      "676 loss: tensor(1.4670)\n",
      "train : 0.7904017633503657\n",
      "test : 0.5355747387551113\n",
      "677 loss: tensor(1.5045)\n",
      "train : 0.7730688307784791\n",
      "test : 0.515765561108587\n",
      "678 loss: tensor(1.5370)\n",
      "train : 0.732090972848412\n",
      "test : 0.5064061790095411\n",
      "679 loss: tensor(1.6465)\n",
      "train : 0.716962228233644\n",
      "test : 0.47751022262607906\n",
      "680 loss: tensor(1.6991)\n",
      "train : 0.6927161607053401\n",
      "test : 0.4845979100408905\n",
      "681 loss: tensor(1.7827)\n",
      "train : 0.7167618475102695\n",
      "test : 0.4814175374829623\n",
      "682 loss: tensor(1.7228)\n",
      "train : 0.7412082957619477\n",
      "test : 0.5024988641526579\n",
      "683 loss: tensor(1.6256)\n",
      "train : 0.7729686404167919\n",
      "test : 0.5227623807360291\n",
      "684 loss: tensor(1.5677)\n",
      "train : 0.8016230838593328\n",
      "test : 0.532394366197183\n",
      "685 loss: tensor(1.4790)\n",
      "train : 0.8139464983468591\n",
      "test : 0.5482053611994548\n",
      "686 loss: tensor(1.4655)\n",
      "train : 0.8258691513876365\n",
      "test : 0.5415720127214902\n",
      "687 loss: tensor(1.4355)\n",
      "train : 0.8215609658350866\n",
      "test : 0.551385733757383\n",
      "688 loss: tensor(1.4384)\n",
      "train : 0.8252680092175133\n",
      "test : 0.5402089959109496\n",
      "689 loss: tensor(1.4347)\n",
      "train : 0.8135457369001102\n",
      "test : 0.5472058155383916\n",
      "690 loss: tensor(1.4465)\n",
      "train : 0.8133453561767358\n",
      "test : 0.5310313493866424\n",
      "691 loss: tensor(1.4557)\n",
      "train : 0.7980162308385933\n",
      "test : 0.5398455247614721\n",
      "692 loss: tensor(1.4755)\n",
      "train : 0.7963129946899108\n",
      "test : 0.5203998182644253\n",
      "693 loss: tensor(1.4932)\n",
      "train : 0.7748722572888488\n",
      "test : 0.5264879600181736\n",
      "694 loss: tensor(1.5159)\n",
      "train : 0.7780783488628394\n",
      "test : 0.5127669241253976\n",
      "695 loss: tensor(1.5303)\n",
      "train : 0.7630497946097585\n",
      "test : 0.5168559745570195\n",
      "696 loss: tensor(1.5433)\n",
      "train : 0.7754733994589721\n",
      "test : 0.5148568832348932\n",
      "697 loss: tensor(1.5351)\n",
      "train : 0.7769762548842801\n",
      "test : 0.5165833711949114\n",
      "698 loss: tensor(1.5242)\n",
      "train : 0.7917042380522994\n",
      "test : 0.5270331667423899\n",
      "699 loss: tensor(1.4945)\n",
      "train : 0.8031259392846408\n",
      "test : 0.5290322580645161\n",
      "700 loss: tensor(1.4676)\n",
      "train : 0.8125438332832382\n",
      "test : 0.541753748296229\n",
      "701 loss: tensor(1.4464)\n",
      "train : 0.8255685803025749\n",
      "test : 0.5381190368014539\n",
      "702 loss: tensor(1.4275)\n",
      "train : 0.8198577296864041\n",
      "test : 0.5481144934120854\n",
      "703 loss: tensor(1.4291)\n",
      "train : 0.82216210800521\n",
      "test : 0.5324852339845525\n",
      "704 loss: tensor(1.4278)\n",
      "train : 0.8069331730287547\n",
      "test : 0.5416628805088596\n",
      "705 loss: tensor(1.4539)\n",
      "train : 0.8020238453060815\n",
      "test : 0.5194911403907315\n",
      "706 loss: tensor(1.4714)\n",
      "train : 0.7775773970544033\n",
      "test : 0.5291231258518855\n",
      "707 loss: tensor(1.5271)\n",
      "train : 0.7761747319907825\n",
      "test : 0.5041344843253067\n",
      "708 loss: tensor(1.5429)\n",
      "train : 0.7585412283338343\n",
      "test : 0.5192185370286233\n",
      "709 loss: tensor(1.5845)\n",
      "train : 0.7698627392044886\n",
      "test : 0.49995456610631533\n",
      "710 loss: tensor(1.5636)\n",
      "train : 0.7789800621180243\n",
      "test : 0.5271240345297592\n",
      "711 loss: tensor(1.5320)\n",
      "train : 0.7964131850515981\n",
      "test : 0.5119491140390732\n",
      "712 loss: tensor(1.5063)\n",
      "train : 0.8078348862839395\n",
      "test : 0.5396637891867333\n",
      "713 loss: tensor(1.4624)\n",
      "train : 0.8215609658350866\n",
      "test : 0.5274875056792367\n",
      "714 loss: tensor(1.4397)\n",
      "train : 0.8318805730888689\n",
      "test : 0.5510222626079055\n",
      "715 loss: tensor(1.4125)\n",
      "train : 0.8393948502154093\n",
      "test : 0.5403907314856883\n",
      "716 loss: tensor(1.3964)\n",
      "train : 0.8420999899809638\n",
      "test : 0.5532939572921399\n",
      "717 loss: tensor(1.3862)\n",
      "train : 0.845606652640016\n",
      "test : 0.5439345751930941\n",
      "718 loss: tensor(1.3797)\n",
      "train : 0.8409978960024046\n",
      "test : 0.5536574284416175\n",
      "719 loss: tensor(1.3849)\n",
      "train : 0.8391944694920349\n",
      "test : 0.5425715583825533\n",
      "720 loss: tensor(1.3908)\n",
      "train : 0.8252680092175133\n",
      "test : 0.547296683325761\n",
      "721 loss: tensor(1.4170)\n",
      "train : 0.8129445947299869\n",
      "test : 0.5254884143571104\n",
      "722 loss: tensor(1.4452)\n",
      "train : 0.7836890091173229\n",
      "test : 0.5217628350749659\n",
      "723 loss: tensor(1.5107)\n",
      "train : 0.75403266205791\n",
      "test : 0.4953203089504771\n",
      "724 loss: tensor(1.5796)\n",
      "train : 0.7202685101693217\n",
      "test : 0.48886869604725125\n",
      "725 loss: tensor(1.6762)\n",
      "train : 0.7141568981064021\n",
      "test : 0.46933212176283506\n",
      "726 loss: tensor(1.7078)\n",
      "train : 0.7350966836990281\n",
      "test : 0.4905951840072694\n",
      "727 loss: tensor(1.6547)\n",
      "train : 0.7798817753732091\n",
      "test : 0.50804179918219\n",
      "728 loss: tensor(1.5194)\n",
      "train : 0.8272718164512574\n",
      "test : 0.5430258973194003\n",
      "729 loss: tensor(1.4241)\n",
      "train : 0.8390942791303476\n",
      "test : 0.5412085415720127\n",
      "730 loss: tensor(1.3844)\n",
      "train : 0.8528203586814949\n",
      "test : 0.5542935029532031\n",
      "731 loss: tensor(1.3611)\n",
      "train : 0.8408977056407173\n",
      "test : 0.546569741026806\n",
      "732 loss: tensor(1.3693)\n",
      "train : 0.8479110309588218\n",
      "test : 0.548750567923671\n",
      "733 loss: tensor(1.3652)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 0.8325819056206792\n",
      "test : 0.5422989550204452\n",
      "734 loss: tensor(1.3869)\n",
      "train : 0.8331830477908025\n",
      "test : 0.5387551113130395\n",
      "735 loss: tensor(1.3916)\n",
      "train : 0.8131449754533614\n",
      "test : 0.5340299863698319\n",
      "736 loss: tensor(1.4253)\n",
      "train : 0.8111411682196172\n",
      "test : 0.5272149023171285\n",
      "737 loss: tensor(1.4360)\n",
      "train : 0.7910029055204889\n",
      "test : 0.5238527941844616\n",
      "738 loss: tensor(1.4767)\n",
      "train : 0.7906021440737401\n",
      "test : 0.5155838255338483\n",
      "739 loss: tensor(1.4830)\n",
      "train : 0.778378919947901\n",
      "test : 0.5197637437528396\n",
      "740 loss: tensor(1.5071)\n",
      "train : 0.7864943392445647\n",
      "test : 0.5154929577464789\n",
      "741 loss: tensor(1.4920)\n",
      "train : 0.7920048091373609\n",
      "test : 0.5244888686960473\n",
      "742 loss: tensor(1.4798)\n",
      "train : 0.8067327923053802\n",
      "test : 0.5259427532939573\n",
      "743 loss: tensor(1.4432)\n",
      "train : 0.8199579200480914\n",
      "test : 0.5373012267151295\n",
      "744 loss: tensor(1.4102)\n",
      "train : 0.8331830477908025\n",
      "test : 0.5410268059972739\n",
      "745 loss: tensor(1.3780)\n",
      "train : 0.84510570083158\n",
      "test : 0.5520218082689686\n",
      "746 loss: tensor(1.3553)\n",
      "train : 0.8511171225328124\n",
      "test : 0.5512948659700136\n",
      "747 loss: tensor(1.3363)\n",
      "train : 0.8560264502554854\n",
      "test : 0.5582008178100863\n",
      "748 loss: tensor(1.3305)\n",
      "train : 0.8538222622983669\n",
      "test : 0.5514766015447524\n",
      "749 loss: tensor(1.3240)\n",
      "train : 0.8525197875964332\n",
      "test : 0.5590186278964108\n",
      "750 loss: tensor(1.3348)\n",
      "train : 0.8384931369602244\n",
      "test : 0.5427532939572921\n",
      "751 loss: tensor(1.3448)\n",
      "train : 0.8271716260895702\n",
      "test : 0.5483870967741935\n",
      "752 loss: tensor(1.3852)\n",
      "train : 0.7966135657749724\n",
      "test : 0.522489777373921\n",
      "753 loss: tensor(1.4335)\n",
      "train : 0.7619477006311993\n",
      "test : 0.5122217174011813\n",
      "754 loss: tensor(1.5416)\n",
      "train : 0.7072437631499849\n",
      "test : 0.46715129486597\n",
      "755 loss: tensor(1.6800)\n",
      "train : 0.6644624787095481\n",
      "test : 0.4491594729668333\n",
      "756 loss: tensor(1.8607)\n",
      "train : 0.6340046087566377\n",
      "test : 0.41790095411176736\n",
      "757 loss: tensor(1.9200)\n",
      "train : 0.6861035968339846\n",
      "test : 0.4573375738300772\n",
      "758 loss: tensor(1.7992)\n",
      "train : 0.7837891994790102\n",
      "test : 0.506133575647433\n",
      "759 loss: tensor(1.4961)\n",
      "train : 0.8482116020438834\n",
      "test : 0.5474784189004998\n",
      "760 loss: tensor(1.3629)\n",
      "train : 0.8789700430818556\n",
      "test : 0.567741935483871\n",
      "761 loss: tensor(1.3089)\n",
      "train : 0.8856827973148983\n",
      "test : 0.5671967287596547\n",
      "762 loss: tensor(1.2900)\n",
      "train : 0.8878869852720168\n",
      "test : 0.5687414811449342\n",
      "763 loss: tensor(1.2793)\n",
      "train : 0.8906923153992586\n",
      "test : 0.5691049522944116\n",
      "764 loss: tensor(1.2724)\n",
      "train : 0.889389840697325\n",
      "test : 0.5693775556565198\n",
      "765 loss: tensor(1.2675)\n",
      "train : 0.8909928864843202\n",
      "test : 0.5698318945933667\n",
      "766 loss: tensor(1.2642)\n",
      "train : 0.8881875563570785\n",
      "test : 0.5700136301681054\n",
      "767 loss: tensor(1.2624)\n",
      "train : 0.8900911732291353\n",
      "test : 0.5671058609722853\n",
      "768 loss: tensor(1.2620)\n",
      "train : 0.8842801322512774\n",
      "test : 0.5697410268059973\n",
      "769 loss: tensor(1.2643)\n",
      "train : 0.884079751527903\n",
      "test : 0.5638346206269877\n",
      "770 loss: tensor(1.2691)\n",
      "train : 0.8737601442741209\n",
      "test : 0.5652885052248978\n",
      "771 loss: tensor(1.2804)\n",
      "train : 0.8691513876365093\n",
      "test : 0.5537482962289868\n",
      "772 loss: tensor(1.2979)\n",
      "train : 0.8510169321711252\n",
      "test : 0.5563834620626987\n",
      "773 loss: tensor(1.3318)\n",
      "train : 0.8315800020038072\n",
      "test : 0.5300318037255792\n",
      "774 loss: tensor(1.3832)\n",
      "train : 0.7900010019036169\n",
      "test : 0.53139482053612\n",
      "775 loss: tensor(1.4759)\n",
      "train : 0.7440136258891895\n",
      "test : 0.4810540663334848\n",
      "776 loss: tensor(1.6045)\n",
      "train : 0.6938182546838995\n",
      "test : 0.4770558836892322\n",
      "777 loss: tensor(1.7992)\n",
      "train : 0.6693718064322213\n",
      "test : 0.435620172648796\n",
      "778 loss: tensor(1.9058)\n",
      "train : 0.699028153491634\n",
      "test : 0.4768741481144934\n",
      "779 loss: tensor(1.8445)\n",
      "train : 0.7693617873960525\n",
      "test : 0.4990458882326215\n",
      "780 loss: tensor(1.5806)\n",
      "train : 0.8317803827271817\n",
      "test : 0.5339391185824626\n",
      "781 loss: tensor(1.3958)\n",
      "train : 0.8574291153191063\n",
      "test : 0.5555656519763744\n",
      "782 loss: tensor(1.3440)\n",
      "train : 0.8777677587416091\n",
      "test : 0.5573830077237619\n",
      "783 loss: tensor(1.3012)\n",
      "train : 0.8815749924857229\n",
      "test : 0.5671058609722853\n",
      "784 loss: tensor(1.2809)\n",
      "train : 0.8906923153992586\n",
      "test : 0.5651976374375284\n",
      "785 loss: tensor(1.2690)\n",
      "train : 0.8860835587616471\n",
      "test : 0.5716492503407542\n",
      "786 loss: tensor(1.2611)\n",
      "train : 0.8905921250375713\n",
      "test : 0.5647432985006815\n",
      "787 loss: tensor(1.2584)\n",
      "train : 0.8834786093577798\n",
      "test : 0.5712857791912767\n",
      "788 loss: tensor(1.2577)\n",
      "train : 0.8855826069532111\n",
      "test : 0.5618355293048615\n",
      "789 loss: tensor(1.2607)\n",
      "train : 0.8768660454864242\n",
      "test : 0.5672875965470241\n",
      "790 loss: tensor(1.2686)\n",
      "train : 0.8720569081254383\n",
      "test : 0.5540208995910949\n",
      "791 loss: tensor(1.2809)\n",
      "train : 0.8550245466386134\n",
      "test : 0.5601999091322126\n",
      "792 loss: tensor(1.3079)\n",
      "train : 0.8426009417893998\n",
      "test : 0.536210813266697\n",
      "793 loss: tensor(1.3407)\n",
      "train : 0.8085362188157499\n",
      "test : 0.539572921399364\n",
      "794 loss: tensor(1.4189)\n",
      "train : 0.7779781585011522\n",
      "test : 0.5091322126306225\n",
      "795 loss: tensor(1.4963)\n",
      "train : 0.7282837391042981\n",
      "test : 0.4955929123125852\n",
      "796 loss: tensor(1.6758)\n",
      "train : 0.71746318004208\n",
      "test : 0.4716946842344389\n",
      "797 loss: tensor(1.7050)\n",
      "train : 0.7041378619376816\n",
      "test : 0.4756928668786915\n",
      "798 loss: tensor(1.7683)\n",
      "train : 0.7737701633102896\n",
      "test : 0.4990458882326215\n",
      "799 loss: tensor(1.5108)\n",
      "train : 0.8137461176234846\n",
      "test : 0.5281235801908224\n",
      "800 loss: tensor(1.4506)\n",
      "train : 0.8542230237451157\n",
      "test : 0.5422989550204452\n",
      "801 loss: tensor(1.3349)\n",
      "train : 0.8740607153591824\n",
      "test : 0.5607451158564289\n",
      "802 loss: tensor(1.3027)\n",
      "train : 0.8775673780182347\n",
      "test : 0.5618355293048615\n",
      "803 loss: tensor(1.2751)\n",
      "train : 0.8853822262298366\n",
      "test : 0.567014993184916\n",
      "804 loss: tensor(1.2652)\n",
      "train : 0.8855826069532111\n",
      "test : 0.5669241253975466\n",
      "805 loss: tensor(1.2580)\n",
      "train : 0.8838793708045286\n",
      "test : 0.5666515220354384\n",
      "806 loss: tensor(1.2558)\n",
      "train : 0.8829776575493438\n",
      "test : 0.5664697864606997\n",
      "807 loss: tensor(1.2571)\n",
      "train : 0.8805730888688508\n",
      "test : 0.5619263970922308\n",
      "808 loss: tensor(1.2616)\n",
      "train : 0.8747620478909929\n",
      "test : 0.5633802816901409\n",
      "809 loss: tensor(1.2710)\n",
      "train : 0.8664462478709548\n",
      "test : 0.5537482962289868\n",
      "810 loss: tensor(1.2865)\n",
      "train : 0.8581304478509167\n",
      "test : 0.5577464788732395\n",
      "811 loss: tensor(1.3074)\n",
      "train : 0.8380923755134756\n",
      "test : 0.5449341208541572\n",
      "812 loss: tensor(1.3409)\n",
      "train : 0.8313796212804329\n",
      "test : 0.5407542026351658\n",
      "813 loss: tensor(1.3722)\n",
      "train : 0.8043282236248873\n",
      "test : 0.5227623807360291\n",
      "814 loss: tensor(1.4294)\n",
      "train : 0.8025247971145176\n",
      "test : 0.5291231258518855\n",
      "815 loss: tensor(1.4344)\n",
      "train : 0.7849914838192565\n",
      "test : 0.5120399818264425\n",
      "816 loss: tensor(1.4876)\n",
      "train : 0.8144474501552951\n",
      "test : 0.5296683325761018\n",
      "817 loss: tensor(1.4046)\n",
      "train : 0.8152489730487927\n",
      "test : 0.5269422989550204\n",
      "818 loss: tensor(1.4103)\n",
      "train : 0.8507163610860635\n",
      "test : 0.5442071785552022\n",
      "819 loss: tensor(1.3185)\n",
      "train : 0.8531209297665565\n",
      "test : 0.5498409813721036\n",
      "820 loss: tensor(1.3129)\n",
      "train : 0.8695521490832582\n",
      "test : 0.5560199909132213\n",
      "821 loss: tensor(1.2792)\n",
      "train : 0.8696523394449454\n",
      "test : 0.5551113130395275\n",
      "822 loss: tensor(1.2801)\n",
      "train : 0.8687506261897605\n",
      "test : 0.5548387096774193\n",
      "823 loss: tensor(1.2832)\n",
      "train : 0.8604348261697224\n",
      "test : 0.5474784189004998\n",
      "824 loss: tensor(1.2951)\n",
      "train : 0.8507163610860635\n",
      "test : 0.5479327578373466\n",
      "825 loss: tensor(1.3167)\n",
      "train : 0.8392946598537221\n",
      "test : 0.5350295320308951\n",
      "826 loss: tensor(1.3435)\n",
      "train : 0.8245666766857028\n",
      "test : 0.5358473421172194\n",
      "827 loss: tensor(1.3682)\n",
      "train : 0.8150485923254183\n",
      "test : 0.5225806451612903\n",
      "828 loss: tensor(1.4070)\n",
      "train : 0.813445546538423\n",
      "test : 0.5289413902771467\n",
      "829 loss: tensor(1.3990)\n",
      "train : 0.8139464983468591\n",
      "test : 0.5227623807360291\n",
      "830 loss: tensor(1.4176)\n",
      "train : 0.8329826670674281\n",
      "test : 0.5344843253066788\n",
      "831 loss: tensor(1.3549)\n",
      "train : 0.844103797214708\n",
      "test : 0.5350295320308951\n",
      "832 loss: tensor(1.3398)\n",
      "train : 0.8658451057008316\n",
      "test : 0.5512948659700136\n",
      "833 loss: tensor(1.2820)\n",
      "train : 0.8764652840396754\n",
      "test : 0.5533848250795094\n",
      "834 loss: tensor(1.2589)\n",
      "train : 0.886484320208396\n",
      "test : 0.5646524307133122\n",
      "835 loss: tensor(1.2353)\n",
      "train : 0.8915940286544435\n",
      "test : 0.5635620172648796\n",
      "836 loss: tensor(1.2189)\n",
      "train : 0.8949003105901212\n",
      "test : 0.5688323489323035\n",
      "837 loss: tensor(1.2146)\n",
      "train : 0.8967037371004909\n",
      "test : 0.5659245797364835\n",
      "838 loss: tensor(1.2061)\n",
      "train : 0.8944995491433724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.571830985915493\n",
      "839 loss: tensor(1.2098)\n",
      "train : 0.894800120228434\n",
      "test : 0.5661063153112221\n",
      "840 loss: tensor(1.2056)\n",
      "train : 0.8905921250375713\n",
      "test : 0.5676510676965016\n",
      "841 loss: tensor(1.2149)\n",
      "train : 0.890391744314197\n",
      "test : 0.562925942753294\n",
      "842 loss: tensor(1.2128)\n",
      "train : 0.883578799719467\n",
      "test : 0.565106769650159\n",
      "843 loss: tensor(1.2277)\n",
      "train : 0.8811742310389741\n",
      "test : 0.5568378009995456\n",
      "844 loss: tensor(1.2261)\n",
      "train : 0.8756637611461777\n",
      "test : 0.5596547024079964\n",
      "845 loss: tensor(1.2458)\n",
      "train : 0.8725578599338744\n",
      "test : 0.5495683780099955\n",
      "846 loss: tensor(1.2440)\n",
      "train : 0.8647430117222723\n",
      "test : 0.5552021808268969\n",
      "847 loss: tensor(1.2661)\n",
      "train : 0.8643422502755235\n",
      "test : 0.5447523852794185\n",
      "848 loss: tensor(1.2617)\n",
      "train : 0.8590321611061016\n",
      "test : 0.5518400726942299\n",
      "849 loss: tensor(1.2778)\n",
      "train : 0.8618374912333433\n",
      "test : 0.5414811449341208\n",
      "850 loss: tensor(1.2698)\n",
      "train : 0.8617373008716561\n",
      "test : 0.5502953203089505\n",
      "851 loss: tensor(1.2723)\n",
      "train : 0.8645426309988979\n",
      "test : 0.5439345751930941\n",
      "852 loss: tensor(1.2633)\n",
      "train : 0.8719567177637512\n",
      "test : 0.5498409813721036\n",
      "853 loss: tensor(1.2524)\n",
      "train : 0.8723574792105\n",
      "test : 0.548750567923671\n",
      "854 loss: tensor(1.2489)\n",
      "train : 0.8776675683799219\n",
      "test : 0.5546569741026806\n",
      "855 loss: tensor(1.2387)\n",
      "train : 0.8717563370403767\n",
      "test : 0.5495683780099955\n",
      "856 loss: tensor(1.2469)\n",
      "train : 0.8648432020839595\n",
      "test : 0.5498409813721036\n",
      "857 loss: tensor(1.2556)\n",
      "train : 0.8546237851918645\n",
      "test : 0.538391640163562\n",
      "858 loss: tensor(1.2848)\n",
      "train : 0.8328824767057409\n",
      "test : 0.5353930031803725\n",
      "859 loss: tensor(1.3279)\n",
      "train : 0.8089369802624987\n",
      "test : 0.5167651067696502\n",
      "860 loss: tensor(1.3921)\n",
      "train : 0.7821861536920148\n",
      "test : 0.5110404361653794\n",
      "861 loss: tensor(1.4652)\n",
      "train : 0.7573389439935878\n",
      "test : 0.4867787369377556\n",
      "862 loss: tensor(1.5458)\n",
      "train : 0.7439134355275022\n",
      "test : 0.49304861426624264\n",
      "863 loss: tensor(1.5835)\n",
      "train : 0.7388037270814548\n",
      "test : 0.4736937755565652\n",
      "864 loss: tensor(1.6035)\n",
      "train : 0.7501252379521091\n",
      "test : 0.5024079963652885\n",
      "865 loss: tensor(1.5700)\n",
      "train : 0.7661556958220619\n",
      "test : 0.4802362562471604\n",
      "866 loss: tensor(1.5260)\n",
      "train : 0.7902013826269912\n",
      "test : 0.5194911403907315\n",
      "867 loss: tensor(1.4600)\n",
      "train : 0.8206592525799018\n",
      "test : 0.5073148568832349\n",
      "868 loss: tensor(1.3966)\n",
      "train : 0.8405971345556558\n",
      "test : 0.5429350295320309\n",
      "869 loss: tensor(1.3434)\n",
      "train : 0.866045486424206\n",
      "test : 0.5358473421172194\n",
      "870 loss: tensor(1.2955)\n",
      "train : 0.876765855124737\n",
      "test : 0.5631985461154021\n",
      "871 loss: tensor(1.2504)\n",
      "train : 0.8976054503556757\n",
      "test : 0.5575647432985007\n",
      "872 loss: tensor(1.2217)\n",
      "train : 0.9014126840997896\n",
      "test : 0.5712857791912767\n",
      "873 loss: tensor(1.1943)\n",
      "train : 0.9136359082256287\n",
      "test : 0.5663789186733303\n",
      "874 loss: tensor(1.1798)\n",
      "train : 0.9107303877366997\n",
      "test : 0.5756474329850069\n",
      "875 loss: tensor(1.1670)\n",
      "train : 0.9214507564372307\n",
      "test : 0.5701953657428441\n",
      "876 loss: tensor(1.1603)\n",
      "train : 0.9153391443743112\n",
      "test : 0.5765561108587006\n",
      "877 loss: tensor(1.1539)\n",
      "train : 0.9229536118625388\n",
      "test : 0.5729213993639255\n",
      "878 loss: tensor(1.1509)\n",
      "train : 0.917242761246368\n",
      "test : 0.5767378464334394\n",
      "879 loss: tensor(1.1476)\n",
      "train : 0.9237551347560364\n",
      "test : 0.5727396637891867\n",
      "880 loss: tensor(1.1469)\n",
      "train : 0.9164412383528705\n",
      "test : 0.5772830531576556\n",
      "881 loss: tensor(1.1453)\n",
      "train : 0.9221520889690412\n",
      "test : 0.5726487960018174\n",
      "882 loss: tensor(1.1466)\n",
      "train : 0.9130347660555055\n",
      "test : 0.5751022262607905\n",
      "883 loss: tensor(1.1468)\n",
      "train : 0.9194469492034866\n",
      "test : 0.5697410268059973\n",
      "884 loss: tensor(1.1510)\n",
      "train : 0.9080252479711451\n",
      "test : 0.5744661517492049\n",
      "885 loss: tensor(1.1533)\n",
      "train : 0.9134355275022543\n",
      "test : 0.5641072239890959\n",
      "886 loss: tensor(1.1621)\n",
      "train : 0.9020138262699129\n",
      "test : 0.5697410268059973\n",
      "887 loss: tensor(1.1676)\n",
      "train : 0.9020138262699129\n",
      "test : 0.5562017264879601\n",
      "888 loss: tensor(1.1844)\n",
      "train : 0.8884881274421401\n",
      "test : 0.5634711494775102\n",
      "889 loss: tensor(1.1945)\n",
      "train : 0.883578799719467\n",
      "test : 0.5490231712857792\n",
      "890 loss: tensor(1.2247)\n",
      "train : 0.8659452960625188\n",
      "test : 0.55474784189005\n",
      "891 loss: tensor(1.2414)\n",
      "train : 0.8583308285742911\n",
      "test : 0.5293048614266243\n",
      "892 loss: tensor(1.2928)\n",
      "train : 0.8356878068329827\n",
      "test : 0.5398455247614721\n",
      "893 loss: tensor(1.3111)\n",
      "train : 0.8243662959623285\n",
      "test : 0.5122217174011813\n",
      "894 loss: tensor(1.3775)\n",
      "train : 0.8132451658150486\n",
      "test : 0.5251249432076329\n",
      "895 loss: tensor(1.3781)\n",
      "train : 0.8120428814748021\n",
      "test : 0.5041344843253067\n",
      "896 loss: tensor(1.4064)\n",
      "train : 0.8212603947500251\n",
      "test : 0.5259427532939573\n",
      "897 loss: tensor(1.3670)\n",
      "train : 0.8539224526600541\n",
      "test : 0.5263970922308042\n",
      "898 loss: tensor(1.3045)\n",
      "train : 0.8721570984871255\n",
      "test : 0.5501135847342117\n",
      "899 loss: tensor(1.2429)\n",
      "train : 0.9052199178439034\n",
      "test : 0.5596547024079964\n",
      "900 loss: tensor(1.1874)\n",
      "train : 0.9105300070133253\n",
      "test : 0.5672875965470241\n",
      "901 loss: tensor(1.1625)\n",
      "train : 0.9189459973950506\n",
      "test : 0.5721944570649705\n",
      "902 loss: tensor(1.1465)\n",
      "train : 0.918645426309989\n",
      "test : 0.570649704679691\n",
      "903 loss: tensor(1.1412)\n",
      "train : 0.9210499949904819\n",
      "test : 0.5764652430713312\n",
      "904 loss: tensor(1.1367)\n",
      "train : 0.9185452359483017\n",
      "test : 0.5690140845070423\n",
      "905 loss: tensor(1.1368)\n",
      "train : 0.9189459973950506\n",
      "test : 0.577373920945025\n",
      "906 loss: tensor(1.1360)\n",
      "train : 0.9155395250976857\n",
      "test : 0.5673784643343934\n",
      "907 loss: tensor(1.1395)\n",
      "train : 0.9141368600340647\n",
      "test : 0.574738755111313\n",
      "908 loss: tensor(1.1422)\n",
      "train : 0.9105300070133253\n",
      "test : 0.5620172648796001\n",
      "909 loss: tensor(1.1501)\n",
      "train : 0.9060214407374011\n",
      "test : 0.569195820081781\n",
      "910 loss: tensor(1.1573)\n",
      "train : 0.8980062118024246\n",
      "test : 0.5557473875511131\n",
      "911 loss: tensor(1.1738)\n",
      "train : 0.890391744314197\n",
      "test : 0.561744661517492\n",
      "912 loss: tensor(1.1902)\n",
      "train : 0.8712553852319407\n",
      "test : 0.5432985006815084\n",
      "913 loss: tensor(1.2265)\n",
      "train : 0.8554253080853622\n",
      "test : 0.5412085415720127\n",
      "914 loss: tensor(1.2682)\n",
      "train : 0.8138463079851719\n",
      "test : 0.5161290322580645\n",
      "915 loss: tensor(1.3631)\n",
      "train : 0.778378919947901\n",
      "test : 0.49613811903680144\n",
      "916 loss: tensor(1.4863)\n",
      "train : 0.7050395751928664\n",
      "test : 0.4607905497501136\n",
      "917 loss: tensor(1.7598)\n",
      "train : 0.6680693317302876\n",
      "test : 0.4338936846887778\n",
      "918 loss: tensor(1.9025)\n",
      "train : 0.6937180643222122\n",
      "test : 0.4629713766469786\n",
      "919 loss: tensor(1.8642)\n",
      "train : 0.7787796813946498\n",
      "test : 0.4813266696955929\n",
      "920 loss: tensor(1.4947)\n",
      "train : 0.8269712453661958\n",
      "test : 0.5414811449341208\n",
      "921 loss: tensor(1.3599)\n",
      "train : 0.8494138863841298\n",
      "test : 0.5257610177192186\n",
      "922 loss: tensor(1.3119)\n",
      "train : 0.8590321611061016\n",
      "test : 0.5555656519763744\n",
      "923 loss: tensor(1.2820)\n",
      "train : 0.886484320208396\n",
      "test : 0.5428441617446615\n",
      "924 loss: tensor(1.2329)\n",
      "train : 0.9005109708446047\n",
      "test : 0.572557928214448\n",
      "925 loss: tensor(1.1941)\n",
      "train : 0.9183448552249274\n",
      "test : 0.5635620172648796\n",
      "926 loss: tensor(1.1564)\n",
      "train : 0.9278629395852119\n",
      "test : 0.5836437982735121\n",
      "927 loss: tensor(1.1318)\n",
      "train : 0.9333734094780082\n",
      "test : 0.5760109041344843\n",
      "928 loss: tensor(1.1150)\n",
      "train : 0.9377817853922452\n",
      "test : 0.585097682871422\n",
      "929 loss: tensor(1.1047)\n",
      "train : 0.9382827372006813\n",
      "test : 0.5783734666060881\n",
      "930 loss: tensor(1.0976)\n",
      "train : 0.9390842600941789\n",
      "test : 0.5864606996819628\n",
      "931 loss: tensor(1.0927)\n",
      "train : 0.9402865444344254\n",
      "test : 0.5804634257155838\n",
      "932 loss: tensor(1.0889)\n",
      "train : 0.9403867347961126\n",
      "test : 0.5859154929577465\n",
      "933 loss: tensor(1.0859)\n",
      "train : 0.9401863540727382\n",
      "test : 0.5811903680145388\n",
      "934 loss: tensor(1.0834)\n",
      "train : 0.9420899709447951\n",
      "test : 0.5858246251703771\n",
      "935 loss: tensor(1.0813)\n",
      "train : 0.9401863540727382\n",
      "test : 0.5811903680145388\n",
      "936 loss: tensor(1.0795)\n",
      "train : 0.9427913034766056\n",
      "test : 0.5867333030440709\n",
      "937 loss: tensor(1.0780)\n",
      "train : 0.9406873058811742\n",
      "test : 0.5817355747387551\n",
      "938 loss: tensor(1.0767)\n",
      "train : 0.9432922552850416\n",
      "test : 0.5864606996819628\n",
      "939 loss: tensor(1.0758)\n",
      "train : 0.940086163711051\n",
      "test : 0.5811903680145388\n",
      "940 loss: tensor(1.0750)\n",
      "train : 0.9432922552850416\n",
      "test : 0.5864606996819628\n",
      "941 loss: tensor(1.0749)\n",
      "train : 0.9384831179240557\n",
      "test : 0.5809177646524307\n",
      "942 loss: tensor(1.0750)\n",
      "train : 0.9399859733493638\n",
      "test : 0.5852794184461608\n",
      "943 loss: tensor(1.0763)\n",
      "train : 0.9373810239454965\n",
      "test : 0.5805542935029532\n",
      "944 loss: tensor(1.0779)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 0.9347760745416291\n",
      "test : 0.5843707405724671\n",
      "945 loss: tensor(1.0822)\n",
      "train : 0.932271315499449\n",
      "test : 0.5777373920945025\n",
      "946 loss: tensor(1.0873)\n",
      "train : 0.9271616070534014\n",
      "test : 0.57664697864607\n",
      "947 loss: tensor(1.0989)\n",
      "train : 0.9179440937781785\n",
      "test : 0.5689232167196728\n",
      "948 loss: tensor(1.1123)\n",
      "train : 0.9052199178439034\n",
      "test : 0.5657428441617447\n",
      "949 loss: tensor(1.1429)\n",
      "train : 0.887486223825268\n",
      "test : 0.5517492049068605\n",
      "950 loss: tensor(1.1813)\n",
      "train : 0.845606652640016\n",
      "test : 0.5378464334393458\n",
      "951 loss: tensor(1.2784)\n",
      "train : 0.7903015729886785\n",
      "test : 0.4918673330304407\n",
      "952 loss: tensor(1.4357)\n",
      "train : 0.6661657148582306\n",
      "test : 0.4391640163562017\n",
      "953 loss: tensor(1.8985)\n",
      "train : 0.587917042380523\n",
      "test : 0.3755565651976374\n",
      "954 loss: tensor(2.4654)\n",
      "train : 0.6532411582005812\n",
      "test : 0.42289868241708317\n",
      "955 loss: tensor(2.1195)\n",
      "train : 0.8171525899208496\n",
      "test : 0.5221263062244434\n",
      "956 loss: tensor(1.4054)\n",
      "train : 0.8769662358481114\n",
      "test : 0.5473875511131304\n",
      "957 loss: tensor(1.2706)\n",
      "train : 0.8994088768660455\n",
      "test : 0.5649250340754203\n",
      "958 loss: tensor(1.2004)\n",
      "train : 0.9237551347560364\n",
      "test : 0.5702862335302136\n",
      "959 loss: tensor(1.1518)\n",
      "train : 0.9334735998396955\n",
      "test : 0.5814629713766469\n",
      "960 loss: tensor(1.1215)\n",
      "train : 0.9413886384129847\n",
      "test : 0.5808268968650613\n",
      "961 loss: tensor(1.1030)\n",
      "train : 0.9420899709447951\n",
      "test : 0.5854611540208996\n",
      "962 loss: tensor(1.0907)\n",
      "train : 0.9453962528804729\n",
      "test : 0.5842798727850976\n",
      "963 loss: tensor(1.0822)\n",
      "train : 0.9450956817954113\n",
      "test : 0.5865515674693321\n",
      "964 loss: tensor(1.0757)\n",
      "train : 0.947400060114217\n",
      "test : 0.5847342117219446\n",
      "965 loss: tensor(1.0706)\n",
      "train : 0.9470994890291554\n",
      "test : 0.586278964107224\n",
      "966 loss: tensor(1.0665)\n",
      "train : 0.9485021540927763\n",
      "test : 0.5851885506587915\n",
      "967 loss: tensor(1.0630)\n",
      "train : 0.9477006311992786\n",
      "test : 0.5868241708314402\n",
      "968 loss: tensor(1.0600)\n",
      "train : 0.9484019637310891\n",
      "test : 0.5847342117219446\n",
      "969 loss: tensor(1.0574)\n",
      "train : 0.9489029155395251\n",
      "test : 0.587005906406179\n",
      "970 loss: tensor(1.0551)\n",
      "train : 0.9492034866245868\n",
      "test : 0.5853702862335303\n",
      "971 loss: tensor(1.0531)\n",
      "train : 0.9495040577096483\n",
      "test : 0.5860063607451158\n",
      "972 loss: tensor(1.0513)\n",
      "train : 0.94980462879471\n",
      "test : 0.5854611540208996\n",
      "973 loss: tensor(1.0498)\n",
      "train : 0.9496042480713356\n",
      "test : 0.5861880963198546\n",
      "974 loss: tensor(1.0486)\n",
      "train : 0.9499048191563971\n",
      "test : 0.5835529304861427\n",
      "975 loss: tensor(1.0477)\n",
      "train : 0.9502053902414588\n",
      "test : 0.5865515674693321\n",
      "976 loss: tensor(1.0473)\n",
      "train : 0.9496042480713356\n",
      "test : 0.5826442526124489\n",
      "977 loss: tensor(1.0476)\n",
      "train : 0.9485021540927763\n",
      "test : 0.5845524761472058\n",
      "978 loss: tensor(1.0490)\n",
      "train : 0.9455966336038473\n",
      "test : 0.5782825988187188\n",
      "979 loss: tensor(1.0518)\n",
      "train : 0.9440937781785392\n",
      "test : 0.5823716492503408\n",
      "980 loss: tensor(1.0580)\n",
      "train : 0.9390842600941789\n",
      "test : 0.573284870513403\n",
      "981 loss: tensor(1.0677)\n",
      "train : 0.9271616070534014\n",
      "test : 0.5796456156292594\n",
      "982 loss: tensor(1.0887)\n",
      "train : 0.9139364793106903\n",
      "test : 0.5587460245343026\n",
      "983 loss: tensor(1.1202)\n",
      "train : 0.8815749924857229\n",
      "test : 0.5620172648796001\n",
      "984 loss: tensor(1.1941)\n",
      "train : 0.8412984670874661\n",
      "test : 0.523943661971831\n",
      "985 loss: tensor(1.2971)\n",
      "train : 0.7542330427812844\n",
      "test : 0.49586551567469334\n",
      "986 loss: tensor(1.5719)\n",
      "train : 0.677086464282136\n",
      "test : 0.4368923216719673\n",
      "987 loss: tensor(1.8497)\n",
      "train : 0.564572688107404\n",
      "test : 0.38064516129032255\n",
      "988 loss: tensor(2.4949)\n",
      "train : 0.6189760545035567\n",
      "test : 0.39409359382099046\n",
      "989 loss: tensor(2.1582)\n",
      "train : 0.721871555956317\n",
      "test : 0.4636074511585643\n",
      "990 loss: tensor(1.7331)\n",
      "train : 0.8213605851117123\n",
      "test : 0.5134938664243526\n",
      "991 loss: tensor(1.3612)\n",
      "train : 0.8395952309387837\n",
      "test : 0.5351203998182644\n",
      "992 loss: tensor(1.3342)\n",
      "train : 0.891393647931069\n",
      "test : 0.5520218082689686\n",
      "993 loss: tensor(1.2151)\n",
      "train : 0.9248572287345957\n",
      "test : 0.5713766469786461\n",
      "994 loss: tensor(1.1447)\n",
      "train : 0.9408876866045487\n",
      "test : 0.5795547478418901\n",
      "995 loss: tensor(1.1052)\n",
      "train : 0.9483017733694019\n",
      "test : 0.5845524761472058\n",
      "996 loss: tensor(1.0848)\n",
      "train : 0.951307484220018\n",
      "test : 0.5838255338482508\n",
      "997 loss: tensor(1.0729)\n",
      "train : 0.9523093878368901\n",
      "test : 0.5873693775556565\n",
      "998 loss: tensor(1.0647)\n",
      "train : 0.9531109107303878\n",
      "test : 0.5841890049977283\n",
      "999 loss: tensor(1.0583)\n",
      "train : 0.9535116721771365\n",
      "test : 0.5879145842798728\n",
      "1000 loss: tensor(1.0531)\n",
      "train : 0.9539124336238853\n",
      "test : 0.5841890049977283\n",
      "1001 loss: tensor(1.0487)\n",
      "train : 0.9548141468790702\n",
      "test : 0.5880054520672422\n",
      "1002 loss: tensor(1.0450)\n",
      "train : 0.954213004708947\n",
      "test : 0.5844616083598364\n",
      "1003 loss: tensor(1.0417)\n",
      "train : 0.9551147179641318\n",
      "test : 0.587732848705134\n",
      "1004 loss: tensor(1.0388)\n",
      "train : 0.9557158601342551\n",
      "test : 0.5845524761472058\n",
      "1005 loss: tensor(1.0363)\n",
      "train : 0.9558160504959423\n",
      "test : 0.5876419809177647\n",
      "1006 loss: tensor(1.0339)\n",
      "train : 0.9562168119426911\n",
      "test : 0.5841890049977283\n",
      "1007 loss: tensor(1.0319)\n",
      "train : 0.9561166215810039\n",
      "test : 0.5875511131303953\n",
      "1008 loss: tensor(1.0299)\n",
      "train : 0.9564171926660655\n",
      "test : 0.5840072694229895\n",
      "1009 loss: tensor(1.0282)\n",
      "train : 0.9565173830277527\n",
      "test : 0.587732848705134\n",
      "1010 loss: tensor(1.0266)\n",
      "train : 0.9569181444745015\n",
      "test : 0.5838255338482508\n",
      "1011 loss: tensor(1.0251)\n",
      "train : 0.95661757338944\n",
      "test : 0.5884597910040891\n",
      "1012 loss: tensor(1.0238)\n",
      "train : 0.957118525197876\n",
      "test : 0.5841890049977283\n",
      "1013 loss: tensor(1.0226)\n",
      "train : 0.95661757338944\n",
      "test : 0.5878237164925034\n",
      "1014 loss: tensor(1.0216)\n",
      "train : 0.9573189059212504\n",
      "test : 0.5841890049977283\n",
      "1015 loss: tensor(1.0207)\n",
      "train : 0.9567177637511272\n",
      "test : 0.5880054520672422\n",
      "1016 loss: tensor(1.0201)\n",
      "train : 0.957118525197876\n",
      "test : 0.5832803271240345\n",
      "1017 loss: tensor(1.0198)\n",
      "train : 0.9564171926660655\n",
      "test : 0.587005906406179\n",
      "1018 loss: tensor(1.0201)\n",
      "train : 0.9558160504959423\n",
      "test : 0.5817355747387551\n",
      "1019 loss: tensor(1.0209)\n",
      "train : 0.9538122432621982\n",
      "test : 0.5885506587914584\n",
      "1020 loss: tensor(1.0228)\n",
      "train : 0.9530107203687005\n",
      "test : 0.5791004089050431\n",
      "1021 loss: tensor(1.0260)\n",
      "train : 0.9480012022843403\n",
      "test : 0.5845524761472058\n",
      "1022 loss: tensor(1.0319)\n",
      "train : 0.9444945396252881\n",
      "test : 0.5743752839618356\n",
      "1023 loss: tensor(1.0402)\n",
      "train : 0.935677787796814\n",
      "test : 0.577373920945025\n",
      "1024 loss: tensor(1.0568)\n",
      "train : 0.9242560865644724\n",
      "test : 0.5605633802816902\n",
      "1025 loss: tensor(1.0795)\n",
      "train : 0.9025147780783489\n",
      "test : 0.5651976374375284\n",
      "1026 loss: tensor(1.1287)\n",
      "train : 0.870453862338443\n",
      "test : 0.5338482507950931\n",
      "1027 loss: tensor(1.2007)\n",
      "train : 0.8185552549844705\n",
      "test : 0.5275783734666061\n",
      "1028 loss: tensor(1.3520)\n",
      "train : 0.7554353271215309\n",
      "test : 0.47096774193548385\n",
      "1029 loss: tensor(1.6043)\n",
      "train : 0.7100490932772268\n",
      "test : 0.4711494775102226\n",
      "1030 loss: tensor(1.7895)\n",
      "train : 0.7172627993187055\n",
      "test : 0.4460699681962744\n",
      "1031 loss: tensor(1.7726)\n",
      "train : 0.7338943993587816\n",
      "test : 0.4851431167651068\n",
      "1032 loss: tensor(1.6480)\n",
      "train : 0.8204588718565274\n",
      "test : 0.497955474784189\n",
      "1033 loss: tensor(1.3769)\n",
      "train : 0.8748622382526801\n",
      "test : 0.5541117673784643\n",
      "1034 loss: tensor(1.2564)\n",
      "train : 0.9262598937982166\n",
      "test : 0.5633802816901409\n",
      "1035 loss: tensor(1.1174)\n",
      "train : 0.9426911131149184\n",
      "test : 0.5812812358019083\n",
      "1036 loss: tensor(1.0795)\n",
      "train : 0.9533112914537621\n",
      "test : 0.5824625170377101\n",
      "1037 loss: tensor(1.0495)\n",
      "train : 0.9588217613465585\n",
      "test : 0.5854611540208996\n",
      "1038 loss: tensor(1.0361)\n",
      "train : 0.9614267107504259\n",
      "test : 0.587732848705134\n",
      "1039 loss: tensor(1.0274)\n",
      "train : 0.9621280432822362\n",
      "test : 0.5874602453430259\n",
      "1040 loss: tensor(1.0214)\n",
      "train : 0.9619276625588619\n",
      "test : 0.587732848705134\n",
      "1041 loss: tensor(1.0171)\n",
      "train : 0.9626289950906722\n",
      "test : 0.587732848705134\n",
      "1042 loss: tensor(1.0135)\n",
      "train : 0.962528804728985\n",
      "test : 0.5870967741935483\n",
      "1043 loss: tensor(1.0106)\n",
      "train : 0.9629295661757339\n",
      "test : 0.5874602453430259\n",
      "1044 loss: tensor(1.0081)\n",
      "train : 0.9633303276224827\n",
      "test : 0.5870967741935483\n",
      "1045 loss: tensor(1.0059)\n",
      "train : 0.9636308987075444\n",
      "test : 0.5873693775556565\n",
      "1046 loss: tensor(1.0040)\n",
      "train : 0.9632301372607955\n",
      "test : 0.5879145842798728\n",
      "1047 loss: tensor(1.0023)\n",
      "train : 0.9641318505159804\n",
      "test : 0.5867333030440709\n",
      "1048 loss: tensor(1.0007)\n",
      "train : 0.9635307083458571\n",
      "test : 0.5880054520672422\n",
      "1049 loss: tensor(0.9993)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 0.9643322312393547\n",
      "test : 0.5868241708314402\n",
      "1050 loss: tensor(0.9981)\n",
      "train : 0.9642320408776676\n",
      "test : 0.5880963198546115\n",
      "1051 loss: tensor(0.9969)\n",
      "train : 0.9641318505159804\n",
      "test : 0.5867333030440709\n",
      "1052 loss: tensor(0.9959)\n",
      "train : 0.9646328023244164\n",
      "test : 0.5880054520672422\n",
      "1053 loss: tensor(0.9950)\n",
      "train : 0.9646328023244164\n",
      "test : 0.5861880963198546\n",
      "1054 loss: tensor(0.9942)\n",
      "train : 0.9640316601542932\n",
      "test : 0.5884597910040891\n",
      "1055 loss: tensor(0.9936)\n",
      "train : 0.9642320408776676\n",
      "test : 0.5861880963198546\n",
      "1056 loss: tensor(0.9933)\n",
      "train : 0.9639314697926059\n",
      "test : 0.5880054520672422\n",
      "1057 loss: tensor(0.9931)\n",
      "train : 0.9640316601542932\n",
      "test : 0.5847342117219446\n",
      "1058 loss: tensor(0.9935)\n",
      "train : 0.962528804728985\n",
      "test : 0.587005906406179\n",
      "1059 loss: tensor(0.9941)\n",
      "train : 0.962528804728985\n",
      "test : 0.5835529304861427\n",
      "1060 loss: tensor(0.9958)\n",
      "train : 0.9602244264101794\n",
      "test : 0.5854611540208996\n",
      "1061 loss: tensor(0.9980)\n",
      "train : 0.958120428814748\n",
      "test : 0.5794638800545207\n",
      "1062 loss: tensor(1.0030)\n",
      "train : 0.9540126239855726\n",
      "test : 0.5814629713766469\n",
      "1063 loss: tensor(1.0085)\n",
      "train : 0.9477006311992786\n",
      "test : 0.57664697864607\n",
      "1064 loss: tensor(1.0226)\n",
      "train : 0.9401863540727382\n",
      "test : 0.5743752839618356\n",
      "1065 loss: tensor(1.0382)\n",
      "train : 0.9159402865444344\n",
      "test : 0.5621081326669696\n",
      "1066 loss: tensor(1.0863)\n",
      "train : 0.8836789900811542\n",
      "test : 0.5489323034984098\n",
      "1067 loss: tensor(1.1552)\n",
      "train : 0.8167518284741008\n",
      "test : 0.5207632894139028\n",
      "1068 loss: tensor(1.4279)\n",
      "train : 0.7451157198677487\n",
      "test : 0.48850522489777376\n",
      "1069 loss: tensor(1.6094)\n",
      "train : 0.6978258691513877\n",
      "test : 0.4518855065879146\n",
      "1070 loss: tensor(2.0649)\n",
      "train : 0.6879070233443543\n",
      "test : 0.4497955474784189\n",
      "1071 loss: tensor(1.9431)\n",
      "train : 0.7219717463180042\n",
      "test : 0.466424352567015\n",
      "1072 loss: tensor(1.7750)\n",
      "train : 0.7390041078048292\n",
      "test : 0.46787823716492505\n",
      "1073 loss: tensor(1.5257)\n",
      "train : 0.7994188959022142\n",
      "test : 0.5207632894139028\n",
      "1074 loss: tensor(1.4418)\n",
      "train : 0.8750626189760545\n",
      "test : 0.5318491594729668\n",
      "1075 loss: tensor(1.2115)\n",
      "train : 0.931770363691013\n",
      "test : 0.5779191276692413\n",
      "1076 loss: tensor(1.1036)\n",
      "train : 0.9524095781985773\n",
      "test : 0.573284870513403\n",
      "1077 loss: tensor(1.0539)\n",
      "train : 0.9622282336439234\n",
      "test : 0.5896410722398909\n",
      "1078 loss: tensor(1.0282)\n",
      "train : 0.9645326119627292\n",
      "test : 0.5859154929577465\n",
      "1079 loss: tensor(1.0154)\n",
      "train : 0.9678388938984069\n",
      "test : 0.5897319400272604\n",
      "1080 loss: tensor(1.0069)\n",
      "train : 0.9670373710049093\n",
      "test : 0.5884597910040891\n",
      "1081 loss: tensor(1.0009)\n",
      "train : 0.968339845706843\n",
      "test : 0.5909132212630622\n",
      "1082 loss: tensor(0.9962)\n",
      "train : 0.9678388938984069\n",
      "test : 0.5894593366651522\n",
      "1083 loss: tensor(0.9925)\n",
      "train : 0.9684400360685302\n",
      "test : 0.5909132212630622\n",
      "1084 loss: tensor(0.9893)\n",
      "train : 0.9687406071535918\n",
      "test : 0.5884597910040891\n",
      "1085 loss: tensor(0.9865)\n",
      "train : 0.9684400360685302\n",
      "test : 0.5908223534756929\n",
      "1086 loss: tensor(0.9841)\n",
      "train : 0.9694419396854023\n",
      "test : 0.5883689232167196\n",
      "1087 loss: tensor(0.9820)\n",
      "train : 0.9689409878769663\n",
      "test : 0.5902771467514766\n",
      "1088 loss: tensor(0.9801)\n",
      "train : 0.9702434625788999\n",
      "test : 0.5890049977283053\n",
      "1089 loss: tensor(0.9784)\n",
      "train : 0.9692415589620279\n",
      "test : 0.5899136756019991\n",
      "1090 loss: tensor(0.9768)\n",
      "train : 0.9703436529405871\n",
      "test : 0.588914129940936\n",
      "1091 loss: tensor(0.9753)\n",
      "train : 0.9697425107704639\n",
      "test : 0.5893684688777828\n",
      "1092 loss: tensor(0.9740)\n",
      "train : 0.9707444143873359\n",
      "test : 0.5890049977283053\n",
      "1093 loss: tensor(0.9727)\n",
      "train : 0.9702434625788999\n",
      "test : 0.5893684688777828\n",
      "1094 loss: tensor(0.9714)\n",
      "train : 0.9710449854723976\n",
      "test : 0.5890958655156747\n",
      "1095 loss: tensor(0.9703)\n",
      "train : 0.9704438433022743\n",
      "test : 0.5894593366651522\n",
      "1096 loss: tensor(0.9692)\n",
      "train : 0.9711451758340848\n",
      "test : 0.5883689232167196\n",
      "1097 loss: tensor(0.9681)\n",
      "train : 0.9710449854723976\n",
      "test : 0.5893684688777828\n",
      "1098 loss: tensor(0.9671)\n",
      "train : 0.9715459372808336\n",
      "test : 0.5882780554293503\n",
      "1099 loss: tensor(0.9661)\n",
      "train : 0.971245366195772\n",
      "test : 0.5894593366651522\n",
      "1100 loss: tensor(0.9652)\n",
      "train : 0.9719466987275824\n",
      "test : 0.5888232621535665\n",
      "1101 loss: tensor(0.9643)\n",
      "train : 0.971746318004208\n",
      "test : 0.5894593366651522\n",
      "1102 loss: tensor(0.9634)\n",
      "train : 0.9721470794509568\n",
      "test : 0.589186733303044\n",
      "1103 loss: tensor(0.9625)\n",
      "train : 0.9723474601743313\n",
      "test : 0.589186733303044\n",
      "1104 loss: tensor(0.9617)\n",
      "train : 0.9725478408977056\n",
      "test : 0.5900045433893685\n",
      "1105 loss: tensor(0.9609)\n",
      "train : 0.9728484119827673\n",
      "test : 0.5890049977283053\n",
      "1106 loss: tensor(0.9601)\n",
      "train : 0.9729486023444545\n",
      "test : 0.5900954111767378\n",
      "1107 loss: tensor(0.9594)\n",
      "train : 0.9730487927061416\n",
      "test : 0.5888232621535665\n",
      "1108 loss: tensor(0.9587)\n",
      "train : 0.9732491734295161\n",
      "test : 0.5895502044525216\n",
      "1109 loss: tensor(0.9580)\n",
      "train : 0.9731489830678288\n",
      "test : 0.5885506587914584\n",
      "1110 loss: tensor(0.9574)\n",
      "train : 0.9732491734295161\n",
      "test : 0.588914129940936\n",
      "1111 loss: tensor(0.9568)\n",
      "train : 0.9730487927061416\n",
      "test : 0.588187187641981\n",
      "1112 loss: tensor(0.9564)\n",
      "train : 0.9731489830678288\n",
      "test : 0.587732848705134\n",
      "1113 loss: tensor(0.9560)\n",
      "train : 0.9727482216210801\n",
      "test : 0.5876419809177647\n",
      "1114 loss: tensor(0.9558)\n",
      "train : 0.9728484119827673\n",
      "test : 0.5871876419809178\n",
      "1115 loss: tensor(0.9559)\n",
      "train : 0.971746318004208\n",
      "test : 0.5883689232167196\n",
      "1116 loss: tensor(0.9562)\n",
      "train : 0.9713455565574591\n",
      "test : 0.5860063607451158\n",
      "1117 loss: tensor(0.9570)\n",
      "train : 0.971245366195772\n",
      "test : 0.5887323943661972\n",
      "1118 loss: tensor(0.9585)\n",
      "train : 0.9687406071535918\n",
      "test : 0.5836437982735121\n",
      "1119 loss: tensor(0.9607)\n",
      "train : 0.9671375613665966\n",
      "test : 0.5887323943661972\n",
      "1120 loss: tensor(0.9649)\n",
      "train : 0.9647329926861036\n",
      "test : 0.5799182189913675\n",
      "1121 loss: tensor(0.9705)\n",
      "train : 0.9575192866446248\n",
      "test : 0.5868241708314402\n",
      "1122 loss: tensor(0.9814)\n",
      "train : 0.9520088167518285\n",
      "test : 0.5724670604270786\n",
      "1123 loss: tensor(0.9971)\n",
      "train : 0.9354774070734395\n",
      "test : 0.577373920945025\n",
      "1124 loss: tensor(1.0294)\n",
      "train : 0.9135357178639415\n",
      "test : 0.5530213539300318\n",
      "1125 loss: tensor(1.0809)\n",
      "train : 0.8695521490832582\n",
      "test : 0.5529304861426624\n",
      "1126 loss: tensor(1.2018)\n",
      "train : 0.8078348862839395\n",
      "test : 0.5032258064516129\n",
      "1127 loss: tensor(1.4013)\n",
      "train : 0.7124536619577196\n",
      "test : 0.4700590640617901\n",
      "1128 loss: tensor(1.7566)\n",
      "train : 0.6355074641819457\n",
      "test : 0.40508859609268516\n",
      "1129 loss: tensor(2.3198)\n",
      "train : 0.6696723775172828\n",
      "test : 0.42498864152657884\n",
      "1130 loss: tensor(1.8560)\n",
      "train : 0.7466185752930569\n",
      "test : 0.47060427078600636\n",
      "1131 loss: tensor(1.5749)\n",
      "train : 0.8131449754533614\n",
      "test : 0.48923216719672874\n",
      "1132 loss: tensor(1.4075)\n",
      "train : 0.8758641418695522\n",
      "test : 0.5411176737846434\n",
      "1133 loss: tensor(1.2212)\n",
      "train : 0.9301673179040176\n",
      "test : 0.5627442071785552\n",
      "1134 loss: tensor(1.1018)\n",
      "train : 0.9601242360484922\n",
      "test : 0.5838255338482508\n",
      "1135 loss: tensor(1.0256)\n",
      "train : 0.9696423204087766\n",
      "test : 0.5941844616083598\n",
      "1136 loss: tensor(0.9974)\n",
      "train : 0.9729486023444545\n",
      "test : 0.5912766924125398\n",
      "1137 loss: tensor(0.9864)\n",
      "train : 0.9745516481314498\n",
      "test : 0.5940935938209905\n",
      "1138 loss: tensor(0.9792)\n",
      "train : 0.975152790301573\n",
      "test : 0.5924579736483416\n",
      "1139 loss: tensor(0.9738)\n",
      "train : 0.9758541228333835\n",
      "test : 0.5931849159472967\n",
      "1140 loss: tensor(0.9692)\n",
      "train : 0.976154693918445\n",
      "test : 0.5931849159472967\n",
      "1141 loss: tensor(0.9654)\n",
      "train : 0.9766556457268811\n",
      "test : 0.5931849159472967\n",
      "1142 loss: tensor(0.9621)\n",
      "train : 0.9766556457268811\n",
      "test : 0.5928214447978192\n",
      "1143 loss: tensor(0.9592)\n",
      "train : 0.9765554553651938\n",
      "test : 0.5926397092230804\n",
      "1144 loss: tensor(0.9567)\n",
      "train : 0.9767558360885683\n",
      "test : 0.5923671058609723\n",
      "1145 loss: tensor(0.9544)\n",
      "train : 0.9765554553651938\n",
      "test : 0.5925488414357111\n",
      "1146 loss: tensor(0.9524)\n",
      "train : 0.9766556457268811\n",
      "test : 0.5929123125851885\n",
      "1147 loss: tensor(0.9506)\n",
      "train : 0.9766556457268811\n",
      "test : 0.593003180372558\n",
      "1148 loss: tensor(0.9489)\n",
      "train : 0.9765554553651938\n",
      "test : 0.593275783734666\n",
      "1149 loss: tensor(0.9474)\n",
      "train : 0.9764552650035067\n",
      "test : 0.5929123125851885\n",
      "1150 loss: tensor(0.9460)\n",
      "train : 0.9765554553651938\n",
      "test : 0.593003180372558\n",
      "1151 loss: tensor(0.9447)\n",
      "train : 0.9765554553651938\n",
      "test : 0.5927305770104498\n",
      "1152 loss: tensor(0.9435)\n",
      "train : 0.9768560264502555\n",
      "test : 0.5927305770104498\n",
      "1153 loss: tensor(0.9423)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 0.9766556457268811\n",
      "test : 0.5922762380736029\n",
      "1154 loss: tensor(0.9412)\n",
      "train : 0.9769562168119427\n",
      "test : 0.5920036347114948\n",
      "1155 loss: tensor(0.9402)\n",
      "train : 0.9767558360885683\n",
      "test : 0.5929123125851885\n",
      "1156 loss: tensor(0.9392)\n",
      "train : 0.9770564071736298\n",
      "test : 0.5925488414357111\n",
      "1157 loss: tensor(0.9382)\n",
      "train : 0.9771565975353171\n",
      "test : 0.5925488414357111\n",
      "1158 loss: tensor(0.9373)\n",
      "train : 0.9774571686203787\n",
      "test : 0.5923671058609723\n",
      "1159 loss: tensor(0.9365)\n",
      "train : 0.9777577397054403\n",
      "test : 0.5928214447978192\n",
      "1160 loss: tensor(0.9356)\n",
      "train : 0.9777577397054403\n",
      "test : 0.5928214447978192\n",
      "1161 loss: tensor(0.9348)\n",
      "train : 0.9779581204288148\n",
      "test : 0.5927305770104498\n",
      "1162 loss: tensor(0.9340)\n",
      "train : 0.9778579300671275\n",
      "test : 0.5929123125851885\n",
      "1163 loss: tensor(0.9333)\n",
      "train : 0.9783588818755635\n",
      "test : 0.593003180372558\n",
      "1164 loss: tensor(0.9325)\n",
      "train : 0.978559262598938\n",
      "test : 0.5925488414357111\n",
      "1165 loss: tensor(0.9318)\n",
      "train : 0.9787596433223124\n",
      "test : 0.5926397092230804\n",
      "1166 loss: tensor(0.9311)\n",
      "train : 0.9787596433223124\n",
      "test : 0.5923671058609723\n",
      "1167 loss: tensor(0.9304)\n",
      "train : 0.979060214407374\n",
      "test : 0.5931849159472967\n",
      "1168 loss: tensor(0.9297)\n",
      "train : 0.9787596433223124\n",
      "test : 0.5929123125851885\n",
      "1169 loss: tensor(0.9291)\n",
      "train : 0.9793607854924357\n",
      "test : 0.5933666515220355\n",
      "1170 loss: tensor(0.9284)\n",
      "train : 0.9789600240456868\n",
      "test : 0.5933666515220355\n",
      "1171 loss: tensor(0.9278)\n",
      "train : 0.9797615469391845\n",
      "test : 0.5937301226715129\n",
      "1172 loss: tensor(0.9272)\n",
      "train : 0.9789600240456868\n",
      "test : 0.5930940481599273\n",
      "1173 loss: tensor(0.9267)\n",
      "train : 0.9797615469391845\n",
      "test : 0.5940935938209905\n",
      "1174 loss: tensor(0.9261)\n",
      "train : 0.9791604047690612\n",
      "test : 0.5924579736483416\n",
      "1175 loss: tensor(0.9256)\n",
      "train : 0.9794609758541228\n",
      "test : 0.5937301226715129\n",
      "1176 loss: tensor(0.9252)\n",
      "train : 0.9793607854924357\n",
      "test : 0.5921853702862335\n",
      "1177 loss: tensor(0.9247)\n",
      "train : 0.97956116621581\n",
      "test : 0.593003180372558\n",
      "1178 loss: tensor(0.9244)\n",
      "train : 0.9794609758541228\n",
      "test : 0.591821899136756\n",
      "1179 loss: tensor(0.9240)\n",
      "train : 0.9796613565774972\n",
      "test : 0.5929123125851885\n",
      "1180 loss: tensor(0.9238)\n",
      "train : 0.9792605951307484\n",
      "test : 0.5909132212630622\n",
      "1181 loss: tensor(0.9236)\n",
      "train : 0.9794609758541228\n",
      "test : 0.5919127669241254\n",
      "1182 loss: tensor(0.9237)\n",
      "train : 0.9791604047690612\n",
      "test : 0.5893684688777828\n",
      "1183 loss: tensor(0.9237)\n",
      "train : 0.9789600240456868\n",
      "test : 0.5917310313493866\n",
      "1184 loss: tensor(0.9241)\n",
      "train : 0.9783588818755635\n",
      "test : 0.5875511131303953\n",
      "1185 loss: tensor(0.9244)\n",
      "train : 0.9779581204288148\n",
      "test : 0.5913675601999091\n",
      "1186 loss: tensor(0.9256)\n",
      "train : 0.9779581204288148\n",
      "test : 0.5857337573830077\n",
      "1187 loss: tensor(0.9264)\n",
      "train : 0.9768560264502555\n",
      "test : 0.5916401635620173\n",
      "1188 loss: tensor(0.9290)\n",
      "train : 0.976154693918445\n",
      "test : 0.5836437982735121\n",
      "1189 loss: tensor(0.9306)\n",
      "train : 0.9735497445145777\n",
      "test : 0.5911858246251703\n",
      "1190 loss: tensor(0.9359)\n",
      "train : 0.9716461276425208\n",
      "test : 0.5814629713766469\n",
      "1191 loss: tensor(0.9390)\n",
      "train : 0.9667367999198477\n",
      "test : 0.5876419809177647\n",
      "1192 loss: tensor(0.9499)\n",
      "train : 0.9635307083458571\n",
      "test : 0.5760109041344843\n",
      "1193 loss: tensor(0.9564)\n",
      "train : 0.9555154794108807\n",
      "test : 0.5837346660608814\n",
      "1194 loss: tensor(0.9805)\n",
      "train : 0.9475002504759042\n",
      "test : 0.563652885052249\n",
      "1195 loss: tensor(0.9957)\n",
      "train : 0.9253581805430318\n",
      "test : 0.5714675147660154\n",
      "1196 loss: tensor(1.0593)\n",
      "train : 0.8938984069732492\n",
      "test : 0.5355747387551113\n",
      "1197 loss: tensor(1.1206)\n",
      "train : 0.820759442941589\n",
      "test : 0.5282144479781917\n",
      "1198 loss: tensor(1.3818)\n",
      "train : 0.7008315800020039\n",
      "test : 0.4298955020445252\n",
      "1199 loss: tensor(1.9239)\n",
      "train : 0.5912233243162007\n",
      "test : 0.4087233075874602\n",
      "1200 loss: tensor(2.8148)\n",
      "train : 0.546037471195271\n",
      "test : 0.35674693321217626\n",
      "1201 loss: tensor(3.1456)\n",
      "train : 0.6392145075643723\n",
      "test : 0.4188096319854612\n",
      "1202 loss: tensor(2.0179)\n",
      "train : 0.6851016932171126\n",
      "test : 0.4468877782825988\n",
      "1203 loss: tensor(1.6914)\n",
      "train : 0.7717663560765454\n",
      "test : 0.47769195820081783\n",
      "1204 loss: tensor(1.5078)\n",
      "train : 0.9156397154593728\n",
      "test : 0.5546569741026806\n",
      "1205 loss: tensor(1.1492)\n",
      "train : 0.9616270914738002\n",
      "test : 0.5781917310313494\n",
      "1206 loss: tensor(1.0516)\n",
      "train : 0.9755535517483218\n",
      "test : 0.5883689232167196\n",
      "1207 loss: tensor(1.0115)\n",
      "train : 0.9774571686203787\n",
      "test : 0.5900954111767378\n",
      "1208 loss: tensor(0.9940)\n",
      "train : 0.9792605951307484\n",
      "test : 0.5910040890504317\n",
      "1209 loss: tensor(0.9819)\n",
      "train : 0.97956116621581\n",
      "test : 0.5916401635620173\n",
      "1210 loss: tensor(0.9723)\n",
      "train : 0.97956116621581\n",
      "test : 0.5930940481599273\n",
      "1211 loss: tensor(0.9646)\n",
      "train : 0.9803626891093077\n",
      "test : 0.5930940481599273\n",
      "1212 loss: tensor(0.9582)\n",
      "train : 0.9804628794709949\n",
      "test : 0.5930940481599273\n",
      "1213 loss: tensor(0.9527)\n",
      "train : 0.980563069832682\n",
      "test : 0.594457064970468\n",
      "1214 loss: tensor(0.9480)\n",
      "train : 0.9807634505560565\n",
      "test : 0.5948205361199455\n",
      "1215 loss: tensor(0.9440)\n",
      "train : 0.9807634505560565\n",
      "test : 0.5945479327578373\n",
      "1216 loss: tensor(0.9404)\n",
      "train : 0.9811642120028053\n",
      "test : 0.5942753293957292\n",
      "1217 loss: tensor(0.9372)\n",
      "train : 0.9813645927261797\n",
      "test : 0.594002726033621\n",
      "1218 loss: tensor(0.9344)\n",
      "train : 0.9811642120028053\n",
      "test : 0.5937301226715129\n",
      "1219 loss: tensor(0.9318)\n",
      "train : 0.9812644023644925\n",
      "test : 0.5939118582462517\n",
      "1220 loss: tensor(0.9295)\n",
      "train : 0.981464783087867\n",
      "test : 0.5946388005452067\n",
      "1221 loss: tensor(0.9275)\n",
      "train : 0.981464783087867\n",
      "test : 0.5941844616083598\n",
      "1222 loss: tensor(0.9256)\n",
      "train : 0.9816651638112414\n",
      "test : 0.5936392548841436\n",
      "1223 loss: tensor(0.9238)\n",
      "train : 0.9818655445346157\n",
      "test : 0.5927305770104498\n",
      "1224 loss: tensor(0.9222)\n",
      "train : 0.9816651638112414\n",
      "test : 0.5927305770104498\n",
      "1225 loss: tensor(0.9207)\n",
      "train : 0.9823664963430518\n",
      "test : 0.5922762380736029\n",
      "1226 loss: tensor(0.9193)\n",
      "train : 0.9822663059813646\n",
      "test : 0.5921853702862335\n",
      "1227 loss: tensor(0.9180)\n",
      "train : 0.9821661156196774\n",
      "test : 0.5924579736483416\n",
      "1228 loss: tensor(0.9168)\n",
      "train : 0.9822663059813646\n",
      "test : 0.5926397092230804\n",
      "1229 loss: tensor(0.9156)\n",
      "train : 0.9823664963430518\n",
      "test : 0.5923671058609723\n",
      "1230 loss: tensor(0.9145)\n",
      "train : 0.982466686704739\n",
      "test : 0.5924579736483416\n",
      "1231 loss: tensor(0.9135)\n",
      "train : 0.9825668770664262\n",
      "test : 0.5923671058609723\n",
      "1232 loss: tensor(0.9125)\n",
      "train : 0.9826670674281134\n",
      "test : 0.5920036347114948\n",
      "1233 loss: tensor(0.9115)\n",
      "train : 0.9828674481514879\n",
      "test : 0.591821899136756\n",
      "1234 loss: tensor(0.9106)\n",
      "train : 0.982967638513175\n",
      "test : 0.5921853702862335\n",
      "1235 loss: tensor(0.9098)\n",
      "train : 0.9830678288748622\n",
      "test : 0.5920945024988642\n",
      "1236 loss: tensor(0.9089)\n",
      "train : 0.9831680192365494\n",
      "test : 0.5919127669241254\n",
      "1237 loss: tensor(0.9081)\n",
      "train : 0.9833683999599239\n",
      "test : 0.591821899136756\n",
      "1238 loss: tensor(0.9074)\n",
      "train : 0.9833683999599239\n",
      "test : 0.591821899136756\n",
      "1239 loss: tensor(0.9066)\n",
      "train : 0.983468590321611\n",
      "test : 0.591821899136756\n",
      "1240 loss: tensor(0.9059)\n",
      "train : 0.9835687806832982\n",
      "test : 0.591821899136756\n",
      "1241 loss: tensor(0.9052)\n",
      "train : 0.9835687806832982\n",
      "test : 0.5917310313493866\n",
      "1242 loss: tensor(0.9045)\n",
      "train : 0.9836689710449855\n",
      "test : 0.5919127669241254\n",
      "1243 loss: tensor(0.9038)\n",
      "train : 0.9839695421300471\n",
      "test : 0.5917310313493866\n",
      "1244 loss: tensor(0.9032)\n",
      "train : 0.9839695421300471\n",
      "test : 0.5919127669241254\n",
      "1245 loss: tensor(0.9026)\n",
      "train : 0.9842701132151087\n",
      "test : 0.5916401635620173\n",
      "1246 loss: tensor(0.9019)\n",
      "train : 0.9842701132151087\n",
      "test : 0.5915492957746479\n",
      "1247 loss: tensor(0.9013)\n",
      "train : 0.9842701132151087\n",
      "test : 0.5919127669241254\n",
      "1248 loss: tensor(0.9007)\n",
      "train : 0.9845706843001704\n",
      "test : 0.5920036347114948\n",
      "1249 loss: tensor(0.9002)\n",
      "train : 0.9848712553852319\n",
      "test : 0.5920945024988642\n",
      "1250 loss: tensor(0.8996)\n",
      "train : 0.9849714457469192\n",
      "test : 0.5923671058609723\n",
      "1251 loss: tensor(0.8990)\n",
      "train : 0.9850716361086064\n",
      "test : 0.5921853702862335\n",
      "1252 loss: tensor(0.8985)\n",
      "train : 0.9850716361086064\n",
      "test : 0.5923671058609723\n",
      "1253 loss: tensor(0.8979)\n",
      "train : 0.9851718264702936\n",
      "test : 0.5924579736483416\n",
      "1254 loss: tensor(0.8974)\n",
      "train : 0.9851718264702936\n",
      "test : 0.5922762380736029\n",
      "1255 loss: tensor(0.8969)\n",
      "train : 0.9853722071936679\n",
      "test : 0.5922762380736029\n",
      "1256 loss: tensor(0.8963)\n",
      "train : 0.9854723975553552\n",
      "test : 0.5920036347114948\n",
      "1257 loss: tensor(0.8958)\n",
      "train : 0.9855725879170424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.5924579736483416\n",
      "1258 loss: tensor(0.8953)\n",
      "train : 0.9854723975553552\n",
      "test : 0.5921853702862335\n",
      "1259 loss: tensor(0.8948)\n",
      "train : 0.9855725879170424\n",
      "test : 0.5921853702862335\n",
      "1260 loss: tensor(0.8943)\n",
      "train : 0.9855725879170424\n",
      "test : 0.5922762380736029\n",
      "1261 loss: tensor(0.8939)\n",
      "train : 0.9856727782787296\n",
      "test : 0.591821899136756\n",
      "1262 loss: tensor(0.8934)\n",
      "train : 0.985873159002104\n",
      "test : 0.5922762380736029\n",
      "1263 loss: tensor(0.8929)\n",
      "train : 0.985873159002104\n",
      "test : 0.5919127669241254\n",
      "1264 loss: tensor(0.8924)\n",
      "train : 0.985873159002104\n",
      "test : 0.5920945024988642\n",
      "1265 loss: tensor(0.8920)\n",
      "train : 0.9859733493637912\n",
      "test : 0.5917310313493866\n",
      "1266 loss: tensor(0.8915)\n",
      "train : 0.9859733493637912\n",
      "test : 0.5920945024988642\n",
      "1267 loss: tensor(0.8911)\n",
      "train : 0.9862739204488529\n",
      "test : 0.5916401635620173\n",
      "1268 loss: tensor(0.8906)\n",
      "train : 0.9861737300871656\n",
      "test : 0.5921853702862335\n",
      "1269 loss: tensor(0.8902)\n",
      "train : 0.9862739204488529\n",
      "test : 0.5920036347114948\n",
      "1270 loss: tensor(0.8897)\n",
      "train : 0.98637411081054\n",
      "test : 0.5919127669241254\n",
      "1271 loss: tensor(0.8893)\n",
      "train : 0.9864743011722272\n",
      "test : 0.5919127669241254\n",
      "1272 loss: tensor(0.8889)\n",
      "train : 0.9864743011722272\n",
      "test : 0.5916401635620173\n",
      "1273 loss: tensor(0.8884)\n",
      "train : 0.9866746818956016\n",
      "test : 0.5917310313493866\n",
      "1274 loss: tensor(0.8880)\n",
      "train : 0.9866746818956016\n",
      "test : 0.591821899136756\n",
      "1275 loss: tensor(0.8876)\n",
      "train : 0.9867748722572889\n",
      "test : 0.5917310313493866\n",
      "1276 loss: tensor(0.8872)\n",
      "train : 0.9867748722572889\n",
      "test : 0.5920036347114948\n",
      "1277 loss: tensor(0.8867)\n",
      "train : 0.9870754433423504\n",
      "test : 0.5913675601999091\n",
      "1278 loss: tensor(0.8863)\n",
      "train : 0.9867748722572889\n",
      "test : 0.5919127669241254\n",
      "1279 loss: tensor(0.8859)\n",
      "train : 0.9870754433423504\n",
      "test : 0.591821899136756\n",
      "1280 loss: tensor(0.8855)\n",
      "train : 0.9869752529806632\n",
      "test : 0.5922762380736029\n",
      "1281 loss: tensor(0.8851)\n",
      "train : 0.9871756337040377\n",
      "test : 0.591821899136756\n",
      "1282 loss: tensor(0.8847)\n",
      "train : 0.9871756337040377\n",
      "test : 0.5923671058609723\n",
      "1283 loss: tensor(0.8843)\n",
      "train : 0.9872758240657249\n",
      "test : 0.5916401635620173\n",
      "1284 loss: tensor(0.8839)\n",
      "train : 0.9872758240657249\n",
      "test : 0.5922762380736029\n",
      "1285 loss: tensor(0.8835)\n",
      "train : 0.9872758240657249\n",
      "test : 0.591821899136756\n",
      "1286 loss: tensor(0.8831)\n",
      "train : 0.9873760144274121\n",
      "test : 0.5924579736483416\n",
      "1287 loss: tensor(0.8828)\n",
      "train : 0.9872758240657249\n",
      "test : 0.5920945024988642\n",
      "1288 loss: tensor(0.8824)\n",
      "train : 0.9874762047890993\n",
      "test : 0.5923671058609723\n",
      "1289 loss: tensor(0.8820)\n",
      "train : 0.9874762047890993\n",
      "test : 0.5920036347114948\n",
      "1290 loss: tensor(0.8816)\n",
      "train : 0.9875763951507865\n",
      "test : 0.5924579736483416\n",
      "1291 loss: tensor(0.8812)\n",
      "train : 0.9876765855124737\n",
      "test : 0.5921853702862335\n",
      "1292 loss: tensor(0.8809)\n",
      "train : 0.9875763951507865\n",
      "test : 0.5921853702862335\n",
      "1293 loss: tensor(0.8805)\n",
      "train : 0.9877767758741609\n",
      "test : 0.5920036347114948\n",
      "1294 loss: tensor(0.8801)\n",
      "train : 0.9878769662358481\n",
      "test : 0.5919127669241254\n",
      "1295 loss: tensor(0.8798)\n",
      "train : 0.9878769662358481\n",
      "test : 0.5920945024988642\n",
      "1296 loss: tensor(0.8794)\n",
      "train : 0.9880773469592226\n",
      "test : 0.5920036347114948\n",
      "1297 loss: tensor(0.8790)\n",
      "train : 0.9881775373209097\n",
      "test : 0.5920945024988642\n",
      "1298 loss: tensor(0.8787)\n",
      "train : 0.9880773469592226\n",
      "test : 0.5920036347114948\n",
      "1299 loss: tensor(0.8783)\n",
      "train : 0.9882777276825969\n",
      "test : 0.5924579736483416\n",
      "1300 loss: tensor(0.8780)\n",
      "train : 0.9881775373209097\n",
      "test : 0.5920036347114948\n",
      "1301 loss: tensor(0.8776)\n",
      "train : 0.9882777276825969\n",
      "test : 0.5925488414357111\n",
      "1302 loss: tensor(0.8773)\n",
      "train : 0.9883779180442841\n",
      "test : 0.5919127669241254\n",
      "1303 loss: tensor(0.8769)\n",
      "train : 0.9882777276825969\n",
      "test : 0.5927305770104498\n",
      "1304 loss: tensor(0.8766)\n",
      "train : 0.9884781084059714\n",
      "test : 0.5919127669241254\n",
      "1305 loss: tensor(0.8762)\n",
      "train : 0.9882777276825969\n",
      "test : 0.5927305770104498\n",
      "1306 loss: tensor(0.8759)\n",
      "train : 0.9888788698527202\n",
      "test : 0.5920945024988642\n",
      "1307 loss: tensor(0.8755)\n",
      "train : 0.9882777276825969\n",
      "test : 0.5927305770104498\n",
      "1308 loss: tensor(0.8752)\n",
      "train : 0.9888788698527202\n",
      "test : 0.5925488414357111\n",
      "1309 loss: tensor(0.8749)\n",
      "train : 0.9885782987676586\n",
      "test : 0.5926397092230804\n",
      "1310 loss: tensor(0.8745)\n",
      "train : 0.9888788698527202\n",
      "test : 0.5924579736483416\n",
      "1311 loss: tensor(0.8742)\n",
      "train : 0.9885782987676586\n",
      "test : 0.5927305770104498\n",
      "1312 loss: tensor(0.8738)\n",
      "train : 0.9888788698527202\n",
      "test : 0.5928214447978192\n",
      "1313 loss: tensor(0.8735)\n",
      "train : 0.9888788698527202\n",
      "test : 0.5927305770104498\n",
      "1314 loss: tensor(0.8732)\n",
      "train : 0.9888788698527202\n",
      "test : 0.5928214447978192\n",
      "1315 loss: tensor(0.8729)\n",
      "train : 0.9888788698527202\n",
      "test : 0.5925488414357111\n",
      "1316 loss: tensor(0.8725)\n",
      "train : 0.9890792505760946\n",
      "test : 0.593003180372558\n",
      "1317 loss: tensor(0.8722)\n",
      "train : 0.9888788698527202\n",
      "test : 0.5923671058609723\n",
      "1318 loss: tensor(0.8719)\n",
      "train : 0.9891794409377818\n",
      "test : 0.5929123125851885\n",
      "1319 loss: tensor(0.8716)\n",
      "train : 0.9889790602144074\n",
      "test : 0.5924579736483416\n",
      "1320 loss: tensor(0.8713)\n",
      "train : 0.9894800120228434\n",
      "test : 0.593003180372558\n",
      "1321 loss: tensor(0.8710)\n",
      "train : 0.9890792505760946\n",
      "test : 0.5927305770104498\n",
      "1322 loss: tensor(0.8706)\n",
      "train : 0.9894800120228434\n",
      "test : 0.5930940481599273\n",
      "1323 loss: tensor(0.8703)\n",
      "train : 0.9890792505760946\n",
      "test : 0.5929123125851885\n",
      "1324 loss: tensor(0.8700)\n",
      "train : 0.9894800120228434\n",
      "test : 0.5930940481599273\n",
      "1325 loss: tensor(0.8697)\n",
      "train : 0.9891794409377818\n",
      "test : 0.5930940481599273\n",
      "1326 loss: tensor(0.8694)\n",
      "train : 0.9895802023845306\n",
      "test : 0.5930940481599273\n",
      "1327 loss: tensor(0.8691)\n",
      "train : 0.9891794409377818\n",
      "test : 0.5930940481599273\n",
      "1328 loss: tensor(0.8688)\n",
      "train : 0.9895802023845306\n",
      "test : 0.593003180372558\n",
      "1329 loss: tensor(0.8685)\n",
      "train : 0.9894800120228434\n",
      "test : 0.5930940481599273\n",
      "1330 loss: tensor(0.8682)\n",
      "train : 0.9897805831079051\n",
      "test : 0.593003180372558\n",
      "1331 loss: tensor(0.8679)\n",
      "train : 0.9896803927462178\n",
      "test : 0.593275783734666\n",
      "1332 loss: tensor(0.8676)\n",
      "train : 0.9898807734695922\n",
      "test : 0.593003180372558\n",
      "1333 loss: tensor(0.8673)\n",
      "train : 0.9897805831079051\n",
      "test : 0.5930940481599273\n",
      "1334 loss: tensor(0.8670)\n",
      "train : 0.9898807734695922\n",
      "test : 0.5930940481599273\n",
      "1335 loss: tensor(0.8667)\n",
      "train : 0.9899809638312794\n",
      "test : 0.5930940481599273\n",
      "1336 loss: tensor(0.8664)\n",
      "train : 0.9898807734695922\n",
      "test : 0.5930940481599273\n",
      "1337 loss: tensor(0.8661)\n",
      "train : 0.9901813445546539\n",
      "test : 0.593003180372558\n",
      "1338 loss: tensor(0.8658)\n",
      "train : 0.9897805831079051\n",
      "test : 0.5929123125851885\n",
      "1339 loss: tensor(0.8656)\n",
      "train : 0.9903817252780283\n",
      "test : 0.5930940481599273\n",
      "1340 loss: tensor(0.8653)\n",
      "train : 0.9897805831079051\n",
      "test : 0.5933666515220355\n",
      "1341 loss: tensor(0.8650)\n",
      "train : 0.9904819156397154\n",
      "test : 0.593003180372558\n",
      "1342 loss: tensor(0.8647)\n",
      "train : 0.9897805831079051\n",
      "test : 0.5934575193094048\n",
      "1343 loss: tensor(0.8644)\n",
      "train : 0.9904819156397154\n",
      "test : 0.5928214447978192\n",
      "1344 loss: tensor(0.8641)\n",
      "train : 0.9897805831079051\n",
      "test : 0.593275783734666\n",
      "1345 loss: tensor(0.8638)\n",
      "train : 0.9905821060014026\n",
      "test : 0.5928214447978192\n",
      "1346 loss: tensor(0.8635)\n",
      "train : 0.9898807734695922\n",
      "test : 0.593275783734666\n",
      "1347 loss: tensor(0.8632)\n",
      "train : 0.9905821060014026\n",
      "test : 0.5925488414357111\n",
      "1348 loss: tensor(0.8629)\n",
      "train : 0.9899809638312794\n",
      "test : 0.5931849159472967\n",
      "1349 loss: tensor(0.8627)\n",
      "train : 0.9906822963630899\n",
      "test : 0.5925488414357111\n",
      "1350 loss: tensor(0.8624)\n",
      "train : 0.9900811541929666\n",
      "test : 0.5930940481599273\n",
      "1351 loss: tensor(0.8621)\n",
      "train : 0.9909828674481514\n",
      "test : 0.5925488414357111\n",
      "1352 loss: tensor(0.8618)\n",
      "train : 0.9901813445546539\n",
      "test : 0.5928214447978192\n",
      "1353 loss: tensor(0.8615)\n",
      "train : 0.9909828674481514\n",
      "test : 0.5929123125851885\n",
      "1354 loss: tensor(0.8612)\n",
      "train : 0.9900811541929666\n",
      "test : 0.5929123125851885\n",
      "1355 loss: tensor(0.8610)\n",
      "train : 0.9910830578098387\n",
      "test : 0.5928214447978192\n",
      "1356 loss: tensor(0.8607)\n",
      "train : 0.9900811541929666\n",
      "test : 0.5926397092230804\n",
      "1357 loss: tensor(0.8604)\n",
      "train : 0.9910830578098387\n",
      "test : 0.5926397092230804\n",
      "1358 loss: tensor(0.8601)\n",
      "train : 0.9901813445546539\n",
      "test : 0.5926397092230804\n",
      "1359 loss: tensor(0.8599)\n",
      "train : 0.9910830578098387\n",
      "test : 0.5926397092230804\n",
      "1360 loss: tensor(0.8596)\n",
      "train : 0.9902815349163411\n",
      "test : 0.5926397092230804\n",
      "1361 loss: tensor(0.8593)\n",
      "train : 0.9910830578098387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.5926397092230804\n",
      "1362 loss: tensor(0.8591)\n",
      "train : 0.9904819156397154\n",
      "test : 0.5925488414357111\n",
      "1363 loss: tensor(0.8588)\n",
      "train : 0.9910830578098387\n",
      "test : 0.5928214447978192\n",
      "1364 loss: tensor(0.8585)\n",
      "train : 0.9905821060014026\n",
      "test : 0.5928214447978192\n",
      "1365 loss: tensor(0.8583)\n",
      "train : 0.9912834385332131\n",
      "test : 0.5928214447978192\n",
      "1366 loss: tensor(0.8580)\n",
      "train : 0.9904819156397154\n",
      "test : 0.593003180372558\n",
      "1367 loss: tensor(0.8577)\n",
      "train : 0.9911832481715259\n",
      "test : 0.5926397092230804\n",
      "1368 loss: tensor(0.8574)\n",
      "train : 0.9905821060014026\n",
      "test : 0.5928214447978192\n",
      "1369 loss: tensor(0.8572)\n",
      "train : 0.9913836288949003\n",
      "test : 0.5925488414357111\n",
      "1370 loss: tensor(0.8569)\n",
      "train : 0.9906822963630899\n",
      "test : 0.5926397092230804\n",
      "1371 loss: tensor(0.8567)\n",
      "train : 0.9913836288949003\n",
      "test : 0.5925488414357111\n",
      "1372 loss: tensor(0.8564)\n",
      "train : 0.9907824867247771\n",
      "test : 0.5926397092230804\n",
      "1373 loss: tensor(0.8561)\n",
      "train : 0.9913836288949003\n",
      "test : 0.5927305770104498\n",
      "1374 loss: tensor(0.8559)\n",
      "train : 0.9908826770864643\n",
      "test : 0.5926397092230804\n",
      "1375 loss: tensor(0.8556)\n",
      "train : 0.9914838192565875\n",
      "test : 0.5927305770104498\n",
      "1376 loss: tensor(0.8553)\n",
      "train : 0.9910830578098387\n",
      "test : 0.5926397092230804\n",
      "1377 loss: tensor(0.8550)\n",
      "train : 0.9914838192565875\n",
      "test : 0.5926397092230804\n",
      "1378 loss: tensor(0.8548)\n",
      "train : 0.9911832481715259\n",
      "test : 0.5926397092230804\n",
      "1379 loss: tensor(0.8545)\n",
      "train : 0.9917843903416491\n",
      "test : 0.5926397092230804\n",
      "1380 loss: tensor(0.8542)\n",
      "train : 0.9912834385332131\n",
      "test : 0.5930940481599273\n",
      "1381 loss: tensor(0.8539)\n",
      "train : 0.9917843903416491\n",
      "test : 0.5927305770104498\n",
      "1382 loss: tensor(0.8537)\n",
      "train : 0.9912834385332131\n",
      "test : 0.5930940481599273\n",
      "1383 loss: tensor(0.8534)\n",
      "train : 0.9917843903416491\n",
      "test : 0.5928214447978192\n",
      "1384 loss: tensor(0.8531)\n",
      "train : 0.9913836288949003\n",
      "test : 0.5930940481599273\n",
      "1385 loss: tensor(0.8528)\n",
      "train : 0.9918845807033363\n",
      "test : 0.5927305770104498\n",
      "1386 loss: tensor(0.8526)\n",
      "train : 0.9913836288949003\n",
      "test : 0.593003180372558\n",
      "1387 loss: tensor(0.8523)\n",
      "train : 0.9918845807033363\n",
      "test : 0.5924579736483416\n",
      "1388 loss: tensor(0.8521)\n",
      "train : 0.9915840096182748\n",
      "test : 0.593003180372558\n",
      "1389 loss: tensor(0.8518)\n",
      "train : 0.9919847710650236\n",
      "test : 0.5921853702862335\n",
      "1390 loss: tensor(0.8516)\n",
      "train : 0.9916841999799619\n",
      "test : 0.5930940481599273\n",
      "1391 loss: tensor(0.8513)\n",
      "train : 0.9919847710650236\n",
      "test : 0.5922762380736029\n",
      "1392 loss: tensor(0.8511)\n",
      "train : 0.9916841999799619\n",
      "test : 0.593003180372558\n",
      "1393 loss: tensor(0.8508)\n",
      "train : 0.992185151788398\n",
      "test : 0.5920945024988642\n",
      "1394 loss: tensor(0.8506)\n",
      "train : 0.9917843903416491\n",
      "test : 0.593003180372558\n",
      "1395 loss: tensor(0.8504)\n",
      "train : 0.992185151788398\n",
      "test : 0.5923671058609723\n",
      "1396 loss: tensor(0.8501)\n",
      "train : 0.9917843903416491\n",
      "test : 0.5929123125851885\n",
      "1397 loss: tensor(0.8499)\n",
      "train : 0.9922853421500851\n",
      "test : 0.5925488414357111\n",
      "1398 loss: tensor(0.8496)\n",
      "train : 0.9917843903416491\n",
      "test : 0.5929123125851885\n",
      "1399 loss: tensor(0.8494)\n",
      "train : 0.9922853421500851\n",
      "test : 0.5923671058609723\n",
      "1400 loss: tensor(0.8492)\n",
      "train : 0.9918845807033363\n",
      "test : 0.5931849159472967\n",
      "1401 loss: tensor(0.8489)\n",
      "train : 0.9922853421500851\n",
      "test : 0.5924579736483416\n",
      "1402 loss: tensor(0.8487)\n",
      "train : 0.9918845807033363\n",
      "test : 0.593003180372558\n",
      "1403 loss: tensor(0.8485)\n",
      "train : 0.9922853421500851\n",
      "test : 0.5923671058609723\n",
      "1404 loss: tensor(0.8482)\n",
      "train : 0.9919847710650236\n",
      "test : 0.5930940481599273\n",
      "1405 loss: tensor(0.8480)\n",
      "train : 0.9922853421500851\n",
      "test : 0.5921853702862335\n",
      "1406 loss: tensor(0.8478)\n",
      "train : 0.9919847710650236\n",
      "test : 0.5927305770104498\n",
      "1407 loss: tensor(0.8476)\n",
      "train : 0.992686103596834\n",
      "test : 0.5921853702862335\n",
      "1408 loss: tensor(0.8473)\n",
      "train : 0.9919847710650236\n",
      "test : 0.5923671058609723\n",
      "1409 loss: tensor(0.8471)\n",
      "train : 0.992686103596834\n",
      "test : 0.5925488414357111\n",
      "1410 loss: tensor(0.8469)\n",
      "train : 0.9919847710650236\n",
      "test : 0.5920945024988642\n",
      "1411 loss: tensor(0.8467)\n",
      "train : 0.992686103596834\n",
      "test : 0.5928214447978192\n",
      "1412 loss: tensor(0.8465)\n",
      "train : 0.9919847710650236\n",
      "test : 0.5923671058609723\n",
      "1413 loss: tensor(0.8462)\n",
      "train : 0.992686103596834\n",
      "test : 0.5929123125851885\n",
      "1414 loss: tensor(0.8460)\n",
      "train : 0.9919847710650236\n",
      "test : 0.5926397092230804\n",
      "1415 loss: tensor(0.8458)\n",
      "train : 0.992686103596834\n",
      "test : 0.5928214447978192\n",
      "1416 loss: tensor(0.8456)\n",
      "train : 0.9919847710650236\n",
      "test : 0.5927305770104498\n",
      "1417 loss: tensor(0.8454)\n",
      "train : 0.992686103596834\n",
      "test : 0.5928214447978192\n",
      "1418 loss: tensor(0.8451)\n",
      "train : 0.9920849614267108\n",
      "test : 0.5925488414357111\n",
      "1419 loss: tensor(0.8449)\n",
      "train : 0.992686103596834\n",
      "test : 0.5925488414357111\n",
      "1420 loss: tensor(0.8447)\n",
      "train : 0.9920849614267108\n",
      "test : 0.5925488414357111\n",
      "1421 loss: tensor(0.8445)\n",
      "train : 0.9928864843202084\n",
      "test : 0.5924579736483416\n",
      "1422 loss: tensor(0.8442)\n",
      "train : 0.9920849614267108\n",
      "test : 0.5925488414357111\n",
      "1423 loss: tensor(0.8440)\n",
      "train : 0.9928864843202084\n",
      "test : 0.5924579736483416\n",
      "1424 loss: tensor(0.8437)\n",
      "train : 0.992185151788398\n",
      "test : 0.5925488414357111\n",
      "1425 loss: tensor(0.8435)\n",
      "train : 0.9928864843202084\n",
      "test : 0.5925488414357111\n",
      "1426 loss: tensor(0.8433)\n",
      "train : 0.9924857228734596\n",
      "test : 0.5927305770104498\n",
      "1427 loss: tensor(0.8430)\n",
      "train : 0.9929866746818956\n",
      "test : 0.5925488414357111\n",
      "1428 loss: tensor(0.8428)\n",
      "train : 0.9925859132351468\n",
      "test : 0.5923671058609723\n",
      "1429 loss: tensor(0.8426)\n",
      "train : 0.9929866746818956\n",
      "test : 0.5924579736483416\n",
      "1430 loss: tensor(0.8423)\n",
      "train : 0.9928864843202084\n",
      "test : 0.5921853702862335\n",
      "1431 loss: tensor(0.8421)\n",
      "train : 0.9929866746818956\n",
      "test : 0.5923671058609723\n",
      "1432 loss: tensor(0.8419)\n",
      "train : 0.9929866746818956\n",
      "test : 0.5923671058609723\n",
      "1433 loss: tensor(0.8417)\n",
      "train : 0.9930868650435828\n",
      "test : 0.5923671058609723\n",
      "1434 loss: tensor(0.8414)\n",
      "train : 0.9929866746818956\n",
      "test : 0.5922762380736029\n",
      "1435 loss: tensor(0.8412)\n",
      "train : 0.9930868650435828\n",
      "test : 0.5921853702862335\n",
      "1436 loss: tensor(0.8410)\n",
      "train : 0.9929866746818956\n",
      "test : 0.5923671058609723\n",
      "1437 loss: tensor(0.8408)\n",
      "train : 0.9930868650435828\n",
      "test : 0.5922762380736029\n",
      "1438 loss: tensor(0.8406)\n",
      "train : 0.99318705540527\n",
      "test : 0.5923671058609723\n",
      "1439 loss: tensor(0.8403)\n",
      "train : 0.9930868650435828\n",
      "test : 0.5921853702862335\n",
      "1440 loss: tensor(0.8401)\n",
      "train : 0.9933874361286444\n",
      "test : 0.5921853702862335\n",
      "1441 loss: tensor(0.8399)\n",
      "train : 0.9932872457669573\n",
      "test : 0.5922762380736029\n",
      "1442 loss: tensor(0.8397)\n",
      "train : 0.9933874361286444\n",
      "test : 0.5921853702862335\n",
      "1443 loss: tensor(0.8395)\n",
      "train : 0.9932872457669573\n",
      "test : 0.5923671058609723\n",
      "1444 loss: tensor(0.8393)\n",
      "train : 0.9933874361286444\n",
      "test : 0.5921853702862335\n",
      "1445 loss: tensor(0.8391)\n",
      "train : 0.9933874361286444\n",
      "test : 0.5923671058609723\n",
      "1446 loss: tensor(0.8389)\n",
      "train : 0.9933874361286444\n",
      "test : 0.5921853702862335\n",
      "1447 loss: tensor(0.8387)\n",
      "train : 0.9933874361286444\n",
      "test : 0.5922762380736029\n",
      "1448 loss: tensor(0.8385)\n",
      "train : 0.9934876264903316\n",
      "test : 0.5925488414357111\n",
      "1449 loss: tensor(0.8383)\n",
      "train : 0.9933874361286444\n",
      "test : 0.5921853702862335\n",
      "1450 loss: tensor(0.8381)\n",
      "train : 0.9936880072137061\n",
      "test : 0.5926397092230804\n",
      "1451 loss: tensor(0.8379)\n",
      "train : 0.9933874361286444\n",
      "test : 0.5921853702862335\n",
      "1452 loss: tensor(0.8378)\n",
      "train : 0.9936880072137061\n",
      "test : 0.5925488414357111\n",
      "1453 loss: tensor(0.8376)\n",
      "train : 0.9933874361286444\n",
      "test : 0.5923671058609723\n",
      "1454 loss: tensor(0.8374)\n",
      "train : 0.9936880072137061\n",
      "test : 0.5927305770104498\n",
      "1455 loss: tensor(0.8372)\n",
      "train : 0.9933874361286444\n",
      "test : 0.5925488414357111\n",
      "1456 loss: tensor(0.8370)\n",
      "train : 0.9937881975753933\n",
      "test : 0.5926397092230804\n",
      "1457 loss: tensor(0.8368)\n",
      "train : 0.9934876264903316\n",
      "test : 0.5924579736483416\n",
      "1458 loss: tensor(0.8366)\n",
      "train : 0.9938883879370805\n",
      "test : 0.5923671058609723\n",
      "1459 loss: tensor(0.8364)\n",
      "train : 0.9934876264903316\n",
      "test : 0.5926397092230804\n",
      "1460 loss: tensor(0.8362)\n",
      "train : 0.9938883879370805\n",
      "test : 0.5923671058609723\n",
      "1461 loss: tensor(0.8360)\n",
      "train : 0.9936880072137061\n",
      "test : 0.5927305770104498\n",
      "1462 loss: tensor(0.8358)\n",
      "train : 0.9938883879370805\n",
      "test : 0.5923671058609723\n",
      "1463 loss: tensor(0.8356)\n",
      "train : 0.9938883879370805\n",
      "test : 0.5928214447978192\n",
      "1464 loss: tensor(0.8354)\n",
      "train : 0.9939885782987676\n",
      "test : 0.5922762380736029\n",
      "1465 loss: tensor(0.8352)\n",
      "train : 0.9939885782987676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.5928214447978192\n",
      "1466 loss: tensor(0.8350)\n",
      "train : 0.9939885782987676\n",
      "test : 0.5922762380736029\n",
      "1467 loss: tensor(0.8348)\n",
      "train : 0.9939885782987676\n",
      "test : 0.5926397092230804\n",
      "1468 loss: tensor(0.8347)\n",
      "train : 0.9939885782987676\n",
      "test : 0.5922762380736029\n",
      "1469 loss: tensor(0.8345)\n",
      "train : 0.9941889590221421\n",
      "test : 0.5927305770104498\n",
      "1470 loss: tensor(0.8343)\n",
      "train : 0.9939885782987676\n",
      "test : 0.5922762380736029\n",
      "1471 loss: tensor(0.8341)\n",
      "train : 0.9942891493838293\n",
      "test : 0.5926397092230804\n",
      "1472 loss: tensor(0.8339)\n",
      "train : 0.9940887686604548\n",
      "test : 0.5924579736483416\n",
      "1473 loss: tensor(0.8337)\n",
      "train : 0.9942891493838293\n",
      "test : 0.5927305770104498\n",
      "1474 loss: tensor(0.8335)\n",
      "train : 0.9941889590221421\n",
      "test : 0.5924579736483416\n",
      "1475 loss: tensor(0.8334)\n",
      "train : 0.9942891493838293\n",
      "test : 0.5925488414357111\n",
      "1476 loss: tensor(0.8332)\n",
      "train : 0.9942891493838293\n",
      "test : 0.5925488414357111\n",
      "1477 loss: tensor(0.8330)\n",
      "train : 0.9943893397455165\n",
      "test : 0.5925488414357111\n",
      "1478 loss: tensor(0.8328)\n",
      "train : 0.9942891493838293\n",
      "test : 0.5928214447978192\n",
      "1479 loss: tensor(0.8326)\n",
      "train : 0.9944895301072036\n",
      "test : 0.5924579736483416\n",
      "1480 loss: tensor(0.8324)\n",
      "train : 0.9943893397455165\n",
      "test : 0.5925488414357111\n",
      "1481 loss: tensor(0.8322)\n",
      "train : 0.9944895301072036\n",
      "test : 0.5923671058609723\n",
      "1482 loss: tensor(0.8320)\n",
      "train : 0.9944895301072036\n",
      "test : 0.5927305770104498\n",
      "1483 loss: tensor(0.8318)\n",
      "train : 0.9945897204688909\n",
      "test : 0.5921853702862335\n",
      "1484 loss: tensor(0.8316)\n",
      "train : 0.9946899108305781\n",
      "test : 0.5926397092230804\n",
      "1485 loss: tensor(0.8315)\n",
      "train : 0.9945897204688909\n",
      "test : 0.5921853702862335\n",
      "1486 loss: tensor(0.8313)\n",
      "train : 0.9948902915539525\n",
      "test : 0.5925488414357111\n",
      "1487 loss: tensor(0.8311)\n",
      "train : 0.9946899108305781\n",
      "test : 0.5920036347114948\n",
      "1488 loss: tensor(0.8309)\n",
      "train : 0.9949904819156398\n",
      "test : 0.5923671058609723\n",
      "1489 loss: tensor(0.8307)\n",
      "train : 0.9946899108305781\n",
      "test : 0.5920036347114948\n",
      "1490 loss: tensor(0.8305)\n",
      "train : 0.9949904819156398\n",
      "test : 0.5924579736483416\n",
      "1491 loss: tensor(0.8303)\n",
      "train : 0.9948902915539525\n",
      "test : 0.5919127669241254\n",
      "1492 loss: tensor(0.8302)\n",
      "train : 0.9949904819156398\n",
      "test : 0.5922762380736029\n",
      "1493 loss: tensor(0.8300)\n",
      "train : 0.9949904819156398\n",
      "test : 0.591821899136756\n",
      "1494 loss: tensor(0.8298)\n",
      "train : 0.995090672277327\n",
      "test : 0.5921853702862335\n",
      "1495 loss: tensor(0.8296)\n",
      "train : 0.995090672277327\n",
      "test : 0.591821899136756\n",
      "1496 loss: tensor(0.8295)\n",
      "train : 0.995090672277327\n",
      "test : 0.5923671058609723\n",
      "1497 loss: tensor(0.8293)\n",
      "train : 0.995090672277327\n",
      "test : 0.591821899136756\n",
      "1498 loss: tensor(0.8291)\n",
      "train : 0.995090672277327\n",
      "test : 0.5920945024988642\n",
      "1499 loss: tensor(0.8289)\n",
      "train : 0.9951908626390141\n",
      "test : 0.591821899136756\n",
      "1500 loss: tensor(0.8288)\n",
      "train : 0.9952910530007013\n",
      "test : 0.5924579736483416\n",
      "1501 loss: tensor(0.8286)\n",
      "train : 0.9952910530007013\n",
      "test : 0.5917310313493866\n",
      "1502 loss: tensor(0.8284)\n",
      "train : 0.9952910530007013\n",
      "test : 0.5926397092230804\n",
      "1503 loss: tensor(0.8283)\n",
      "train : 0.9954914337240758\n",
      "test : 0.5919127669241254\n",
      "1504 loss: tensor(0.8281)\n",
      "train : 0.9952910530007013\n",
      "test : 0.593003180372558\n",
      "1505 loss: tensor(0.8279)\n",
      "train : 0.9956918144474501\n",
      "test : 0.5919127669241254\n",
      "1506 loss: tensor(0.8277)\n",
      "train : 0.9953912433623885\n",
      "test : 0.5928214447978192\n",
      "1507 loss: tensor(0.8276)\n",
      "train : 0.9956918144474501\n",
      "test : 0.5919127669241254\n",
      "1508 loss: tensor(0.8274)\n",
      "train : 0.9953912433623885\n",
      "test : 0.5927305770104498\n",
      "1509 loss: tensor(0.8273)\n",
      "train : 0.9956918144474501\n",
      "test : 0.5920945024988642\n",
      "1510 loss: tensor(0.8271)\n",
      "train : 0.9952910530007013\n",
      "test : 0.593003180372558\n",
      "1511 loss: tensor(0.8269)\n",
      "train : 0.9957920048091373\n",
      "test : 0.5921853702862335\n",
      "1512 loss: tensor(0.8268)\n",
      "train : 0.9953912433623885\n",
      "test : 0.5929123125851885\n",
      "1513 loss: tensor(0.8266)\n",
      "train : 0.9957920048091373\n",
      "test : 0.5921853702862335\n",
      "1514 loss: tensor(0.8264)\n",
      "train : 0.995591624085763\n",
      "test : 0.593003180372558\n",
      "1515 loss: tensor(0.8263)\n",
      "train : 0.9958921951708246\n",
      "test : 0.5920945024988642\n",
      "1516 loss: tensor(0.8261)\n",
      "train : 0.995591624085763\n",
      "test : 0.5927305770104498\n",
      "1517 loss: tensor(0.8260)\n",
      "train : 0.9958921951708246\n",
      "test : 0.5920036347114948\n",
      "1518 loss: tensor(0.8258)\n",
      "train : 0.9956918144474501\n",
      "test : 0.5928214447978192\n",
      "1519 loss: tensor(0.8257)\n",
      "train : 0.9958921951708246\n",
      "test : 0.5920036347114948\n",
      "1520 loss: tensor(0.8255)\n",
      "train : 0.9956918144474501\n",
      "test : 0.5928214447978192\n",
      "1521 loss: tensor(0.8253)\n",
      "train : 0.9958921951708246\n",
      "test : 0.5922762380736029\n",
      "1522 loss: tensor(0.8252)\n",
      "train : 0.9957920048091373\n",
      "test : 0.5928214447978192\n",
      "1523 loss: tensor(0.8250)\n",
      "train : 0.9958921951708246\n",
      "test : 0.5923671058609723\n",
      "1524 loss: tensor(0.8249)\n",
      "train : 0.9957920048091373\n",
      "test : 0.5928214447978192\n",
      "1525 loss: tensor(0.8247)\n",
      "train : 0.9959923855325118\n",
      "test : 0.5926397092230804\n",
      "1526 loss: tensor(0.8245)\n",
      "train : 0.9957920048091373\n",
      "test : 0.5925488414357111\n",
      "1527 loss: tensor(0.8244)\n",
      "train : 0.9959923855325118\n",
      "test : 0.5926397092230804\n",
      "1528 loss: tensor(0.8242)\n",
      "train : 0.9958921951708246\n",
      "test : 0.5924579736483416\n",
      "1529 loss: tensor(0.8241)\n",
      "train : 0.9959923855325118\n",
      "test : 0.5924579736483416\n",
      "1530 loss: tensor(0.8239)\n",
      "train : 0.9959923855325118\n",
      "test : 0.5926397092230804\n",
      "1531 loss: tensor(0.8237)\n",
      "train : 0.9959923855325118\n",
      "test : 0.5925488414357111\n",
      "1532 loss: tensor(0.8236)\n",
      "train : 0.9959923855325118\n",
      "test : 0.5926397092230804\n",
      "1533 loss: tensor(0.8234)\n",
      "train : 0.9959923855325118\n",
      "test : 0.5922762380736029\n",
      "1534 loss: tensor(0.8233)\n",
      "train : 0.996092575894199\n",
      "test : 0.5925488414357111\n",
      "1535 loss: tensor(0.8231)\n",
      "train : 0.996092575894199\n",
      "test : 0.5925488414357111\n",
      "1536 loss: tensor(0.8230)\n",
      "train : 0.996092575894199\n",
      "test : 0.5927305770104498\n",
      "1537 loss: tensor(0.8228)\n",
      "train : 0.996092575894199\n",
      "test : 0.5924579736483416\n",
      "1538 loss: tensor(0.8226)\n",
      "train : 0.9961927662558862\n",
      "test : 0.5929123125851885\n",
      "1539 loss: tensor(0.8225)\n",
      "train : 0.996092575894199\n",
      "test : 0.5924579736483416\n",
      "1540 loss: tensor(0.8223)\n",
      "train : 0.9961927662558862\n",
      "test : 0.5929123125851885\n",
      "1541 loss: tensor(0.8222)\n",
      "train : 0.996092575894199\n",
      "test : 0.5923671058609723\n",
      "1542 loss: tensor(0.8220)\n",
      "train : 0.996092575894199\n",
      "test : 0.5930940481599273\n",
      "1543 loss: tensor(0.8219)\n",
      "train : 0.996092575894199\n",
      "test : 0.5922762380736029\n",
      "1544 loss: tensor(0.8217)\n",
      "train : 0.9961927662558862\n",
      "test : 0.5930940481599273\n",
      "1545 loss: tensor(0.8216)\n",
      "train : 0.996092575894199\n",
      "test : 0.5923671058609723\n",
      "1546 loss: tensor(0.8214)\n",
      "train : 0.9961927662558862\n",
      "test : 0.593003180372558\n",
      "1547 loss: tensor(0.8212)\n",
      "train : 0.996092575894199\n",
      "test : 0.5924579736483416\n",
      "1548 loss: tensor(0.8211)\n",
      "train : 0.9961927662558862\n",
      "test : 0.5931849159472967\n",
      "1549 loss: tensor(0.8209)\n",
      "train : 0.996092575894199\n",
      "test : 0.5924579736483416\n",
      "1550 loss: tensor(0.8208)\n",
      "train : 0.9961927662558862\n",
      "test : 0.593003180372558\n",
      "1551 loss: tensor(0.8206)\n",
      "train : 0.9962929566175734\n",
      "test : 0.5924579736483416\n",
      "1552 loss: tensor(0.8205)\n",
      "train : 0.9961927662558862\n",
      "test : 0.5928214447978192\n",
      "1553 loss: tensor(0.8204)\n",
      "train : 0.9963931469792606\n",
      "test : 0.5925488414357111\n",
      "1554 loss: tensor(0.8202)\n",
      "train : 0.9962929566175734\n",
      "test : 0.5926397092230804\n",
      "1555 loss: tensor(0.8201)\n",
      "train : 0.9963931469792606\n",
      "test : 0.5924579736483416\n",
      "1556 loss: tensor(0.8199)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5924579736483416\n",
      "1557 loss: tensor(0.8198)\n",
      "train : 0.9963931469792606\n",
      "test : 0.5926397092230804\n",
      "1558 loss: tensor(0.8196)\n",
      "train : 0.996593527702635\n",
      "test : 0.5926397092230804\n",
      "1559 loss: tensor(0.8195)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5926397092230804\n",
      "1560 loss: tensor(0.8194)\n",
      "train : 0.996593527702635\n",
      "test : 0.5924579736483416\n",
      "1561 loss: tensor(0.8192)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5927305770104498\n",
      "1562 loss: tensor(0.8191)\n",
      "train : 0.996593527702635\n",
      "test : 0.5924579736483416\n",
      "1563 loss: tensor(0.8190)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5926397092230804\n",
      "1564 loss: tensor(0.8188)\n",
      "train : 0.996593527702635\n",
      "test : 0.5924579736483416\n",
      "1565 loss: tensor(0.8187)\n",
      "train : 0.9963931469792606\n",
      "test : 0.5925488414357111\n",
      "1566 loss: tensor(0.8185)\n",
      "train : 0.996593527702635\n",
      "test : 0.5929123125851885\n",
      "1567 loss: tensor(0.8184)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5924579736483416\n",
      "1568 loss: tensor(0.8183)\n",
      "train : 0.996593527702635\n",
      "test : 0.593003180372558\n",
      "1569 loss: tensor(0.8181)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5924579736483416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570 loss: tensor(0.8180)\n",
      "train : 0.996593527702635\n",
      "test : 0.5929123125851885\n",
      "1571 loss: tensor(0.8179)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5925488414357111\n",
      "1572 loss: tensor(0.8177)\n",
      "train : 0.996593527702635\n",
      "test : 0.5929123125851885\n",
      "1573 loss: tensor(0.8176)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5923671058609723\n",
      "1574 loss: tensor(0.8174)\n",
      "train : 0.996593527702635\n",
      "test : 0.5929123125851885\n",
      "1575 loss: tensor(0.8173)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5922762380736029\n",
      "1576 loss: tensor(0.8172)\n",
      "train : 0.996593527702635\n",
      "test : 0.5927305770104498\n",
      "1577 loss: tensor(0.8170)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5922762380736029\n",
      "1578 loss: tensor(0.8169)\n",
      "train : 0.996593527702635\n",
      "test : 0.5926397092230804\n",
      "1579 loss: tensor(0.8168)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5921853702862335\n",
      "1580 loss: tensor(0.8166)\n",
      "train : 0.996593527702635\n",
      "test : 0.5926397092230804\n",
      "1581 loss: tensor(0.8165)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5919127669241254\n",
      "1582 loss: tensor(0.8164)\n",
      "train : 0.996593527702635\n",
      "test : 0.5926397092230804\n",
      "1583 loss: tensor(0.8162)\n",
      "train : 0.9964933373409478\n",
      "test : 0.591821899136756\n",
      "1584 loss: tensor(0.8161)\n",
      "train : 0.996593527702635\n",
      "test : 0.5925488414357111\n",
      "1585 loss: tensor(0.8159)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5920036347114948\n",
      "1586 loss: tensor(0.8158)\n",
      "train : 0.996593527702635\n",
      "test : 0.5924579736483416\n",
      "1587 loss: tensor(0.8157)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5920945024988642\n",
      "1588 loss: tensor(0.8155)\n",
      "train : 0.996593527702635\n",
      "test : 0.5920945024988642\n",
      "1589 loss: tensor(0.8154)\n",
      "train : 0.9964933373409478\n",
      "test : 0.5923671058609723\n",
      "1590 loss: tensor(0.8153)\n",
      "train : 0.996593527702635\n",
      "test : 0.5920036347114948\n",
      "1591 loss: tensor(0.8151)\n",
      "train : 0.9966937180643222\n",
      "test : 0.5924579736483416\n",
      "1592 loss: tensor(0.8150)\n",
      "train : 0.996593527702635\n",
      "test : 0.5919127669241254\n",
      "1593 loss: tensor(0.8149)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5925488414357111\n",
      "1594 loss: tensor(0.8148)\n",
      "train : 0.9966937180643222\n",
      "test : 0.5919127669241254\n",
      "1595 loss: tensor(0.8146)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5927305770104498\n",
      "1596 loss: tensor(0.8145)\n",
      "train : 0.9966937180643222\n",
      "test : 0.5919127669241254\n",
      "1597 loss: tensor(0.8144)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5925488414357111\n",
      "1598 loss: tensor(0.8142)\n",
      "train : 0.9966937180643222\n",
      "test : 0.5922762380736029\n",
      "1599 loss: tensor(0.8141)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5922762380736029\n",
      "1600 loss: tensor(0.8140)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5924579736483416\n",
      "1601 loss: tensor(0.8139)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5923671058609723\n",
      "1602 loss: tensor(0.8137)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5924579736483416\n",
      "1603 loss: tensor(0.8136)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5922762380736029\n",
      "1604 loss: tensor(0.8135)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5923671058609723\n",
      "1605 loss: tensor(0.8134)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5924579736483416\n",
      "1606 loss: tensor(0.8132)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5923671058609723\n",
      "1607 loss: tensor(0.8131)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5925488414357111\n",
      "1608 loss: tensor(0.8130)\n",
      "train : 0.9967939084260095\n",
      "test : 0.5923671058609723\n",
      "1609 loss: tensor(0.8129)\n",
      "train : 0.9968940987876966\n",
      "test : 0.5925488414357111\n",
      "1610 loss: tensor(0.8127)\n",
      "train : 0.9968940987876966\n",
      "test : 0.5922762380736029\n",
      "1611 loss: tensor(0.8126)\n",
      "train : 0.9968940987876966\n",
      "test : 0.5928214447978192\n",
      "1612 loss: tensor(0.8125)\n",
      "train : 0.9968940987876966\n",
      "test : 0.5923671058609723\n",
      "1613 loss: tensor(0.8124)\n",
      "train : 0.9969942891493838\n",
      "test : 0.5924579736483416\n",
      "1614 loss: tensor(0.8122)\n",
      "train : 0.9968940987876966\n",
      "test : 0.5925488414357111\n",
      "1615 loss: tensor(0.8121)\n",
      "train : 0.9969942891493838\n",
      "test : 0.5924579736483416\n",
      "1616 loss: tensor(0.8120)\n",
      "train : 0.9968940987876966\n",
      "test : 0.5926397092230804\n",
      "1617 loss: tensor(0.8119)\n",
      "train : 0.9969942891493838\n",
      "test : 0.5926397092230804\n",
      "1618 loss: tensor(0.8118)\n",
      "train : 0.9968940987876966\n",
      "test : 0.5926397092230804\n",
      "1619 loss: tensor(0.8116)\n",
      "train : 0.9969942891493838\n",
      "test : 0.5925488414357111\n",
      "1620 loss: tensor(0.8115)\n",
      "train : 0.9969942891493838\n",
      "test : 0.5924579736483416\n",
      "1621 loss: tensor(0.8114)\n",
      "train : 0.997094479511071\n",
      "test : 0.5923671058609723\n",
      "1622 loss: tensor(0.8113)\n",
      "train : 0.9969942891493838\n",
      "test : 0.5924579736483416\n",
      "1623 loss: tensor(0.8112)\n",
      "train : 0.997094479511071\n",
      "test : 0.5924579736483416\n",
      "1624 loss: tensor(0.8111)\n",
      "train : 0.997094479511071\n",
      "test : 0.5922762380736029\n",
      "1625 loss: tensor(0.8109)\n",
      "train : 0.9971946698727583\n",
      "test : 0.5924579736483416\n",
      "1626 loss: tensor(0.8108)\n",
      "train : 0.997094479511071\n",
      "test : 0.5922762380736029\n",
      "1627 loss: tensor(0.8107)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5924579736483416\n",
      "1628 loss: tensor(0.8106)\n",
      "train : 0.997094479511071\n",
      "test : 0.5921853702862335\n",
      "1629 loss: tensor(0.8105)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5923671058609723\n",
      "1630 loss: tensor(0.8104)\n",
      "train : 0.997094479511071\n",
      "test : 0.5922762380736029\n",
      "1631 loss: tensor(0.8102)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5922762380736029\n",
      "1632 loss: tensor(0.8101)\n",
      "train : 0.9971946698727583\n",
      "test : 0.5922762380736029\n",
      "1633 loss: tensor(0.8100)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5922762380736029\n",
      "1634 loss: tensor(0.8099)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5922762380736029\n",
      "1635 loss: tensor(0.8098)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5925488414357111\n",
      "1636 loss: tensor(0.8097)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5922762380736029\n",
      "1637 loss: tensor(0.8096)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5923671058609723\n",
      "1638 loss: tensor(0.8094)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5925488414357111\n",
      "1639 loss: tensor(0.8093)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5923671058609723\n",
      "1640 loss: tensor(0.8092)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5926397092230804\n",
      "1641 loss: tensor(0.8091)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5923671058609723\n",
      "1642 loss: tensor(0.8090)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5923671058609723\n",
      "1643 loss: tensor(0.8089)\n",
      "train : 0.9973950505961326\n",
      "test : 0.5923671058609723\n",
      "1644 loss: tensor(0.8088)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5924579736483416\n",
      "1645 loss: tensor(0.8087)\n",
      "train : 0.9973950505961326\n",
      "test : 0.5925488414357111\n",
      "1646 loss: tensor(0.8086)\n",
      "train : 0.9972948602344455\n",
      "test : 0.5922762380736029\n",
      "1647 loss: tensor(0.8084)\n",
      "train : 0.9973950505961326\n",
      "test : 0.5925488414357111\n",
      "1648 loss: tensor(0.8083)\n",
      "train : 0.9973950505961326\n",
      "test : 0.5922762380736029\n",
      "1649 loss: tensor(0.8082)\n",
      "train : 0.9973950505961326\n",
      "test : 0.5926397092230804\n",
      "1650 loss: tensor(0.8081)\n",
      "train : 0.9973950505961326\n",
      "test : 0.5920036347114948\n",
      "1651 loss: tensor(0.8080)\n",
      "train : 0.9973950505961326\n",
      "test : 0.5924579736483416\n",
      "1652 loss: tensor(0.8079)\n",
      "train : 0.9973950505961326\n",
      "test : 0.5920945024988642\n",
      "1653 loss: tensor(0.8078)\n",
      "train : 0.9974952409578198\n",
      "test : 0.5921853702862335\n",
      "1654 loss: tensor(0.8077)\n",
      "train : 0.9973950505961326\n",
      "test : 0.5920945024988642\n",
      "1655 loss: tensor(0.8076)\n",
      "train : 0.997595431319507\n",
      "test : 0.5921853702862335\n",
      "1656 loss: tensor(0.8074)\n",
      "train : 0.9974952409578198\n",
      "test : 0.5921853702862335\n",
      "1657 loss: tensor(0.8073)\n",
      "train : 0.997595431319507\n",
      "test : 0.5920036347114948\n",
      "1658 loss: tensor(0.8072)\n",
      "train : 0.9974952409578198\n",
      "test : 0.5920945024988642\n",
      "1659 loss: tensor(0.8071)\n",
      "train : 0.997595431319507\n",
      "test : 0.591821899136756\n",
      "1660 loss: tensor(0.8070)\n",
      "train : 0.9974952409578198\n",
      "test : 0.5919127669241254\n",
      "1661 loss: tensor(0.8069)\n",
      "train : 0.997595431319507\n",
      "test : 0.591821899136756\n",
      "1662 loss: tensor(0.8068)\n",
      "train : 0.9974952409578198\n",
      "test : 0.5917310313493866\n",
      "1663 loss: tensor(0.8067)\n",
      "train : 0.997595431319507\n",
      "test : 0.591821899136756\n",
      "1664 loss: tensor(0.8066)\n",
      "train : 0.9974952409578198\n",
      "test : 0.591821899136756\n",
      "1665 loss: tensor(0.8065)\n",
      "train : 0.997595431319507\n",
      "test : 0.5920036347114948\n",
      "1666 loss: tensor(0.8064)\n",
      "train : 0.9974952409578198\n",
      "test : 0.5917310313493866\n",
      "1667 loss: tensor(0.8063)\n",
      "train : 0.997595431319507\n",
      "test : 0.5920036347114948\n",
      "1668 loss: tensor(0.8062)\n",
      "train : 0.9974952409578198\n",
      "test : 0.5917310313493866\n",
      "1669 loss: tensor(0.8061)\n",
      "train : 0.997595431319507\n",
      "test : 0.5920945024988642\n",
      "1670 loss: tensor(0.8060)\n",
      "train : 0.9974952409578198\n",
      "test : 0.5917310313493866\n",
      "1671 loss: tensor(0.8059)\n",
      "train : 0.997595431319507\n",
      "test : 0.5920036347114948\n",
      "1672 loss: tensor(0.8058)\n",
      "train : 0.997595431319507\n",
      "test : 0.5919127669241254\n",
      "1673 loss: tensor(0.8056)\n",
      "train : 0.997595431319507\n",
      "test : 0.5922762380736029\n",
      "1674 loss: tensor(0.8055)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 0.997595431319507\n",
      "test : 0.5920036347114948\n",
      "1675 loss: tensor(0.8054)\n",
      "train : 0.997595431319507\n",
      "test : 0.5924579736483416\n",
      "1676 loss: tensor(0.8053)\n",
      "train : 0.997595431319507\n",
      "test : 0.5919127669241254\n",
      "1677 loss: tensor(0.8052)\n",
      "train : 0.997595431319507\n",
      "test : 0.5925488414357111\n",
      "1678 loss: tensor(0.8051)\n",
      "train : 0.9976956216811943\n",
      "test : 0.5916401635620173\n",
      "1679 loss: tensor(0.8050)\n",
      "train : 0.997595431319507\n",
      "test : 0.5925488414357111\n",
      "1680 loss: tensor(0.8049)\n",
      "train : 0.9976956216811943\n",
      "test : 0.5917310313493866\n",
      "1681 loss: tensor(0.8048)\n",
      "train : 0.997595431319507\n",
      "test : 0.5923671058609723\n",
      "1682 loss: tensor(0.8047)\n",
      "train : 0.9976956216811943\n",
      "test : 0.591821899136756\n",
      "1683 loss: tensor(0.8046)\n",
      "train : 0.9977958120428815\n",
      "test : 0.5924579736483416\n",
      "1684 loss: tensor(0.8045)\n",
      "train : 0.9976956216811943\n",
      "test : 0.5920036347114948\n",
      "1685 loss: tensor(0.8044)\n",
      "train : 0.9978960024045687\n",
      "test : 0.5922762380736029\n",
      "1686 loss: tensor(0.8043)\n",
      "train : 0.9976956216811943\n",
      "test : 0.5920945024988642\n",
      "1687 loss: tensor(0.8042)\n",
      "train : 0.9978960024045687\n",
      "test : 0.5922762380736029\n",
      "1688 loss: tensor(0.8041)\n",
      "train : 0.9976956216811943\n",
      "test : 0.5921853702862335\n",
      "1689 loss: tensor(0.8040)\n",
      "train : 0.9978960024045687\n",
      "test : 0.5923671058609723\n",
      "1690 loss: tensor(0.8039)\n",
      "train : 0.9976956216811943\n",
      "test : 0.5920945024988642\n",
      "1691 loss: tensor(0.8038)\n",
      "train : 0.9979961927662558\n",
      "test : 0.5921853702862335\n",
      "1692 loss: tensor(0.8037)\n",
      "train : 0.9976956216811943\n",
      "test : 0.5921853702862335\n",
      "1693 loss: tensor(0.8036)\n",
      "train : 0.9979961927662558\n",
      "test : 0.5921853702862335\n",
      "1694 loss: tensor(0.8035)\n",
      "train : 0.9977958120428815\n",
      "test : 0.5920945024988642\n",
      "1695 loss: tensor(0.8034)\n",
      "train : 0.9979961927662558\n",
      "test : 0.5921853702862335\n",
      "1696 loss: tensor(0.8033)\n",
      "train : 0.9977958120428815\n",
      "test : 0.5920945024988642\n",
      "1697 loss: tensor(0.8032)\n",
      "train : 0.9979961927662558\n",
      "test : 0.5921853702862335\n",
      "1698 loss: tensor(0.8031)\n",
      "train : 0.9977958120428815\n",
      "test : 0.5921853702862335\n",
      "1699 loss: tensor(0.8030)\n",
      "train : 0.9980963831279431\n",
      "test : 0.5921853702862335\n",
      "1700 loss: tensor(0.8030)\n",
      "train : 0.9977958120428815\n",
      "test : 0.5921853702862335\n",
      "1701 loss: tensor(0.8029)\n",
      "train : 0.9980963831279431\n",
      "test : 0.5920945024988642\n",
      "1702 loss: tensor(0.8028)\n",
      "train : 0.9979961927662558\n",
      "test : 0.5921853702862335\n",
      "1703 loss: tensor(0.8027)\n",
      "train : 0.9980963831279431\n",
      "test : 0.5919127669241254\n",
      "1704 loss: tensor(0.8026)\n",
      "train : 0.9980963831279431\n",
      "test : 0.5921853702862335\n",
      "1705 loss: tensor(0.8025)\n",
      "train : 0.9980963831279431\n",
      "test : 0.5920036347114948\n",
      "1706 loss: tensor(0.8024)\n",
      "train : 0.9980963831279431\n",
      "test : 0.5920945024988642\n",
      "1707 loss: tensor(0.8023)\n",
      "train : 0.9980963831279431\n",
      "test : 0.5917310313493866\n",
      "1708 loss: tensor(0.8022)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5922762380736029\n",
      "1709 loss: tensor(0.8021)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5917310313493866\n",
      "1710 loss: tensor(0.8020)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5921853702862335\n",
      "1711 loss: tensor(0.8019)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5919127669241254\n",
      "1712 loss: tensor(0.8018)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5920945024988642\n",
      "1713 loss: tensor(0.8017)\n",
      "train : 0.9981965734896303\n",
      "test : 0.591821899136756\n",
      "1714 loss: tensor(0.8016)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5920945024988642\n",
      "1715 loss: tensor(0.8015)\n",
      "train : 0.9981965734896303\n",
      "test : 0.591821899136756\n",
      "1716 loss: tensor(0.8014)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5920945024988642\n",
      "1717 loss: tensor(0.8014)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5916401635620173\n",
      "1718 loss: tensor(0.8013)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5920945024988642\n",
      "1719 loss: tensor(0.8012)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5916401635620173\n",
      "1720 loss: tensor(0.8011)\n",
      "train : 0.9982967638513175\n",
      "test : 0.5920945024988642\n",
      "1721 loss: tensor(0.8010)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5915492957746479\n",
      "1722 loss: tensor(0.8009)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5919127669241254\n",
      "1723 loss: tensor(0.8008)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5914584279872785\n",
      "1724 loss: tensor(0.8007)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5920036347114948\n",
      "1725 loss: tensor(0.8006)\n",
      "train : 0.9981965734896303\n",
      "test : 0.591821899136756\n",
      "1726 loss: tensor(0.8005)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5920036347114948\n",
      "1727 loss: tensor(0.8005)\n",
      "train : 0.9981965734896303\n",
      "test : 0.5916401635620173\n",
      "1728 loss: tensor(0.8004)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5920945024988642\n",
      "1729 loss: tensor(0.8003)\n",
      "train : 0.9982967638513175\n",
      "test : 0.591821899136756\n",
      "1730 loss: tensor(0.8002)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5919127669241254\n",
      "1731 loss: tensor(0.8001)\n",
      "train : 0.9982967638513175\n",
      "test : 0.5920036347114948\n",
      "1732 loss: tensor(0.8000)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5920036347114948\n",
      "1733 loss: tensor(0.7999)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5922762380736029\n",
      "1734 loss: tensor(0.7998)\n",
      "train : 0.9983969542130047\n",
      "test : 0.591821899136756\n",
      "1735 loss: tensor(0.7998)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5921853702862335\n",
      "1736 loss: tensor(0.7997)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5919127669241254\n",
      "1737 loss: tensor(0.7996)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5920945024988642\n",
      "1738 loss: tensor(0.7995)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5920036347114948\n",
      "1739 loss: tensor(0.7994)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5920945024988642\n",
      "1740 loss: tensor(0.7993)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5920945024988642\n",
      "1741 loss: tensor(0.7992)\n",
      "train : 0.998497144574692\n",
      "test : 0.591821899136756\n",
      "1742 loss: tensor(0.7992)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5919127669241254\n",
      "1743 loss: tensor(0.7991)\n",
      "train : 0.998497144574692\n",
      "test : 0.5921853702862335\n",
      "1744 loss: tensor(0.7990)\n",
      "train : 0.9983969542130047\n",
      "test : 0.591821899136756\n",
      "1745 loss: tensor(0.7989)\n",
      "train : 0.998497144574692\n",
      "test : 0.5920036347114948\n",
      "1746 loss: tensor(0.7988)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5917310313493866\n",
      "1747 loss: tensor(0.7987)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5921853702862335\n",
      "1748 loss: tensor(0.7986)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5913675601999091\n",
      "1749 loss: tensor(0.7985)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5919127669241254\n",
      "1750 loss: tensor(0.7985)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5915492957746479\n",
      "1751 loss: tensor(0.7984)\n",
      "train : 0.9985973349363791\n",
      "test : 0.591821899136756\n",
      "1752 loss: tensor(0.7983)\n",
      "train : 0.9983969542130047\n",
      "test : 0.5919127669241254\n",
      "1753 loss: tensor(0.7982)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5915492957746479\n",
      "1754 loss: tensor(0.7981)\n",
      "train : 0.9983969542130047\n",
      "test : 0.591821899136756\n",
      "1755 loss: tensor(0.7980)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5915492957746479\n",
      "1756 loss: tensor(0.7979)\n",
      "train : 0.998497144574692\n",
      "test : 0.5919127669241254\n",
      "1757 loss: tensor(0.7979)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5917310313493866\n",
      "1758 loss: tensor(0.7978)\n",
      "train : 0.998497144574692\n",
      "test : 0.591821899136756\n",
      "1759 loss: tensor(0.7977)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5917310313493866\n",
      "1760 loss: tensor(0.7976)\n",
      "train : 0.998497144574692\n",
      "test : 0.591821899136756\n",
      "1761 loss: tensor(0.7975)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5917310313493866\n",
      "1762 loss: tensor(0.7975)\n",
      "train : 0.998497144574692\n",
      "test : 0.5917310313493866\n",
      "1763 loss: tensor(0.7974)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5917310313493866\n",
      "1764 loss: tensor(0.7973)\n",
      "train : 0.998497144574692\n",
      "test : 0.5917310313493866\n",
      "1765 loss: tensor(0.7972)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5916401635620173\n",
      "1766 loss: tensor(0.7971)\n",
      "train : 0.9985973349363791\n",
      "test : 0.591821899136756\n",
      "1767 loss: tensor(0.7970)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5917310313493866\n",
      "1768 loss: tensor(0.7970)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5916401635620173\n",
      "1769 loss: tensor(0.7969)\n",
      "train : 0.9985973349363791\n",
      "test : 0.591821899136756\n",
      "1770 loss: tensor(0.7968)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5917310313493866\n",
      "1771 loss: tensor(0.7967)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5917310313493866\n",
      "1772 loss: tensor(0.7966)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5916401635620173\n",
      "1773 loss: tensor(0.7966)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5919127669241254\n",
      "1774 loss: tensor(0.7965)\n",
      "train : 0.9985973349363791\n",
      "test : 0.591821899136756\n",
      "1775 loss: tensor(0.7964)\n",
      "train : 0.9985973349363791\n",
      "test : 0.591821899136756\n",
      "1776 loss: tensor(0.7963)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5920036347114948\n",
      "1777 loss: tensor(0.7962)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5917310313493866\n",
      "1778 loss: tensor(0.7962)\n",
      "train : 0.9985973349363791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.5921853702862335\n",
      "1779 loss: tensor(0.7961)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5917310313493866\n",
      "1780 loss: tensor(0.7960)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5924579736483416\n",
      "1781 loss: tensor(0.7959)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5915492957746479\n",
      "1782 loss: tensor(0.7959)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5922762380736029\n",
      "1783 loss: tensor(0.7958)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5916401635620173\n",
      "1784 loss: tensor(0.7957)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5921853702862335\n",
      "1785 loss: tensor(0.7956)\n",
      "train : 0.9985973349363791\n",
      "test : 0.591821899136756\n",
      "1786 loss: tensor(0.7956)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5922762380736029\n",
      "1787 loss: tensor(0.7955)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5919127669241254\n",
      "1788 loss: tensor(0.7954)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5922762380736029\n",
      "1789 loss: tensor(0.7953)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5919127669241254\n",
      "1790 loss: tensor(0.7952)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5923671058609723\n",
      "1791 loss: tensor(0.7952)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5917310313493866\n",
      "1792 loss: tensor(0.7951)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5921853702862335\n",
      "1793 loss: tensor(0.7950)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5914584279872785\n",
      "1794 loss: tensor(0.7949)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5922762380736029\n",
      "1795 loss: tensor(0.7949)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5913675601999091\n",
      "1796 loss: tensor(0.7948)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5921853702862335\n",
      "1797 loss: tensor(0.7947)\n",
      "train : 0.9985973349363791\n",
      "test : 0.591094956837801\n",
      "1798 loss: tensor(0.7946)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5919127669241254\n",
      "1799 loss: tensor(0.7946)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5913675601999091\n",
      "1800 loss: tensor(0.7945)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5921853702862335\n",
      "1801 loss: tensor(0.7944)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5912766924125398\n",
      "1802 loss: tensor(0.7943)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5920945024988642\n",
      "1803 loss: tensor(0.7943)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5915492957746479\n",
      "1804 loss: tensor(0.7942)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5920036347114948\n",
      "1805 loss: tensor(0.7941)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5917310313493866\n",
      "1806 loss: tensor(0.7940)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5920036347114948\n",
      "1807 loss: tensor(0.7940)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5915492957746479\n",
      "1808 loss: tensor(0.7939)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5920945024988642\n",
      "1809 loss: tensor(0.7938)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5914584279872785\n",
      "1810 loss: tensor(0.7937)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5919127669241254\n",
      "1811 loss: tensor(0.7937)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5914584279872785\n",
      "1812 loss: tensor(0.7936)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5916401635620173\n",
      "1813 loss: tensor(0.7935)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5916401635620173\n",
      "1814 loss: tensor(0.7935)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5916401635620173\n",
      "1815 loss: tensor(0.7934)\n",
      "train : 0.9985973349363791\n",
      "test : 0.5917310313493866\n",
      "1816 loss: tensor(0.7933)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5916401635620173\n",
      "1817 loss: tensor(0.7932)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5916401635620173\n",
      "1818 loss: tensor(0.7932)\n",
      "train : 0.9986975252980663\n",
      "test : 0.591821899136756\n",
      "1819 loss: tensor(0.7931)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5916401635620173\n",
      "1820 loss: tensor(0.7930)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5919127669241254\n",
      "1821 loss: tensor(0.7929)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5916401635620173\n",
      "1822 loss: tensor(0.7929)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5920036347114948\n",
      "1823 loss: tensor(0.7928)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5916401635620173\n",
      "1824 loss: tensor(0.7927)\n",
      "train : 0.9987977156597535\n",
      "test : 0.5920036347114948\n",
      "1825 loss: tensor(0.7927)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5917310313493866\n",
      "1826 loss: tensor(0.7926)\n",
      "train : 0.9987977156597535\n",
      "test : 0.5919127669241254\n",
      "1827 loss: tensor(0.7925)\n",
      "train : 0.9986975252980663\n",
      "test : 0.591821899136756\n",
      "1828 loss: tensor(0.7925)\n",
      "train : 0.9987977156597535\n",
      "test : 0.5917310313493866\n",
      "1829 loss: tensor(0.7924)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5920036347114948\n",
      "1830 loss: tensor(0.7923)\n",
      "train : 0.9987977156597535\n",
      "test : 0.5917310313493866\n",
      "1831 loss: tensor(0.7922)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5921853702862335\n",
      "1832 loss: tensor(0.7922)\n",
      "train : 0.9987977156597535\n",
      "test : 0.5919127669241254\n",
      "1833 loss: tensor(0.7921)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5922762380736029\n",
      "1834 loss: tensor(0.7920)\n",
      "train : 0.9987977156597535\n",
      "test : 0.5919127669241254\n",
      "1835 loss: tensor(0.7920)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5920036347114948\n",
      "1836 loss: tensor(0.7919)\n",
      "train : 0.9987977156597535\n",
      "test : 0.5920036347114948\n",
      "1837 loss: tensor(0.7918)\n",
      "train : 0.9986975252980663\n",
      "test : 0.5920945024988642\n",
      "1838 loss: tensor(0.7918)\n",
      "train : 0.9988979060214407\n",
      "test : 0.591821899136756\n",
      "1839 loss: tensor(0.7917)\n",
      "train : 0.9987977156597535\n",
      "test : 0.5920945024988642\n",
      "1840 loss: tensor(0.7916)\n",
      "train : 0.9988979060214407\n",
      "test : 0.5919127669241254\n",
      "1841 loss: tensor(0.7916)\n",
      "train : 0.9987977156597535\n",
      "test : 0.5920945024988642\n",
      "1842 loss: tensor(0.7915)\n",
      "train : 0.9988979060214407\n",
      "test : 0.5917310313493866\n",
      "1843 loss: tensor(0.7914)\n",
      "train : 0.9988979060214407\n",
      "test : 0.5920036347114948\n",
      "1844 loss: tensor(0.7914)\n",
      "train : 0.9988979060214407\n",
      "test : 0.5916401635620173\n",
      "1845 loss: tensor(0.7913)\n",
      "train : 0.9988979060214407\n",
      "test : 0.5920945024988642\n",
      "1846 loss: tensor(0.7912)\n",
      "train : 0.9988979060214407\n",
      "test : 0.5917310313493866\n",
      "1847 loss: tensor(0.7912)\n",
      "train : 0.998998096383128\n",
      "test : 0.5920036347114948\n",
      "1848 loss: tensor(0.7911)\n",
      "train : 0.9988979060214407\n",
      "test : 0.5917310313493866\n",
      "1849 loss: tensor(0.7910)\n",
      "train : 0.998998096383128\n",
      "test : 0.5919127669241254\n",
      "1850 loss: tensor(0.7910)\n",
      "train : 0.9988979060214407\n",
      "test : 0.5917310313493866\n",
      "1851 loss: tensor(0.7909)\n",
      "train : 0.998998096383128\n",
      "test : 0.5919127669241254\n",
      "1852 loss: tensor(0.7908)\n",
      "train : 0.9988979060214407\n",
      "test : 0.5916401635620173\n",
      "1853 loss: tensor(0.7908)\n",
      "train : 0.998998096383128\n",
      "test : 0.5920036347114948\n",
      "1854 loss: tensor(0.7907)\n",
      "train : 0.9988979060214407\n",
      "test : 0.591821899136756\n",
      "1855 loss: tensor(0.7906)\n",
      "train : 0.998998096383128\n",
      "test : 0.5917310313493866\n",
      "1856 loss: tensor(0.7906)\n",
      "train : 0.998998096383128\n",
      "test : 0.591821899136756\n",
      "1857 loss: tensor(0.7905)\n",
      "train : 0.998998096383128\n",
      "test : 0.5917310313493866\n",
      "1858 loss: tensor(0.7904)\n",
      "train : 0.998998096383128\n",
      "test : 0.5917310313493866\n",
      "1859 loss: tensor(0.7904)\n",
      "train : 0.998998096383128\n",
      "test : 0.591821899136756\n",
      "1860 loss: tensor(0.7903)\n",
      "train : 0.998998096383128\n",
      "test : 0.5919127669241254\n",
      "1861 loss: tensor(0.7902)\n",
      "train : 0.998998096383128\n",
      "test : 0.5917310313493866\n",
      "1862 loss: tensor(0.7902)\n",
      "train : 0.998998096383128\n",
      "test : 0.591821899136756\n",
      "1863 loss: tensor(0.7901)\n",
      "train : 0.998998096383128\n",
      "test : 0.5915492957746479\n",
      "1864 loss: tensor(0.7900)\n",
      "train : 0.998998096383128\n",
      "test : 0.591821899136756\n",
      "1865 loss: tensor(0.7900)\n",
      "train : 0.998998096383128\n",
      "test : 0.5914584279872785\n",
      "1866 loss: tensor(0.7899)\n",
      "train : 0.998998096383128\n",
      "test : 0.5917310313493866\n",
      "1867 loss: tensor(0.7898)\n",
      "train : 0.998998096383128\n",
      "test : 0.5913675601999091\n",
      "1868 loss: tensor(0.7898)\n",
      "train : 0.998998096383128\n",
      "test : 0.5915492957746479\n",
      "1869 loss: tensor(0.7897)\n",
      "train : 0.998998096383128\n",
      "test : 0.5912766924125398\n",
      "1870 loss: tensor(0.7896)\n",
      "train : 0.998998096383128\n",
      "test : 0.5915492957746479\n",
      "1871 loss: tensor(0.7896)\n",
      "train : 0.998998096383128\n",
      "test : 0.5915492957746479\n",
      "1872 loss: tensor(0.7895)\n",
      "train : 0.998998096383128\n",
      "test : 0.5915492957746479\n",
      "1873 loss: tensor(0.7895)\n",
      "train : 0.998998096383128\n",
      "test : 0.5915492957746479\n",
      "1874 loss: tensor(0.7894)\n",
      "train : 0.998998096383128\n",
      "test : 0.5916401635620173\n",
      "1875 loss: tensor(0.7893)\n",
      "train : 0.998998096383128\n",
      "test : 0.5914584279872785\n",
      "1876 loss: tensor(0.7893)\n",
      "train : 0.998998096383128\n",
      "test : 0.5916401635620173\n",
      "1877 loss: tensor(0.7892)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5912766924125398\n",
      "1878 loss: tensor(0.7891)\n",
      "train : 0.998998096383128\n",
      "test : 0.5914584279872785\n",
      "1879 loss: tensor(0.7891)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5914584279872785\n",
      "1880 loss: tensor(0.7890)\n",
      "train : 0.998998096383128\n",
      "test : 0.5912766924125398\n",
      "1881 loss: tensor(0.7890)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5915492957746479\n",
      "1882 loss: tensor(0.7889)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5912766924125398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1883 loss: tensor(0.7888)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5915492957746479\n",
      "1884 loss: tensor(0.7888)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5912766924125398\n",
      "1885 loss: tensor(0.7887)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5915492957746479\n",
      "1886 loss: tensor(0.7886)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5913675601999091\n",
      "1887 loss: tensor(0.7886)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5914584279872785\n",
      "1888 loss: tensor(0.7885)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5914584279872785\n",
      "1889 loss: tensor(0.7885)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5914584279872785\n",
      "1890 loss: tensor(0.7884)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5914584279872785\n",
      "1891 loss: tensor(0.7883)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5916401635620173\n",
      "1892 loss: tensor(0.7883)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5913675601999091\n",
      "1893 loss: tensor(0.7882)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5914584279872785\n",
      "1894 loss: tensor(0.7882)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5912766924125398\n",
      "1895 loss: tensor(0.7881)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5913675601999091\n",
      "1896 loss: tensor(0.7880)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5912766924125398\n",
      "1897 loss: tensor(0.7880)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5913675601999091\n",
      "1898 loss: tensor(0.7879)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5913675601999091\n",
      "1899 loss: tensor(0.7879)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5911858246251703\n",
      "1900 loss: tensor(0.7878)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5914584279872785\n",
      "1901 loss: tensor(0.7877)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5912766924125398\n",
      "1902 loss: tensor(0.7877)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5916401635620173\n",
      "1903 loss: tensor(0.7876)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5913675601999091\n",
      "1904 loss: tensor(0.7876)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5916401635620173\n",
      "1905 loss: tensor(0.7875)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5912766924125398\n",
      "1906 loss: tensor(0.7874)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5914584279872785\n",
      "1907 loss: tensor(0.7874)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5912766924125398\n",
      "1908 loss: tensor(0.7873)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5914584279872785\n",
      "1909 loss: tensor(0.7873)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5913675601999091\n",
      "1910 loss: tensor(0.7872)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5913675601999091\n",
      "1911 loss: tensor(0.7871)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5913675601999091\n",
      "1912 loss: tensor(0.7871)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5912766924125398\n",
      "1913 loss: tensor(0.7870)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5913675601999091\n",
      "1914 loss: tensor(0.7870)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5913675601999091\n",
      "1915 loss: tensor(0.7869)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5914584279872785\n",
      "1916 loss: tensor(0.7869)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5913675601999091\n",
      "1917 loss: tensor(0.7868)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5913675601999091\n",
      "1918 loss: tensor(0.7867)\n",
      "train : 0.9990982867448152\n",
      "test : 0.591094956837801\n",
      "1919 loss: tensor(0.7867)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5912766924125398\n",
      "1920 loss: tensor(0.7866)\n",
      "train : 0.9990982867448152\n",
      "test : 0.5911858246251703\n",
      "1921 loss: tensor(0.7866)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5913675601999091\n",
      "1922 loss: tensor(0.7865)\n",
      "train : 0.9991984771065023\n",
      "test : 0.591094956837801\n",
      "1923 loss: tensor(0.7865)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5913675601999091\n",
      "1924 loss: tensor(0.7864)\n",
      "train : 0.9991984771065023\n",
      "test : 0.591094956837801\n",
      "1925 loss: tensor(0.7863)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5914584279872785\n",
      "1926 loss: tensor(0.7863)\n",
      "train : 0.9991984771065023\n",
      "test : 0.591094956837801\n",
      "1927 loss: tensor(0.7862)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5914584279872785\n",
      "1928 loss: tensor(0.7862)\n",
      "train : 0.9991984771065023\n",
      "test : 0.591094956837801\n",
      "1929 loss: tensor(0.7861)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5914584279872785\n",
      "1930 loss: tensor(0.7861)\n",
      "train : 0.9991984771065023\n",
      "test : 0.591094956837801\n",
      "1931 loss: tensor(0.7860)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5913675601999091\n",
      "1932 loss: tensor(0.7859)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5910040890504317\n",
      "1933 loss: tensor(0.7859)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5913675601999091\n",
      "1934 loss: tensor(0.7858)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5911858246251703\n",
      "1935 loss: tensor(0.7858)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5913675601999091\n",
      "1936 loss: tensor(0.7857)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5911858246251703\n",
      "1937 loss: tensor(0.7857)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5915492957746479\n",
      "1938 loss: tensor(0.7856)\n",
      "train : 0.9991984771065023\n",
      "test : 0.5911858246251703\n",
      "1939 loss: tensor(0.7856)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5914584279872785\n",
      "1940 loss: tensor(0.7855)\n",
      "train : 0.9991984771065023\n",
      "test : 0.591094956837801\n",
      "1941 loss: tensor(0.7854)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5915492957746479\n",
      "1942 loss: tensor(0.7854)\n",
      "train : 0.9991984771065023\n",
      "test : 0.591094956837801\n",
      "1943 loss: tensor(0.7853)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5915492957746479\n",
      "1944 loss: tensor(0.7853)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5911858246251703\n",
      "1945 loss: tensor(0.7852)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5913675601999091\n",
      "1946 loss: tensor(0.7852)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5911858246251703\n",
      "1947 loss: tensor(0.7851)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5913675601999091\n",
      "1948 loss: tensor(0.7851)\n",
      "train : 0.9992986674681895\n",
      "test : 0.591094956837801\n",
      "1949 loss: tensor(0.7850)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5912766924125398\n",
      "1950 loss: tensor(0.7850)\n",
      "train : 0.9992986674681895\n",
      "test : 0.591094956837801\n",
      "1951 loss: tensor(0.7849)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5911858246251703\n",
      "1952 loss: tensor(0.7849)\n",
      "train : 0.9992986674681895\n",
      "test : 0.591094956837801\n",
      "1953 loss: tensor(0.7848)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5911858246251703\n",
      "1954 loss: tensor(0.7847)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5910040890504317\n",
      "1955 loss: tensor(0.7847)\n",
      "train : 0.9993988578298768\n",
      "test : 0.591094956837801\n",
      "1956 loss: tensor(0.7846)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5910040890504317\n",
      "1957 loss: tensor(0.7846)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5910040890504317\n",
      "1958 loss: tensor(0.7845)\n",
      "train : 0.9992986674681895\n",
      "test : 0.591094956837801\n",
      "1959 loss: tensor(0.7845)\n",
      "train : 0.9993988578298768\n",
      "test : 0.591094956837801\n",
      "1960 loss: tensor(0.7844)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5912766924125398\n",
      "1961 loss: tensor(0.7844)\n",
      "train : 0.9993988578298768\n",
      "test : 0.591094956837801\n",
      "1962 loss: tensor(0.7843)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5912766924125398\n",
      "1963 loss: tensor(0.7843)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5910040890504317\n",
      "1964 loss: tensor(0.7842)\n",
      "train : 0.9992986674681895\n",
      "test : 0.5911858246251703\n",
      "1965 loss: tensor(0.7842)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5910040890504317\n",
      "1966 loss: tensor(0.7841)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5911858246251703\n",
      "1967 loss: tensor(0.7841)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5909132212630622\n",
      "1968 loss: tensor(0.7840)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5912766924125398\n",
      "1969 loss: tensor(0.7840)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5909132212630622\n",
      "1970 loss: tensor(0.7839)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5913675601999091\n",
      "1971 loss: tensor(0.7838)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5910040890504317\n",
      "1972 loss: tensor(0.7838)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5913675601999091\n",
      "1973 loss: tensor(0.7837)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5909132212630622\n",
      "1974 loss: tensor(0.7837)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5914584279872785\n",
      "1975 loss: tensor(0.7836)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5908223534756929\n",
      "1976 loss: tensor(0.7836)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5912766924125398\n",
      "1977 loss: tensor(0.7835)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5908223534756929\n",
      "1978 loss: tensor(0.7835)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5911858246251703\n",
      "1979 loss: tensor(0.7834)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5907314856883235\n",
      "1980 loss: tensor(0.7834)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5910040890504317\n",
      "1981 loss: tensor(0.7833)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5907314856883235\n",
      "1982 loss: tensor(0.7833)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5910040890504317\n",
      "1983 loss: tensor(0.7832)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5907314856883235\n",
      "1984 loss: tensor(0.7832)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5910040890504317\n",
      "1985 loss: tensor(0.7831)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5908223534756929\n",
      "1986 loss: tensor(0.7831)\n",
      "train : 0.999499048191564\n",
      "test : 0.5911858246251703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987 loss: tensor(0.7830)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5908223534756929\n",
      "1988 loss: tensor(0.7830)\n",
      "train : 0.999499048191564\n",
      "test : 0.5910040890504317\n",
      "1989 loss: tensor(0.7829)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5907314856883235\n",
      "1990 loss: tensor(0.7829)\n",
      "train : 0.999499048191564\n",
      "test : 0.5910040890504317\n",
      "1991 loss: tensor(0.7828)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5908223534756929\n",
      "1992 loss: tensor(0.7828)\n",
      "train : 0.999499048191564\n",
      "test : 0.5910040890504317\n",
      "1993 loss: tensor(0.7827)\n",
      "train : 0.9993988578298768\n",
      "test : 0.5908223534756929\n",
      "1994 loss: tensor(0.7827)\n",
      "train : 0.999499048191564\n",
      "test : 0.5910040890504317\n",
      "1995 loss: tensor(0.7826)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "1996 loss: tensor(0.7826)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "1997 loss: tensor(0.7825)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "1998 loss: tensor(0.7825)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "1999 loss: tensor(0.7824)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "2000 loss: tensor(0.7824)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2001 loss: tensor(0.7823)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "2002 loss: tensor(0.7823)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2003 loss: tensor(0.7822)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2004 loss: tensor(0.7822)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "2005 loss: tensor(0.7822)\n",
      "train : 0.999499048191564\n",
      "test : 0.591094956837801\n",
      "2006 loss: tensor(0.7821)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "2007 loss: tensor(0.7821)\n",
      "train : 0.999499048191564\n",
      "test : 0.5911858246251703\n",
      "2008 loss: tensor(0.7820)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "2009 loss: tensor(0.7820)\n",
      "train : 0.999499048191564\n",
      "test : 0.5911858246251703\n",
      "2010 loss: tensor(0.7819)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2011 loss: tensor(0.7819)\n",
      "train : 0.999499048191564\n",
      "test : 0.591094956837801\n",
      "2012 loss: tensor(0.7818)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2013 loss: tensor(0.7818)\n",
      "train : 0.999499048191564\n",
      "test : 0.5911858246251703\n",
      "2014 loss: tensor(0.7817)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2015 loss: tensor(0.7817)\n",
      "train : 0.999499048191564\n",
      "test : 0.591094956837801\n",
      "2016 loss: tensor(0.7816)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2017 loss: tensor(0.7816)\n",
      "train : 0.999499048191564\n",
      "test : 0.5911858246251703\n",
      "2018 loss: tensor(0.7815)\n",
      "train : 0.999499048191564\n",
      "test : 0.5906406179009541\n",
      "2019 loss: tensor(0.7815)\n",
      "train : 0.999499048191564\n",
      "test : 0.5910040890504317\n",
      "2020 loss: tensor(0.7814)\n",
      "train : 0.999499048191564\n",
      "test : 0.5906406179009541\n",
      "2021 loss: tensor(0.7814)\n",
      "train : 0.999499048191564\n",
      "test : 0.591094956837801\n",
      "2022 loss: tensor(0.7813)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "2023 loss: tensor(0.7813)\n",
      "train : 0.999499048191564\n",
      "test : 0.5911858246251703\n",
      "2024 loss: tensor(0.7813)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "2025 loss: tensor(0.7812)\n",
      "train : 0.999499048191564\n",
      "test : 0.5912766924125398\n",
      "2026 loss: tensor(0.7812)\n",
      "train : 0.999499048191564\n",
      "test : 0.5906406179009541\n",
      "2027 loss: tensor(0.7811)\n",
      "train : 0.999499048191564\n",
      "test : 0.591094956837801\n",
      "2028 loss: tensor(0.7811)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2029 loss: tensor(0.7810)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "2030 loss: tensor(0.7810)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "2031 loss: tensor(0.7809)\n",
      "train : 0.999499048191564\n",
      "test : 0.5906406179009541\n",
      "2032 loss: tensor(0.7809)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "2033 loss: tensor(0.7808)\n",
      "train : 0.999499048191564\n",
      "test : 0.5906406179009541\n",
      "2034 loss: tensor(0.7808)\n",
      "train : 0.999499048191564\n",
      "test : 0.5906406179009541\n",
      "2035 loss: tensor(0.7807)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "2036 loss: tensor(0.7807)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "2037 loss: tensor(0.7807)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "2038 loss: tensor(0.7806)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2039 loss: tensor(0.7806)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "2040 loss: tensor(0.7805)\n",
      "train : 0.999499048191564\n",
      "test : 0.5910040890504317\n",
      "2041 loss: tensor(0.7805)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2042 loss: tensor(0.7804)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2043 loss: tensor(0.7804)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2044 loss: tensor(0.7803)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2045 loss: tensor(0.7803)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2046 loss: tensor(0.7802)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "2047 loss: tensor(0.7802)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2048 loss: tensor(0.7802)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "2049 loss: tensor(0.7801)\n",
      "train : 0.999499048191564\n",
      "test : 0.5910040890504317\n",
      "2050 loss: tensor(0.7801)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "2051 loss: tensor(0.7800)\n",
      "train : 0.999499048191564\n",
      "test : 0.5910040890504317\n",
      "2052 loss: tensor(0.7800)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "2053 loss: tensor(0.7799)\n",
      "train : 0.999499048191564\n",
      "test : 0.591094956837801\n",
      "2054 loss: tensor(0.7799)\n",
      "train : 0.999499048191564\n",
      "test : 0.5907314856883235\n",
      "2055 loss: tensor(0.7799)\n",
      "train : 0.999499048191564\n",
      "test : 0.591094956837801\n",
      "2056 loss: tensor(0.7798)\n",
      "train : 0.999499048191564\n",
      "test : 0.5906406179009541\n",
      "2057 loss: tensor(0.7798)\n",
      "train : 0.999499048191564\n",
      "test : 0.5910040890504317\n",
      "2058 loss: tensor(0.7797)\n",
      "train : 0.999499048191564\n",
      "test : 0.5906406179009541\n",
      "2059 loss: tensor(0.7797)\n",
      "train : 0.999499048191564\n",
      "test : 0.591094956837801\n",
      "2060 loss: tensor(0.7796)\n",
      "train : 0.999499048191564\n",
      "test : 0.5906406179009541\n",
      "2061 loss: tensor(0.7796)\n",
      "train : 0.999499048191564\n",
      "test : 0.5910040890504317\n",
      "2062 loss: tensor(0.7795)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5906406179009541\n",
      "2063 loss: tensor(0.7795)\n",
      "train : 0.999499048191564\n",
      "test : 0.5910040890504317\n",
      "2064 loss: tensor(0.7795)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5904588823262154\n",
      "2065 loss: tensor(0.7794)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2066 loss: tensor(0.7794)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5906406179009541\n",
      "2067 loss: tensor(0.7793)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "2068 loss: tensor(0.7793)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5906406179009541\n",
      "2069 loss: tensor(0.7792)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "2070 loss: tensor(0.7792)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5906406179009541\n",
      "2071 loss: tensor(0.7792)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2072 loss: tensor(0.7791)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5906406179009541\n",
      "2073 loss: tensor(0.7791)\n",
      "train : 0.999499048191564\n",
      "test : 0.5908223534756929\n",
      "2074 loss: tensor(0.7790)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5905497501135847\n",
      "2075 loss: tensor(0.7790)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "2076 loss: tensor(0.7789)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5905497501135847\n",
      "2077 loss: tensor(0.7789)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "2078 loss: tensor(0.7789)\n",
      "train : 0.9995992385532512\n",
      "test : 0.590368014538846\n",
      "2079 loss: tensor(0.7788)\n",
      "train : 0.999499048191564\n",
      "test : 0.5909132212630622\n",
      "2080 loss: tensor(0.7788)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2081 loss: tensor(0.7787)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5909132212630622\n",
      "2082 loss: tensor(0.7787)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2083 loss: tensor(0.7787)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5909132212630622\n",
      "2084 loss: tensor(0.7786)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5906406179009541\n",
      "2085 loss: tensor(0.7786)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5908223534756929\n",
      "2086 loss: tensor(0.7785)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5907314856883235\n",
      "2087 loss: tensor(0.7785)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5906406179009541\n",
      "2088 loss: tensor(0.7785)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5907314856883235\n",
      "2089 loss: tensor(0.7784)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5906406179009541\n",
      "2090 loss: tensor(0.7784)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5907314856883235\n",
      "2091 loss: tensor(0.7783)\n",
      "train : 0.9995992385532512\n",
      "test : 0.5906406179009541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092 loss: tensor(0.7783)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5907314856883235\n",
      "2093 loss: tensor(0.7782)\n",
      "train : 0.9995992385532512\n",
      "test : 0.590368014538846\n",
      "2094 loss: tensor(0.7782)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5907314856883235\n",
      "2095 loss: tensor(0.7782)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5902771467514766\n",
      "2096 loss: tensor(0.7781)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5909132212630622\n",
      "2097 loss: tensor(0.7781)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5902771467514766\n",
      "2098 loss: tensor(0.7780)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5908223534756929\n",
      "2099 loss: tensor(0.7780)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5901862789641072\n",
      "2100 loss: tensor(0.7780)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5910040890504317\n",
      "2101 loss: tensor(0.7779)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5900954111767378\n",
      "2102 loss: tensor(0.7779)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5911858246251703\n",
      "2103 loss: tensor(0.7778)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5900954111767378\n",
      "2104 loss: tensor(0.7778)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5911858246251703\n",
      "2105 loss: tensor(0.7778)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5900954111767378\n",
      "2106 loss: tensor(0.7777)\n",
      "train : 0.9996994289149383\n",
      "test : 0.591094956837801\n",
      "2107 loss: tensor(0.7777)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5900954111767378\n",
      "2108 loss: tensor(0.7776)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5911858246251703\n",
      "2109 loss: tensor(0.7776)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5901862789641072\n",
      "2110 loss: tensor(0.7776)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5911858246251703\n",
      "2111 loss: tensor(0.7775)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5902771467514766\n",
      "2112 loss: tensor(0.7775)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5911858246251703\n",
      "2113 loss: tensor(0.7774)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2114 loss: tensor(0.7774)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5910040890504317\n",
      "2115 loss: tensor(0.7774)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2116 loss: tensor(0.7773)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5910040890504317\n",
      "2117 loss: tensor(0.7773)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2118 loss: tensor(0.7772)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5908223534756929\n",
      "2119 loss: tensor(0.7772)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2120 loss: tensor(0.7772)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5908223534756929\n",
      "2121 loss: tensor(0.7771)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2122 loss: tensor(0.7771)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5907314856883235\n",
      "2123 loss: tensor(0.7770)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5907314856883235\n",
      "2124 loss: tensor(0.7770)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5906406179009541\n",
      "2125 loss: tensor(0.7770)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5908223534756929\n",
      "2126 loss: tensor(0.7769)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5906406179009541\n",
      "2127 loss: tensor(0.7769)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5908223534756929\n",
      "2128 loss: tensor(0.7769)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2129 loss: tensor(0.7768)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5909132212630622\n",
      "2130 loss: tensor(0.7768)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2131 loss: tensor(0.7767)\n",
      "train : 0.9996994289149383\n",
      "test : 0.591094956837801\n",
      "2132 loss: tensor(0.7767)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2133 loss: tensor(0.7767)\n",
      "train : 0.9996994289149383\n",
      "test : 0.591094956837801\n",
      "2134 loss: tensor(0.7766)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2135 loss: tensor(0.7766)\n",
      "train : 0.9996994289149383\n",
      "test : 0.591094956837801\n",
      "2136 loss: tensor(0.7766)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2137 loss: tensor(0.7765)\n",
      "train : 0.9996994289149383\n",
      "test : 0.591094956837801\n",
      "2138 loss: tensor(0.7765)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2139 loss: tensor(0.7764)\n",
      "train : 0.9996994289149383\n",
      "test : 0.591094956837801\n",
      "2140 loss: tensor(0.7764)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2141 loss: tensor(0.7764)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5911858246251703\n",
      "2142 loss: tensor(0.7763)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2143 loss: tensor(0.7763)\n",
      "train : 0.9996994289149383\n",
      "test : 0.591094956837801\n",
      "2144 loss: tensor(0.7762)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2145 loss: tensor(0.7762)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5911858246251703\n",
      "2146 loss: tensor(0.7762)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2147 loss: tensor(0.7761)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5909132212630622\n",
      "2148 loss: tensor(0.7761)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2149 loss: tensor(0.7761)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5909132212630622\n",
      "2150 loss: tensor(0.7760)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2151 loss: tensor(0.7760)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5909132212630622\n",
      "2152 loss: tensor(0.7759)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2153 loss: tensor(0.7759)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5906406179009541\n",
      "2154 loss: tensor(0.7759)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2155 loss: tensor(0.7758)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5906406179009541\n",
      "2156 loss: tensor(0.7758)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2157 loss: tensor(0.7758)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2158 loss: tensor(0.7757)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5906406179009541\n",
      "2159 loss: tensor(0.7757)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2160 loss: tensor(0.7757)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2161 loss: tensor(0.7756)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2162 loss: tensor(0.7756)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5906406179009541\n",
      "2163 loss: tensor(0.7755)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2164 loss: tensor(0.7755)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2165 loss: tensor(0.7755)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2166 loss: tensor(0.7754)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5906406179009541\n",
      "2167 loss: tensor(0.7754)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2168 loss: tensor(0.7754)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2169 loss: tensor(0.7753)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5906406179009541\n",
      "2170 loss: tensor(0.7753)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2171 loss: tensor(0.7753)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2172 loss: tensor(0.7752)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2173 loss: tensor(0.7752)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2174 loss: tensor(0.7752)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2175 loss: tensor(0.7751)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2176 loss: tensor(0.7751)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2177 loss: tensor(0.7750)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2178 loss: tensor(0.7750)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2179 loss: tensor(0.7750)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2180 loss: tensor(0.7749)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2181 loss: tensor(0.7749)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5906406179009541\n",
      "2182 loss: tensor(0.7749)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2183 loss: tensor(0.7748)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2184 loss: tensor(0.7748)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2185 loss: tensor(0.7748)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5906406179009541\n",
      "2186 loss: tensor(0.7747)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2187 loss: tensor(0.7747)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5906406179009541\n",
      "2188 loss: tensor(0.7747)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5905497501135847\n",
      "2189 loss: tensor(0.7746)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2190 loss: tensor(0.7746)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5905497501135847\n",
      "2191 loss: tensor(0.7746)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2192 loss: tensor(0.7745)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5905497501135847\n",
      "2193 loss: tensor(0.7745)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5904588823262154\n",
      "2194 loss: tensor(0.7745)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5905497501135847\n",
      "2195 loss: tensor(0.7744)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5902771467514766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196 loss: tensor(0.7744)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5904588823262154\n",
      "2197 loss: tensor(0.7743)\n",
      "train : 0.9996994289149383\n",
      "test : 0.5902771467514766\n",
      "2198 loss: tensor(0.7743)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5905497501135847\n",
      "2199 loss: tensor(0.7743)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2200 loss: tensor(0.7742)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5906406179009541\n",
      "2201 loss: tensor(0.7742)\n",
      "train : 0.9996994289149383\n",
      "test : 0.590368014538846\n",
      "2202 loss: tensor(0.7742)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5906406179009541\n",
      "2203 loss: tensor(0.7741)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5902771467514766\n",
      "2204 loss: tensor(0.7741)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5905497501135847\n",
      "2205 loss: tensor(0.7741)\n",
      "train : 0.9997996192766256\n",
      "test : 0.590368014538846\n",
      "2206 loss: tensor(0.7740)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5902771467514766\n",
      "2207 loss: tensor(0.7740)\n",
      "train : 0.9997996192766256\n",
      "test : 0.590368014538846\n",
      "2208 loss: tensor(0.7740)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5902771467514766\n",
      "2209 loss: tensor(0.7739)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5902771467514766\n",
      "2210 loss: tensor(0.7739)\n",
      "train : 0.9997996192766256\n",
      "test : 0.590368014538846\n",
      "2211 loss: tensor(0.7739)\n",
      "train : 0.9997996192766256\n",
      "test : 0.590368014538846\n",
      "2212 loss: tensor(0.7738)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5904588823262154\n",
      "2213 loss: tensor(0.7738)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5902771467514766\n",
      "2214 loss: tensor(0.7738)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5904588823262154\n",
      "2215 loss: tensor(0.7737)\n",
      "train : 0.9997996192766256\n",
      "test : 0.590368014538846\n",
      "2216 loss: tensor(0.7737)\n",
      "train : 0.9997996192766256\n",
      "test : 0.590368014538846\n",
      "2217 loss: tensor(0.7737)\n",
      "train : 0.9997996192766256\n",
      "test : 0.590368014538846\n",
      "2218 loss: tensor(0.7736)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5904588823262154\n",
      "2219 loss: tensor(0.7736)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5904588823262154\n",
      "2220 loss: tensor(0.7736)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5904588823262154\n",
      "2221 loss: tensor(0.7735)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5904588823262154\n",
      "2222 loss: tensor(0.7735)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5904588823262154\n",
      "2223 loss: tensor(0.7735)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5905497501135847\n",
      "2224 loss: tensor(0.7734)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5904588823262154\n",
      "2225 loss: tensor(0.7734)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5906406179009541\n",
      "2226 loss: tensor(0.7734)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5904588823262154\n",
      "2227 loss: tensor(0.7733)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5905497501135847\n",
      "2228 loss: tensor(0.7733)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5904588823262154\n",
      "2229 loss: tensor(0.7733)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5905497501135847\n",
      "2230 loss: tensor(0.7732)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5904588823262154\n",
      "2231 loss: tensor(0.7732)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5905497501135847\n",
      "2232 loss: tensor(0.7732)\n",
      "train : 0.9997996192766256\n",
      "test : 0.590368014538846\n",
      "2233 loss: tensor(0.7731)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5907314856883235\n",
      "2234 loss: tensor(0.7731)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5905497501135847\n",
      "2235 loss: tensor(0.7731)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5909132212630622\n",
      "2236 loss: tensor(0.7730)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5905497501135847\n",
      "2237 loss: tensor(0.7730)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5909132212630622\n",
      "2238 loss: tensor(0.7730)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5906406179009541\n",
      "2239 loss: tensor(0.7730)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5908223534756929\n",
      "2240 loss: tensor(0.7729)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5906406179009541\n",
      "2241 loss: tensor(0.7729)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5907314856883235\n",
      "2242 loss: tensor(0.7729)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5906406179009541\n",
      "2243 loss: tensor(0.7728)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5907314856883235\n",
      "2244 loss: tensor(0.7728)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5906406179009541\n",
      "2245 loss: tensor(0.7728)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5909132212630622\n",
      "2246 loss: tensor(0.7727)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5907314856883235\n",
      "2247 loss: tensor(0.7727)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5908223534756929\n",
      "2248 loss: tensor(0.7727)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5906406179009541\n",
      "2249 loss: tensor(0.7726)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5906406179009541\n",
      "2250 loss: tensor(0.7726)\n",
      "train : 0.9997996192766256\n",
      "test : 0.5906406179009541\n",
      "2251 loss: tensor(0.7726)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2252 loss: tensor(0.7725)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2253 loss: tensor(0.7725)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2254 loss: tensor(0.7725)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2255 loss: tensor(0.7724)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2256 loss: tensor(0.7724)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2257 loss: tensor(0.7724)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2258 loss: tensor(0.7723)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2259 loss: tensor(0.7723)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5904588823262154\n",
      "2260 loss: tensor(0.7723)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n",
      "2261 loss: tensor(0.7723)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5904588823262154\n",
      "2262 loss: tensor(0.7722)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n",
      "2263 loss: tensor(0.7722)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2264 loss: tensor(0.7722)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n",
      "2265 loss: tensor(0.7721)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2266 loss: tensor(0.7721)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n",
      "2267 loss: tensor(0.7721)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2268 loss: tensor(0.7720)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n",
      "2269 loss: tensor(0.7720)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2270 loss: tensor(0.7720)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5910040890504317\n",
      "2271 loss: tensor(0.7719)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2272 loss: tensor(0.7719)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n",
      "2273 loss: tensor(0.7719)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2274 loss: tensor(0.7719)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n",
      "2275 loss: tensor(0.7718)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2276 loss: tensor(0.7718)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2277 loss: tensor(0.7718)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2278 loss: tensor(0.7717)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2279 loss: tensor(0.7717)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2280 loss: tensor(0.7717)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2281 loss: tensor(0.7716)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2282 loss: tensor(0.7716)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2283 loss: tensor(0.7716)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2284 loss: tensor(0.7715)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2285 loss: tensor(0.7715)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2286 loss: tensor(0.7715)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2287 loss: tensor(0.7715)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2288 loss: tensor(0.7714)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2289 loss: tensor(0.7714)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2290 loss: tensor(0.7714)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2291 loss: tensor(0.7713)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2292 loss: tensor(0.7713)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2293 loss: tensor(0.7713)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2294 loss: tensor(0.7712)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2295 loss: tensor(0.7712)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n",
      "2296 loss: tensor(0.7712)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2297 loss: tensor(0.7712)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n",
      "2298 loss: tensor(0.7711)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2299 loss: tensor(0.7711)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2300 loss: tensor(0.7711)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2301 loss: tensor(0.7710)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2302 loss: tensor(0.7710)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2303 loss: tensor(0.7710)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2304 loss: tensor(0.7710)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2305 loss: tensor(0.7709)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2306 loss: tensor(0.7709)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2307 loss: tensor(0.7709)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2308 loss: tensor(0.7708)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2309 loss: tensor(0.7708)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n",
      "2310 loss: tensor(0.7708)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2311 loss: tensor(0.7707)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n",
      "2312 loss: tensor(0.7707)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2313 loss: tensor(0.7707)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5909132212630622\n",
      "2314 loss: tensor(0.7707)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2315 loss: tensor(0.7706)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5908223534756929\n",
      "2316 loss: tensor(0.7706)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2317 loss: tensor(0.7706)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2318 loss: tensor(0.7705)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2319 loss: tensor(0.7705)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2320 loss: tensor(0.7705)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2321 loss: tensor(0.7705)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2322 loss: tensor(0.7704)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2323 loss: tensor(0.7704)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5904588823262154\n",
      "2324 loss: tensor(0.7704)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2325 loss: tensor(0.7703)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2326 loss: tensor(0.7703)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2327 loss: tensor(0.7703)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5904588823262154\n",
      "2328 loss: tensor(0.7703)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2329 loss: tensor(0.7702)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5904588823262154\n",
      "2330 loss: tensor(0.7702)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2331 loss: tensor(0.7702)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5904588823262154\n",
      "2332 loss: tensor(0.7701)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2333 loss: tensor(0.7701)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5904588823262154\n",
      "2334 loss: tensor(0.7701)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2335 loss: tensor(0.7701)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5904588823262154\n",
      "2336 loss: tensor(0.7700)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2337 loss: tensor(0.7700)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2338 loss: tensor(0.7700)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2339 loss: tensor(0.7699)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2340 loss: tensor(0.7699)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2341 loss: tensor(0.7699)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2342 loss: tensor(0.7699)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2343 loss: tensor(0.7698)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2344 loss: tensor(0.7698)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5907314856883235\n",
      "2345 loss: tensor(0.7698)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2346 loss: tensor(0.7698)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2347 loss: tensor(0.7697)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2348 loss: tensor(0.7697)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2349 loss: tensor(0.7697)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2350 loss: tensor(0.7696)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2351 loss: tensor(0.7696)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2352 loss: tensor(0.7696)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2353 loss: tensor(0.7696)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5905497501135847\n",
      "2354 loss: tensor(0.7695)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2355 loss: tensor(0.7695)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2356 loss: tensor(0.7695)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2357 loss: tensor(0.7695)\n",
      "train : 0.9998998096383128\n",
      "test : 0.5906406179009541\n",
      "2358 loss: tensor(0.7694)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2359 loss: tensor(0.7694)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2360 loss: tensor(0.7694)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2361 loss: tensor(0.7693)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2362 loss: tensor(0.7693)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2363 loss: tensor(0.7693)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2364 loss: tensor(0.7693)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2365 loss: tensor(0.7692)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2366 loss: tensor(0.7692)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2367 loss: tensor(0.7692)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2368 loss: tensor(0.7692)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2369 loss: tensor(0.7691)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2370 loss: tensor(0.7691)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2371 loss: tensor(0.7691)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2372 loss: tensor(0.7690)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2373 loss: tensor(0.7690)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2374 loss: tensor(0.7690)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2375 loss: tensor(0.7690)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2376 loss: tensor(0.7689)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2377 loss: tensor(0.7689)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2378 loss: tensor(0.7689)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2379 loss: tensor(0.7689)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2380 loss: tensor(0.7688)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2381 loss: tensor(0.7688)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2382 loss: tensor(0.7688)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2383 loss: tensor(0.7688)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2384 loss: tensor(0.7687)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2385 loss: tensor(0.7687)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2386 loss: tensor(0.7687)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2387 loss: tensor(0.7686)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2388 loss: tensor(0.7686)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2389 loss: tensor(0.7686)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2390 loss: tensor(0.7686)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2391 loss: tensor(0.7685)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2392 loss: tensor(0.7685)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2393 loss: tensor(0.7685)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2394 loss: tensor(0.7685)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2395 loss: tensor(0.7684)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2396 loss: tensor(0.7684)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2397 loss: tensor(0.7684)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2398 loss: tensor(0.7684)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2399 loss: tensor(0.7683)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2400 loss: tensor(0.7683)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2401 loss: tensor(0.7683)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2402 loss: tensor(0.7683)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2403 loss: tensor(0.7682)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2404 loss: tensor(0.7682)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2405 loss: tensor(0.7682)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2406 loss: tensor(0.7682)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2407 loss: tensor(0.7681)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2408 loss: tensor(0.7681)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2409 loss: tensor(0.7681)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2410 loss: tensor(0.7681)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2411 loss: tensor(0.7680)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2412 loss: tensor(0.7680)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2413 loss: tensor(0.7680)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2414 loss: tensor(0.7680)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2415 loss: tensor(0.7679)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2416 loss: tensor(0.7679)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2417 loss: tensor(0.7679)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2418 loss: tensor(0.7679)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2419 loss: tensor(0.7678)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2420 loss: tensor(0.7678)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2421 loss: tensor(0.7678)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2422 loss: tensor(0.7677)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2423 loss: tensor(0.7677)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2424 loss: tensor(0.7677)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2425 loss: tensor(0.7677)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2426 loss: tensor(0.7676)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2427 loss: tensor(0.7676)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2428 loss: tensor(0.7676)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2429 loss: tensor(0.7676)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2430 loss: tensor(0.7675)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2431 loss: tensor(0.7675)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2432 loss: tensor(0.7675)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2433 loss: tensor(0.7675)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2434 loss: tensor(0.7674)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2435 loss: tensor(0.7674)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2436 loss: tensor(0.7674)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2437 loss: tensor(0.7674)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2438 loss: tensor(0.7674)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2439 loss: tensor(0.7673)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2440 loss: tensor(0.7673)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2441 loss: tensor(0.7673)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2442 loss: tensor(0.7673)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2443 loss: tensor(0.7672)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2444 loss: tensor(0.7672)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2445 loss: tensor(0.7672)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2446 loss: tensor(0.7672)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2447 loss: tensor(0.7671)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2448 loss: tensor(0.7671)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2449 loss: tensor(0.7671)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2450 loss: tensor(0.7671)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2451 loss: tensor(0.7670)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2452 loss: tensor(0.7670)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2453 loss: tensor(0.7670)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2454 loss: tensor(0.7670)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2455 loss: tensor(0.7669)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2456 loss: tensor(0.7669)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2457 loss: tensor(0.7669)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2458 loss: tensor(0.7669)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2459 loss: tensor(0.7668)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2460 loss: tensor(0.7668)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2461 loss: tensor(0.7668)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2462 loss: tensor(0.7668)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2463 loss: tensor(0.7667)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2464 loss: tensor(0.7667)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2465 loss: tensor(0.7667)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2466 loss: tensor(0.7667)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2467 loss: tensor(0.7666)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2468 loss: tensor(0.7666)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2469 loss: tensor(0.7666)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2470 loss: tensor(0.7666)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2471 loss: tensor(0.7666)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2472 loss: tensor(0.7665)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2473 loss: tensor(0.7665)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2474 loss: tensor(0.7665)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2475 loss: tensor(0.7665)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2476 loss: tensor(0.7664)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2477 loss: tensor(0.7664)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2478 loss: tensor(0.7664)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2479 loss: tensor(0.7664)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2480 loss: tensor(0.7663)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2481 loss: tensor(0.7663)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2482 loss: tensor(0.7663)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2483 loss: tensor(0.7663)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2484 loss: tensor(0.7662)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2485 loss: tensor(0.7662)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2486 loss: tensor(0.7662)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2487 loss: tensor(0.7662)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2488 loss: tensor(0.7662)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2489 loss: tensor(0.7661)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2490 loss: tensor(0.7661)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2491 loss: tensor(0.7661)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2492 loss: tensor(0.7661)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2493 loss: tensor(0.7660)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2494 loss: tensor(0.7660)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2495 loss: tensor(0.7660)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2496 loss: tensor(0.7660)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2497 loss: tensor(0.7659)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2498 loss: tensor(0.7659)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2499 loss: tensor(0.7659)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2500 loss: tensor(0.7659)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2501 loss: tensor(0.7658)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2502 loss: tensor(0.7658)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2503 loss: tensor(0.7658)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2504 loss: tensor(0.7658)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2505 loss: tensor(0.7658)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2506 loss: tensor(0.7657)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2507 loss: tensor(0.7657)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2508 loss: tensor(0.7657)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2509 loss: tensor(0.7657)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2510 loss: tensor(0.7656)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2511 loss: tensor(0.7656)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2512 loss: tensor(0.7656)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2513 loss: tensor(0.7656)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2514 loss: tensor(0.7656)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2515 loss: tensor(0.7655)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2516 loss: tensor(0.7655)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2517 loss: tensor(0.7655)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2518 loss: tensor(0.7655)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2519 loss: tensor(0.7654)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2520 loss: tensor(0.7654)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2521 loss: tensor(0.7654)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2522 loss: tensor(0.7654)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2523 loss: tensor(0.7653)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2524 loss: tensor(0.7653)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2525 loss: tensor(0.7653)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2526 loss: tensor(0.7653)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2527 loss: tensor(0.7653)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2528 loss: tensor(0.7652)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2529 loss: tensor(0.7652)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2530 loss: tensor(0.7652)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2531 loss: tensor(0.7652)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2532 loss: tensor(0.7651)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2533 loss: tensor(0.7651)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2534 loss: tensor(0.7651)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2535 loss: tensor(0.7651)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2536 loss: tensor(0.7651)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2537 loss: tensor(0.7650)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2538 loss: tensor(0.7650)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2539 loss: tensor(0.7650)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "2540 loss: tensor(0.7650)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2541 loss: tensor(0.7649)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "2542 loss: tensor(0.7649)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2543 loss: tensor(0.7649)\n",
      "train : 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.5900954111767378\n",
      "2544 loss: tensor(0.7649)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2545 loss: tensor(0.7649)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2546 loss: tensor(0.7648)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2547 loss: tensor(0.7648)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2548 loss: tensor(0.7648)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2549 loss: tensor(0.7648)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2550 loss: tensor(0.7648)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2551 loss: tensor(0.7647)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2552 loss: tensor(0.7647)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2553 loss: tensor(0.7647)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2554 loss: tensor(0.7647)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2555 loss: tensor(0.7646)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2556 loss: tensor(0.7646)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2557 loss: tensor(0.7646)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2558 loss: tensor(0.7646)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2559 loss: tensor(0.7646)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2560 loss: tensor(0.7645)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2561 loss: tensor(0.7645)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2562 loss: tensor(0.7645)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2563 loss: tensor(0.7645)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2564 loss: tensor(0.7644)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2565 loss: tensor(0.7644)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2566 loss: tensor(0.7644)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2567 loss: tensor(0.7644)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2568 loss: tensor(0.7644)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2569 loss: tensor(0.7643)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2570 loss: tensor(0.7643)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2571 loss: tensor(0.7643)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2572 loss: tensor(0.7643)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2573 loss: tensor(0.7643)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2574 loss: tensor(0.7642)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2575 loss: tensor(0.7642)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2576 loss: tensor(0.7642)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2577 loss: tensor(0.7642)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2578 loss: tensor(0.7641)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2579 loss: tensor(0.7641)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2580 loss: tensor(0.7641)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2581 loss: tensor(0.7641)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2582 loss: tensor(0.7641)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2583 loss: tensor(0.7640)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2584 loss: tensor(0.7640)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2585 loss: tensor(0.7640)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2586 loss: tensor(0.7640)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2587 loss: tensor(0.7640)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2588 loss: tensor(0.7639)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2589 loss: tensor(0.7639)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2590 loss: tensor(0.7639)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2591 loss: tensor(0.7639)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2592 loss: tensor(0.7639)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2593 loss: tensor(0.7638)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2594 loss: tensor(0.7638)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2595 loss: tensor(0.7638)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2596 loss: tensor(0.7638)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2597 loss: tensor(0.7637)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2598 loss: tensor(0.7637)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2599 loss: tensor(0.7637)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2600 loss: tensor(0.7637)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2601 loss: tensor(0.7637)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2602 loss: tensor(0.7636)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2603 loss: tensor(0.7636)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2604 loss: tensor(0.7636)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2605 loss: tensor(0.7636)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2606 loss: tensor(0.7636)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2607 loss: tensor(0.7635)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2608 loss: tensor(0.7635)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2609 loss: tensor(0.7635)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2610 loss: tensor(0.7635)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2611 loss: tensor(0.7635)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2612 loss: tensor(0.7634)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2613 loss: tensor(0.7634)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2614 loss: tensor(0.7634)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2615 loss: tensor(0.7634)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2616 loss: tensor(0.7634)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2617 loss: tensor(0.7633)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2618 loss: tensor(0.7633)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2619 loss: tensor(0.7633)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2620 loss: tensor(0.7633)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2621 loss: tensor(0.7632)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2622 loss: tensor(0.7632)\n",
      "train : 1.0\n",
      "test : 0.5911858246251703\n",
      "2623 loss: tensor(0.7632)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2624 loss: tensor(0.7632)\n",
      "train : 1.0\n",
      "test : 0.5911858246251703\n",
      "2625 loss: tensor(0.7632)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2626 loss: tensor(0.7631)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2627 loss: tensor(0.7631)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2628 loss: tensor(0.7631)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2629 loss: tensor(0.7631)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2630 loss: tensor(0.7631)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2631 loss: tensor(0.7630)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2632 loss: tensor(0.7630)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2633 loss: tensor(0.7630)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2634 loss: tensor(0.7630)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2635 loss: tensor(0.7630)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2636 loss: tensor(0.7629)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2637 loss: tensor(0.7629)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2638 loss: tensor(0.7629)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2639 loss: tensor(0.7629)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2640 loss: tensor(0.7629)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2641 loss: tensor(0.7628)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2642 loss: tensor(0.7628)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2643 loss: tensor(0.7628)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2644 loss: tensor(0.7628)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2645 loss: tensor(0.7628)\n",
      "train : 1.0\n",
      "test : 0.5911858246251703\n",
      "2646 loss: tensor(0.7627)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2647 loss: tensor(0.7627)\n",
      "train : 1.0\n",
      "test : 0.5911858246251703\n",
      "2648 loss: tensor(0.7627)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2649 loss: tensor(0.7627)\n",
      "train : 1.0\n",
      "test : 0.5911858246251703\n",
      "2650 loss: tensor(0.7627)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2651 loss: tensor(0.7626)\n",
      "train : 1.0\n",
      "test : 0.5911858246251703\n",
      "2652 loss: tensor(0.7626)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2653 loss: tensor(0.7626)\n",
      "train : 1.0\n",
      "test : 0.5911858246251703\n",
      "2654 loss: tensor(0.7626)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2655 loss: tensor(0.7626)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2656 loss: tensor(0.7625)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2657 loss: tensor(0.7625)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2658 loss: tensor(0.7625)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2659 loss: tensor(0.7625)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2660 loss: tensor(0.7625)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2661 loss: tensor(0.7624)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2662 loss: tensor(0.7624)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2663 loss: tensor(0.7624)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2664 loss: tensor(0.7624)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2665 loss: tensor(0.7624)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2666 loss: tensor(0.7623)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2667 loss: tensor(0.7623)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2668 loss: tensor(0.7623)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2669 loss: tensor(0.7623)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2670 loss: tensor(0.7623)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2671 loss: tensor(0.7622)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2672 loss: tensor(0.7622)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2673 loss: tensor(0.7622)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2674 loss: tensor(0.7622)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2675 loss: tensor(0.7622)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2676 loss: tensor(0.7622)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2677 loss: tensor(0.7621)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2678 loss: tensor(0.7621)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2679 loss: tensor(0.7621)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2680 loss: tensor(0.7621)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2681 loss: tensor(0.7621)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2682 loss: tensor(0.7620)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2683 loss: tensor(0.7620)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2684 loss: tensor(0.7620)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2685 loss: tensor(0.7620)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2686 loss: tensor(0.7620)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2687 loss: tensor(0.7619)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2688 loss: tensor(0.7619)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2689 loss: tensor(0.7619)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2690 loss: tensor(0.7619)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2691 loss: tensor(0.7619)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2692 loss: tensor(0.7618)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2693 loss: tensor(0.7618)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2694 loss: tensor(0.7618)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2695 loss: tensor(0.7618)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2696 loss: tensor(0.7618)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2697 loss: tensor(0.7617)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2698 loss: tensor(0.7617)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2699 loss: tensor(0.7617)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2700 loss: tensor(0.7617)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2701 loss: tensor(0.7617)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2702 loss: tensor(0.7617)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2703 loss: tensor(0.7616)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2704 loss: tensor(0.7616)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2705 loss: tensor(0.7616)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2706 loss: tensor(0.7616)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2707 loss: tensor(0.7616)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2708 loss: tensor(0.7615)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2709 loss: tensor(0.7615)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2710 loss: tensor(0.7615)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2711 loss: tensor(0.7615)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2712 loss: tensor(0.7615)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2713 loss: tensor(0.7614)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2714 loss: tensor(0.7614)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2715 loss: tensor(0.7614)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2716 loss: tensor(0.7614)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2717 loss: tensor(0.7614)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2718 loss: tensor(0.7613)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2719 loss: tensor(0.7613)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2720 loss: tensor(0.7613)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2721 loss: tensor(0.7613)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2722 loss: tensor(0.7613)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2723 loss: tensor(0.7613)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2724 loss: tensor(0.7612)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2725 loss: tensor(0.7612)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2726 loss: tensor(0.7612)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2727 loss: tensor(0.7612)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2728 loss: tensor(0.7612)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2729 loss: tensor(0.7611)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2730 loss: tensor(0.7611)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2731 loss: tensor(0.7611)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2732 loss: tensor(0.7611)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2733 loss: tensor(0.7611)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2734 loss: tensor(0.7610)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2735 loss: tensor(0.7610)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2736 loss: tensor(0.7610)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2737 loss: tensor(0.7610)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2738 loss: tensor(0.7610)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2739 loss: tensor(0.7610)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2740 loss: tensor(0.7609)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2741 loss: tensor(0.7609)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2742 loss: tensor(0.7609)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2743 loss: tensor(0.7609)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2744 loss: tensor(0.7609)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2745 loss: tensor(0.7608)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2746 loss: tensor(0.7608)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2747 loss: tensor(0.7608)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2748 loss: tensor(0.7608)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2749 loss: tensor(0.7608)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2750 loss: tensor(0.7607)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2751 loss: tensor(0.7607)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2752 loss: tensor(0.7607)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2753 loss: tensor(0.7607)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2754 loss: tensor(0.7607)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2755 loss: tensor(0.7607)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2756 loss: tensor(0.7606)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2757 loss: tensor(0.7606)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2758 loss: tensor(0.7606)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2759 loss: tensor(0.7606)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2760 loss: tensor(0.7606)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2761 loss: tensor(0.7605)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2762 loss: tensor(0.7605)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2763 loss: tensor(0.7605)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2764 loss: tensor(0.7605)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2765 loss: tensor(0.7605)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2766 loss: tensor(0.7605)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2767 loss: tensor(0.7604)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2768 loss: tensor(0.7604)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2769 loss: tensor(0.7604)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2770 loss: tensor(0.7604)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2771 loss: tensor(0.7604)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2772 loss: tensor(0.7603)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2773 loss: tensor(0.7603)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2774 loss: tensor(0.7603)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2775 loss: tensor(0.7603)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2776 loss: tensor(0.7603)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2777 loss: tensor(0.7603)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2778 loss: tensor(0.7602)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2779 loss: tensor(0.7602)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2780 loss: tensor(0.7602)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2781 loss: tensor(0.7602)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2782 loss: tensor(0.7602)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2783 loss: tensor(0.7601)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2784 loss: tensor(0.7601)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2785 loss: tensor(0.7601)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2786 loss: tensor(0.7601)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2787 loss: tensor(0.7601)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2788 loss: tensor(0.7601)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2789 loss: tensor(0.7600)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2790 loss: tensor(0.7600)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2791 loss: tensor(0.7600)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2792 loss: tensor(0.7600)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2793 loss: tensor(0.7600)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2794 loss: tensor(0.7600)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2795 loss: tensor(0.7599)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2796 loss: tensor(0.7599)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2797 loss: tensor(0.7599)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2798 loss: tensor(0.7599)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2799 loss: tensor(0.7599)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2800 loss: tensor(0.7598)\n",
      "train : 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.5909132212630622\n",
      "2801 loss: tensor(0.7598)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2802 loss: tensor(0.7598)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2803 loss: tensor(0.7598)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2804 loss: tensor(0.7598)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2805 loss: tensor(0.7598)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2806 loss: tensor(0.7597)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2807 loss: tensor(0.7597)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2808 loss: tensor(0.7597)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2809 loss: tensor(0.7597)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2810 loss: tensor(0.7597)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2811 loss: tensor(0.7597)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2812 loss: tensor(0.7596)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2813 loss: tensor(0.7596)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2814 loss: tensor(0.7596)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2815 loss: tensor(0.7596)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2816 loss: tensor(0.7596)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2817 loss: tensor(0.7595)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2818 loss: tensor(0.7595)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2819 loss: tensor(0.7595)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2820 loss: tensor(0.7595)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2821 loss: tensor(0.7595)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2822 loss: tensor(0.7595)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2823 loss: tensor(0.7594)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2824 loss: tensor(0.7594)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2825 loss: tensor(0.7594)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2826 loss: tensor(0.7594)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2827 loss: tensor(0.7594)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2828 loss: tensor(0.7594)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2829 loss: tensor(0.7593)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2830 loss: tensor(0.7593)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2831 loss: tensor(0.7593)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2832 loss: tensor(0.7593)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2833 loss: tensor(0.7593)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2834 loss: tensor(0.7592)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2835 loss: tensor(0.7592)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2836 loss: tensor(0.7592)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2837 loss: tensor(0.7592)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2838 loss: tensor(0.7592)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2839 loss: tensor(0.7592)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2840 loss: tensor(0.7591)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2841 loss: tensor(0.7591)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2842 loss: tensor(0.7591)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2843 loss: tensor(0.7591)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2844 loss: tensor(0.7591)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2845 loss: tensor(0.7591)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2846 loss: tensor(0.7590)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2847 loss: tensor(0.7590)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2848 loss: tensor(0.7590)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2849 loss: tensor(0.7590)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2850 loss: tensor(0.7590)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2851 loss: tensor(0.7590)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2852 loss: tensor(0.7589)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2853 loss: tensor(0.7589)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2854 loss: tensor(0.7589)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2855 loss: tensor(0.7589)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2856 loss: tensor(0.7589)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2857 loss: tensor(0.7589)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2858 loss: tensor(0.7588)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2859 loss: tensor(0.7588)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2860 loss: tensor(0.7588)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2861 loss: tensor(0.7588)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2862 loss: tensor(0.7588)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2863 loss: tensor(0.7587)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2864 loss: tensor(0.7587)\n",
      "train : 1.0\n",
      "test : 0.591094956837801\n",
      "2865 loss: tensor(0.7587)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2866 loss: tensor(0.7587)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2867 loss: tensor(0.7587)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2868 loss: tensor(0.7587)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2869 loss: tensor(0.7586)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2870 loss: tensor(0.7586)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2871 loss: tensor(0.7586)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2872 loss: tensor(0.7586)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2873 loss: tensor(0.7586)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2874 loss: tensor(0.7586)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2875 loss: tensor(0.7585)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2876 loss: tensor(0.7585)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2877 loss: tensor(0.7585)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2878 loss: tensor(0.7585)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2879 loss: tensor(0.7585)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2880 loss: tensor(0.7585)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2881 loss: tensor(0.7584)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2882 loss: tensor(0.7584)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2883 loss: tensor(0.7584)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2884 loss: tensor(0.7584)\n",
      "train : 1.0\n",
      "test : 0.5910040890504317\n",
      "2885 loss: tensor(0.7584)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2886 loss: tensor(0.7584)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2887 loss: tensor(0.7583)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2888 loss: tensor(0.7583)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2889 loss: tensor(0.7583)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2890 loss: tensor(0.7583)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2891 loss: tensor(0.7583)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2892 loss: tensor(0.7583)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2893 loss: tensor(0.7582)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2894 loss: tensor(0.7582)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2895 loss: tensor(0.7582)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2896 loss: tensor(0.7582)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2897 loss: tensor(0.7582)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2898 loss: tensor(0.7582)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2899 loss: tensor(0.7581)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2900 loss: tensor(0.7581)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2901 loss: tensor(0.7581)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2902 loss: tensor(0.7581)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2903 loss: tensor(0.7581)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2904 loss: tensor(0.7581)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2905 loss: tensor(0.7580)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2906 loss: tensor(0.7580)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2907 loss: tensor(0.7580)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2908 loss: tensor(0.7580)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2909 loss: tensor(0.7580)\n",
      "train : 1.0\n",
      "test : 0.5909132212630622\n",
      "2910 loss: tensor(0.7580)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2911 loss: tensor(0.7579)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2912 loss: tensor(0.7579)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2913 loss: tensor(0.7579)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2914 loss: tensor(0.7579)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2915 loss: tensor(0.7579)\n",
      "train : 1.0\n",
      "test : 0.5908223534756929\n",
      "2916 loss: tensor(0.7579)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2917 loss: tensor(0.7578)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2918 loss: tensor(0.7578)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2919 loss: tensor(0.7578)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2920 loss: tensor(0.7578)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2921 loss: tensor(0.7578)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2922 loss: tensor(0.7578)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2923 loss: tensor(0.7577)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2924 loss: tensor(0.7577)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2925 loss: tensor(0.7577)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2926 loss: tensor(0.7577)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2927 loss: tensor(0.7577)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2928 loss: tensor(0.7577)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2929 loss: tensor(0.7576)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2930 loss: tensor(0.7576)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2931 loss: tensor(0.7576)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2932 loss: tensor(0.7576)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2933 loss: tensor(0.7576)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2934 loss: tensor(0.7576)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2935 loss: tensor(0.7575)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2936 loss: tensor(0.7575)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2937 loss: tensor(0.7575)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2938 loss: tensor(0.7575)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2939 loss: tensor(0.7575)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2940 loss: tensor(0.7575)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2941 loss: tensor(0.7574)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2942 loss: tensor(0.7574)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2943 loss: tensor(0.7574)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2944 loss: tensor(0.7574)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2945 loss: tensor(0.7574)\n",
      "train : 1.0\n",
      "test : 0.5907314856883235\n",
      "2946 loss: tensor(0.7574)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2947 loss: tensor(0.7574)\n",
      "train : 1.0\n",
      "test : 0.5906406179009541\n",
      "2948 loss: tensor(0.7573)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2949 loss: tensor(0.7573)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2950 loss: tensor(0.7573)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2951 loss: tensor(0.7573)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2952 loss: tensor(0.7573)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2953 loss: tensor(0.7573)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "2954 loss: tensor(0.7572)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2955 loss: tensor(0.7572)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2956 loss: tensor(0.7572)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2957 loss: tensor(0.7572)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2958 loss: tensor(0.7572)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2959 loss: tensor(0.7572)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2960 loss: tensor(0.7571)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2961 loss: tensor(0.7571)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2962 loss: tensor(0.7571)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2963 loss: tensor(0.7571)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2964 loss: tensor(0.7571)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2965 loss: tensor(0.7571)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2966 loss: tensor(0.7570)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2967 loss: tensor(0.7570)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2968 loss: tensor(0.7570)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2969 loss: tensor(0.7570)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2970 loss: tensor(0.7570)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2971 loss: tensor(0.7570)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2972 loss: tensor(0.7569)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2973 loss: tensor(0.7569)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "2974 loss: tensor(0.7569)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2975 loss: tensor(0.7569)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2976 loss: tensor(0.7569)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2977 loss: tensor(0.7569)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2978 loss: tensor(0.7569)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2979 loss: tensor(0.7568)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2980 loss: tensor(0.7568)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2981 loss: tensor(0.7568)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2982 loss: tensor(0.7568)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2983 loss: tensor(0.7568)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2984 loss: tensor(0.7568)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2985 loss: tensor(0.7567)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2986 loss: tensor(0.7567)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2987 loss: tensor(0.7567)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2988 loss: tensor(0.7567)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2989 loss: tensor(0.7567)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2990 loss: tensor(0.7567)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2991 loss: tensor(0.7566)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2992 loss: tensor(0.7566)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "2993 loss: tensor(0.7566)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2994 loss: tensor(0.7566)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2995 loss: tensor(0.7566)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "2996 loss: tensor(0.7566)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "2997 loss: tensor(0.7566)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "2998 loss: tensor(0.7565)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "2999 loss: tensor(0.7565)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3000 loss: tensor(0.7565)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3001 loss: tensor(0.7565)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3002 loss: tensor(0.7565)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3003 loss: tensor(0.7565)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3004 loss: tensor(0.7564)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3005 loss: tensor(0.7564)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3006 loss: tensor(0.7564)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3007 loss: tensor(0.7564)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3008 loss: tensor(0.7564)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3009 loss: tensor(0.7564)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3010 loss: tensor(0.7563)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3011 loss: tensor(0.7563)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3012 loss: tensor(0.7563)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3013 loss: tensor(0.7563)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3014 loss: tensor(0.7563)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3015 loss: tensor(0.7563)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3016 loss: tensor(0.7563)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3017 loss: tensor(0.7562)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3018 loss: tensor(0.7562)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3019 loss: tensor(0.7562)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3020 loss: tensor(0.7562)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3021 loss: tensor(0.7562)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3022 loss: tensor(0.7562)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3023 loss: tensor(0.7561)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3024 loss: tensor(0.7561)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3025 loss: tensor(0.7561)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3026 loss: tensor(0.7561)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3027 loss: tensor(0.7561)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3028 loss: tensor(0.7561)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3029 loss: tensor(0.7560)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3030 loss: tensor(0.7560)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3031 loss: tensor(0.7560)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3032 loss: tensor(0.7560)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3033 loss: tensor(0.7560)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3034 loss: tensor(0.7560)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3035 loss: tensor(0.7560)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3036 loss: tensor(0.7559)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3037 loss: tensor(0.7559)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3038 loss: tensor(0.7559)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3039 loss: tensor(0.7559)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3040 loss: tensor(0.7559)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3041 loss: tensor(0.7559)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3042 loss: tensor(0.7558)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3043 loss: tensor(0.7558)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3044 loss: tensor(0.7558)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3045 loss: tensor(0.7558)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3046 loss: tensor(0.7558)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3047 loss: tensor(0.7558)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3048 loss: tensor(0.7558)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3049 loss: tensor(0.7557)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3050 loss: tensor(0.7557)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3051 loss: tensor(0.7557)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3052 loss: tensor(0.7557)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3053 loss: tensor(0.7557)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3054 loss: tensor(0.7557)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3055 loss: tensor(0.7556)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3056 loss: tensor(0.7556)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3057 loss: tensor(0.7556)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3058 loss: tensor(0.7556)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3059 loss: tensor(0.7556)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3060 loss: tensor(0.7556)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3061 loss: tensor(0.7556)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3062 loss: tensor(0.7555)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3063 loss: tensor(0.7555)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3064 loss: tensor(0.7555)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3065 loss: tensor(0.7555)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3066 loss: tensor(0.7555)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3067 loss: tensor(0.7555)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3068 loss: tensor(0.7554)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3069 loss: tensor(0.7554)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3070 loss: tensor(0.7554)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3071 loss: tensor(0.7554)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3072 loss: tensor(0.7554)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3073 loss: tensor(0.7554)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3074 loss: tensor(0.7554)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3075 loss: tensor(0.7553)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3076 loss: tensor(0.7553)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3077 loss: tensor(0.7553)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3078 loss: tensor(0.7553)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3079 loss: tensor(0.7553)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3080 loss: tensor(0.7553)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3081 loss: tensor(0.7552)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3082 loss: tensor(0.7552)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3083 loss: tensor(0.7552)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3084 loss: tensor(0.7552)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3085 loss: tensor(0.7552)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3086 loss: tensor(0.7552)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3087 loss: tensor(0.7552)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3088 loss: tensor(0.7551)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3089 loss: tensor(0.7551)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3090 loss: tensor(0.7551)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3091 loss: tensor(0.7551)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3092 loss: tensor(0.7551)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3093 loss: tensor(0.7551)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3094 loss: tensor(0.7551)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3095 loss: tensor(0.7550)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3096 loss: tensor(0.7550)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3097 loss: tensor(0.7550)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3098 loss: tensor(0.7550)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3099 loss: tensor(0.7550)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3100 loss: tensor(0.7550)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3101 loss: tensor(0.7549)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3102 loss: tensor(0.7549)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3103 loss: tensor(0.7549)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3104 loss: tensor(0.7549)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3105 loss: tensor(0.7549)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3106 loss: tensor(0.7549)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3107 loss: tensor(0.7549)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3108 loss: tensor(0.7548)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3109 loss: tensor(0.7548)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3110 loss: tensor(0.7548)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3111 loss: tensor(0.7548)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3112 loss: tensor(0.7548)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3113 loss: tensor(0.7548)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3114 loss: tensor(0.7548)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3115 loss: tensor(0.7547)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3116 loss: tensor(0.7547)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3117 loss: tensor(0.7547)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3118 loss: tensor(0.7547)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3119 loss: tensor(0.7547)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3120 loss: tensor(0.7547)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3121 loss: tensor(0.7547)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3122 loss: tensor(0.7546)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3123 loss: tensor(0.7546)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3124 loss: tensor(0.7546)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3125 loss: tensor(0.7546)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3126 loss: tensor(0.7546)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3127 loss: tensor(0.7546)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3128 loss: tensor(0.7546)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3129 loss: tensor(0.7545)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3130 loss: tensor(0.7545)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3131 loss: tensor(0.7545)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3132 loss: tensor(0.7545)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "3133 loss: tensor(0.7545)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3134 loss: tensor(0.7545)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "3135 loss: tensor(0.7545)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3136 loss: tensor(0.7544)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3137 loss: tensor(0.7544)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3138 loss: tensor(0.7544)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3139 loss: tensor(0.7544)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3140 loss: tensor(0.7544)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3141 loss: tensor(0.7544)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3142 loss: tensor(0.7544)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3143 loss: tensor(0.7543)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "3144 loss: tensor(0.7543)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3145 loss: tensor(0.7543)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "3146 loss: tensor(0.7543)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3147 loss: tensor(0.7543)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3148 loss: tensor(0.7543)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3149 loss: tensor(0.7543)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3150 loss: tensor(0.7543)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3151 loss: tensor(0.7542)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3152 loss: tensor(0.7542)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3153 loss: tensor(0.7542)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3154 loss: tensor(0.7542)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3155 loss: tensor(0.7542)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3156 loss: tensor(0.7542)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3157 loss: tensor(0.7542)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3158 loss: tensor(0.7541)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3159 loss: tensor(0.7541)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3160 loss: tensor(0.7541)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3161 loss: tensor(0.7541)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3162 loss: tensor(0.7541)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3163 loss: tensor(0.7541)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3164 loss: tensor(0.7541)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3165 loss: tensor(0.7540)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3166 loss: tensor(0.7540)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3167 loss: tensor(0.7540)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3168 loss: tensor(0.7540)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3169 loss: tensor(0.7540)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3170 loss: tensor(0.7540)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3171 loss: tensor(0.7540)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3172 loss: tensor(0.7540)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3173 loss: tensor(0.7539)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3174 loss: tensor(0.7539)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3175 loss: tensor(0.7539)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3176 loss: tensor(0.7539)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3177 loss: tensor(0.7539)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3178 loss: tensor(0.7539)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3179 loss: tensor(0.7539)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3180 loss: tensor(0.7538)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3181 loss: tensor(0.7538)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3182 loss: tensor(0.7538)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3183 loss: tensor(0.7538)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3184 loss: tensor(0.7538)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3185 loss: tensor(0.7538)\n",
      "train : 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.5900954111767378\n",
      "3186 loss: tensor(0.7538)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3187 loss: tensor(0.7537)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3188 loss: tensor(0.7537)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3189 loss: tensor(0.7537)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3190 loss: tensor(0.7537)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3191 loss: tensor(0.7537)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3192 loss: tensor(0.7537)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3193 loss: tensor(0.7537)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3194 loss: tensor(0.7537)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3195 loss: tensor(0.7536)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3196 loss: tensor(0.7536)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3197 loss: tensor(0.7536)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3198 loss: tensor(0.7536)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3199 loss: tensor(0.7536)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3200 loss: tensor(0.7536)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3201 loss: tensor(0.7536)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3202 loss: tensor(0.7535)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3203 loss: tensor(0.7535)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3204 loss: tensor(0.7535)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3205 loss: tensor(0.7535)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3206 loss: tensor(0.7535)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3207 loss: tensor(0.7535)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3208 loss: tensor(0.7535)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3209 loss: tensor(0.7535)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3210 loss: tensor(0.7534)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3211 loss: tensor(0.7534)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3212 loss: tensor(0.7534)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3213 loss: tensor(0.7534)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3214 loss: tensor(0.7534)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3215 loss: tensor(0.7534)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3216 loss: tensor(0.7534)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3217 loss: tensor(0.7533)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3218 loss: tensor(0.7533)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3219 loss: tensor(0.7533)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3220 loss: tensor(0.7533)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3221 loss: tensor(0.7533)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3222 loss: tensor(0.7533)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3223 loss: tensor(0.7533)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3224 loss: tensor(0.7533)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3225 loss: tensor(0.7532)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3226 loss: tensor(0.7532)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3227 loss: tensor(0.7532)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3228 loss: tensor(0.7532)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3229 loss: tensor(0.7532)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3230 loss: tensor(0.7532)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3231 loss: tensor(0.7532)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3232 loss: tensor(0.7531)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3233 loss: tensor(0.7531)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3234 loss: tensor(0.7531)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3235 loss: tensor(0.7531)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3236 loss: tensor(0.7531)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3237 loss: tensor(0.7531)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3238 loss: tensor(0.7531)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3239 loss: tensor(0.7531)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3240 loss: tensor(0.7530)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3241 loss: tensor(0.7530)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3242 loss: tensor(0.7530)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3243 loss: tensor(0.7530)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3244 loss: tensor(0.7530)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3245 loss: tensor(0.7530)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3246 loss: tensor(0.7530)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3247 loss: tensor(0.7529)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3248 loss: tensor(0.7529)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3249 loss: tensor(0.7529)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3250 loss: tensor(0.7529)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3251 loss: tensor(0.7529)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3252 loss: tensor(0.7529)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3253 loss: tensor(0.7529)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3254 loss: tensor(0.7529)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3255 loss: tensor(0.7528)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3256 loss: tensor(0.7528)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3257 loss: tensor(0.7528)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3258 loss: tensor(0.7528)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3259 loss: tensor(0.7528)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3260 loss: tensor(0.7528)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3261 loss: tensor(0.7528)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3262 loss: tensor(0.7528)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3263 loss: tensor(0.7527)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3264 loss: tensor(0.7527)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3265 loss: tensor(0.7527)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3266 loss: tensor(0.7527)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3267 loss: tensor(0.7527)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3268 loss: tensor(0.7527)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3269 loss: tensor(0.7527)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3270 loss: tensor(0.7526)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3271 loss: tensor(0.7526)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3272 loss: tensor(0.7526)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3273 loss: tensor(0.7526)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3274 loss: tensor(0.7526)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3275 loss: tensor(0.7526)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3276 loss: tensor(0.7526)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3277 loss: tensor(0.7526)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3278 loss: tensor(0.7525)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3279 loss: tensor(0.7525)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3280 loss: tensor(0.7525)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3281 loss: tensor(0.7525)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3282 loss: tensor(0.7525)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3283 loss: tensor(0.7525)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3284 loss: tensor(0.7525)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3285 loss: tensor(0.7524)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3286 loss: tensor(0.7524)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3287 loss: tensor(0.7524)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3288 loss: tensor(0.7524)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3289 loss: tensor(0.7524)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3290 loss: tensor(0.7524)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3291 loss: tensor(0.7524)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3292 loss: tensor(0.7524)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3293 loss: tensor(0.7523)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3294 loss: tensor(0.7523)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3295 loss: tensor(0.7523)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3296 loss: tensor(0.7523)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3297 loss: tensor(0.7523)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3298 loss: tensor(0.7523)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3299 loss: tensor(0.7523)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3300 loss: tensor(0.7523)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3301 loss: tensor(0.7522)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3302 loss: tensor(0.7522)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3303 loss: tensor(0.7522)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3304 loss: tensor(0.7522)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3305 loss: tensor(0.7522)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3306 loss: tensor(0.7522)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3307 loss: tensor(0.7522)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3308 loss: tensor(0.7522)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3309 loss: tensor(0.7521)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3310 loss: tensor(0.7521)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3311 loss: tensor(0.7521)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3312 loss: tensor(0.7521)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3313 loss: tensor(0.7521)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3314 loss: tensor(0.7521)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3315 loss: tensor(0.7521)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3316 loss: tensor(0.7520)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3317 loss: tensor(0.7520)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3318 loss: tensor(0.7520)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3319 loss: tensor(0.7520)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3320 loss: tensor(0.7520)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3321 loss: tensor(0.7520)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3322 loss: tensor(0.7520)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3323 loss: tensor(0.7520)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3324 loss: tensor(0.7519)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3325 loss: tensor(0.7519)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3326 loss: tensor(0.7519)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3327 loss: tensor(0.7519)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3328 loss: tensor(0.7519)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3329 loss: tensor(0.7519)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3330 loss: tensor(0.7519)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3331 loss: tensor(0.7519)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3332 loss: tensor(0.7518)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3333 loss: tensor(0.7518)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3334 loss: tensor(0.7518)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3335 loss: tensor(0.7518)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3336 loss: tensor(0.7518)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3337 loss: tensor(0.7518)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3338 loss: tensor(0.7518)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3339 loss: tensor(0.7518)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3340 loss: tensor(0.7517)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3341 loss: tensor(0.7517)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3342 loss: tensor(0.7517)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3343 loss: tensor(0.7517)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3344 loss: tensor(0.7517)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3345 loss: tensor(0.7517)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3346 loss: tensor(0.7517)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3347 loss: tensor(0.7517)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3348 loss: tensor(0.7516)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3349 loss: tensor(0.7516)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3350 loss: tensor(0.7516)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3351 loss: tensor(0.7516)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3352 loss: tensor(0.7516)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3353 loss: tensor(0.7516)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3354 loss: tensor(0.7516)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3355 loss: tensor(0.7515)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3356 loss: tensor(0.7515)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3357 loss: tensor(0.7515)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3358 loss: tensor(0.7515)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3359 loss: tensor(0.7515)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3360 loss: tensor(0.7515)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3361 loss: tensor(0.7515)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3362 loss: tensor(0.7515)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3363 loss: tensor(0.7514)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3364 loss: tensor(0.7514)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3365 loss: tensor(0.7514)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3366 loss: tensor(0.7514)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3367 loss: tensor(0.7514)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3368 loss: tensor(0.7514)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3369 loss: tensor(0.7514)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3370 loss: tensor(0.7514)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3371 loss: tensor(0.7513)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3372 loss: tensor(0.7513)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3373 loss: tensor(0.7513)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3374 loss: tensor(0.7513)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3375 loss: tensor(0.7513)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3376 loss: tensor(0.7513)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3377 loss: tensor(0.7513)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3378 loss: tensor(0.7513)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3379 loss: tensor(0.7512)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3380 loss: tensor(0.7512)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3381 loss: tensor(0.7512)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3382 loss: tensor(0.7512)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3383 loss: tensor(0.7512)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3384 loss: tensor(0.7512)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3385 loss: tensor(0.7512)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3386 loss: tensor(0.7512)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3387 loss: tensor(0.7511)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3388 loss: tensor(0.7511)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3389 loss: tensor(0.7511)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3390 loss: tensor(0.7511)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3391 loss: tensor(0.7511)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3392 loss: tensor(0.7511)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3393 loss: tensor(0.7511)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3394 loss: tensor(0.7511)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3395 loss: tensor(0.7510)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3396 loss: tensor(0.7510)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3397 loss: tensor(0.7510)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3398 loss: tensor(0.7510)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3399 loss: tensor(0.7510)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3400 loss: tensor(0.7510)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3401 loss: tensor(0.7510)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3402 loss: tensor(0.7510)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3403 loss: tensor(0.7509)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3404 loss: tensor(0.7509)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3405 loss: tensor(0.7509)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3406 loss: tensor(0.7509)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3407 loss: tensor(0.7509)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3408 loss: tensor(0.7509)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3409 loss: tensor(0.7509)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3410 loss: tensor(0.7509)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3411 loss: tensor(0.7508)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3412 loss: tensor(0.7508)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3413 loss: tensor(0.7508)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3414 loss: tensor(0.7508)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3415 loss: tensor(0.7508)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3416 loss: tensor(0.7508)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3417 loss: tensor(0.7508)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3418 loss: tensor(0.7508)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3419 loss: tensor(0.7507)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3420 loss: tensor(0.7507)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3421 loss: tensor(0.7507)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3422 loss: tensor(0.7507)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3423 loss: tensor(0.7507)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3424 loss: tensor(0.7507)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3425 loss: tensor(0.7507)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3426 loss: tensor(0.7507)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3427 loss: tensor(0.7506)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3428 loss: tensor(0.7506)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3429 loss: tensor(0.7506)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3430 loss: tensor(0.7506)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3431 loss: tensor(0.7506)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3432 loss: tensor(0.7506)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3433 loss: tensor(0.7506)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3434 loss: tensor(0.7506)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3435 loss: tensor(0.7505)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3436 loss: tensor(0.7505)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3437 loss: tensor(0.7505)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3438 loss: tensor(0.7505)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3439 loss: tensor(0.7505)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3440 loss: tensor(0.7505)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3441 loss: tensor(0.7505)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3442 loss: tensor(0.7505)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3443 loss: tensor(0.7504)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3444 loss: tensor(0.7504)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3445 loss: tensor(0.7504)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3446 loss: tensor(0.7504)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3447 loss: tensor(0.7504)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3448 loss: tensor(0.7504)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3449 loss: tensor(0.7504)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3450 loss: tensor(0.7504)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3451 loss: tensor(0.7503)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3452 loss: tensor(0.7503)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3453 loss: tensor(0.7503)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3454 loss: tensor(0.7503)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3455 loss: tensor(0.7503)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3456 loss: tensor(0.7503)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3457 loss: tensor(0.7503)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3458 loss: tensor(0.7503)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3459 loss: tensor(0.7502)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3460 loss: tensor(0.7502)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3461 loss: tensor(0.7502)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3462 loss: tensor(0.7502)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3463 loss: tensor(0.7502)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3464 loss: tensor(0.7502)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3465 loss: tensor(0.7502)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3466 loss: tensor(0.7502)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3467 loss: tensor(0.7502)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3468 loss: tensor(0.7501)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3469 loss: tensor(0.7501)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3470 loss: tensor(0.7501)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3471 loss: tensor(0.7501)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3472 loss: tensor(0.7501)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3473 loss: tensor(0.7501)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3474 loss: tensor(0.7501)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3475 loss: tensor(0.7501)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3476 loss: tensor(0.7500)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3477 loss: tensor(0.7500)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3478 loss: tensor(0.7500)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3479 loss: tensor(0.7500)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3480 loss: tensor(0.7500)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3481 loss: tensor(0.7500)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3482 loss: tensor(0.7500)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3483 loss: tensor(0.7500)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3484 loss: tensor(0.7500)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3485 loss: tensor(0.7499)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3486 loss: tensor(0.7499)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3487 loss: tensor(0.7499)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3488 loss: tensor(0.7499)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3489 loss: tensor(0.7499)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3490 loss: tensor(0.7499)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3491 loss: tensor(0.7499)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3492 loss: tensor(0.7499)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3493 loss: tensor(0.7499)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3494 loss: tensor(0.7498)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3495 loss: tensor(0.7498)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3496 loss: tensor(0.7498)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3497 loss: tensor(0.7498)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3498 loss: tensor(0.7498)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3499 loss: tensor(0.7498)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3500 loss: tensor(0.7498)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3501 loss: tensor(0.7498)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3502 loss: tensor(0.7498)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3503 loss: tensor(0.7497)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3504 loss: tensor(0.7497)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3505 loss: tensor(0.7497)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3506 loss: tensor(0.7497)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3507 loss: tensor(0.7497)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3508 loss: tensor(0.7497)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3509 loss: tensor(0.7497)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3510 loss: tensor(0.7497)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3511 loss: tensor(0.7497)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3512 loss: tensor(0.7496)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3513 loss: tensor(0.7496)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3514 loss: tensor(0.7496)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3515 loss: tensor(0.7496)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3516 loss: tensor(0.7496)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3517 loss: tensor(0.7496)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3518 loss: tensor(0.7496)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3519 loss: tensor(0.7496)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3520 loss: tensor(0.7496)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3521 loss: tensor(0.7495)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3522 loss: tensor(0.7495)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3523 loss: tensor(0.7495)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3524 loss: tensor(0.7495)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3525 loss: tensor(0.7495)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3526 loss: tensor(0.7495)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3527 loss: tensor(0.7495)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3528 loss: tensor(0.7495)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3529 loss: tensor(0.7495)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3530 loss: tensor(0.7494)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3531 loss: tensor(0.7494)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3532 loss: tensor(0.7494)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3533 loss: tensor(0.7494)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3534 loss: tensor(0.7494)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3535 loss: tensor(0.7494)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3536 loss: tensor(0.7494)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3537 loss: tensor(0.7494)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3538 loss: tensor(0.7494)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3539 loss: tensor(0.7493)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3540 loss: tensor(0.7493)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3541 loss: tensor(0.7493)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3542 loss: tensor(0.7493)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3543 loss: tensor(0.7493)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3544 loss: tensor(0.7493)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3545 loss: tensor(0.7493)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3546 loss: tensor(0.7493)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3547 loss: tensor(0.7493)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3548 loss: tensor(0.7492)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3549 loss: tensor(0.7492)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3550 loss: tensor(0.7492)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3551 loss: tensor(0.7492)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3552 loss: tensor(0.7492)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3553 loss: tensor(0.7492)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3554 loss: tensor(0.7492)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3555 loss: tensor(0.7492)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3556 loss: tensor(0.7492)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3557 loss: tensor(0.7491)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3558 loss: tensor(0.7491)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3559 loss: tensor(0.7491)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3560 loss: tensor(0.7491)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3561 loss: tensor(0.7491)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3562 loss: tensor(0.7491)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3563 loss: tensor(0.7491)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3564 loss: tensor(0.7491)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3565 loss: tensor(0.7491)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3566 loss: tensor(0.7490)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3567 loss: tensor(0.7490)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3568 loss: tensor(0.7490)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3569 loss: tensor(0.7490)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3570 loss: tensor(0.7490)\n",
      "train : 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.5900954111767378\n",
      "3571 loss: tensor(0.7490)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3572 loss: tensor(0.7490)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3573 loss: tensor(0.7490)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3574 loss: tensor(0.7490)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3575 loss: tensor(0.7489)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3576 loss: tensor(0.7489)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3577 loss: tensor(0.7489)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3578 loss: tensor(0.7489)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3579 loss: tensor(0.7489)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3580 loss: tensor(0.7489)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3581 loss: tensor(0.7489)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3582 loss: tensor(0.7489)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3583 loss: tensor(0.7489)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3584 loss: tensor(0.7488)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3585 loss: tensor(0.7488)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3586 loss: tensor(0.7488)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3587 loss: tensor(0.7488)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3588 loss: tensor(0.7488)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3589 loss: tensor(0.7488)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3590 loss: tensor(0.7488)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3591 loss: tensor(0.7488)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3592 loss: tensor(0.7488)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3593 loss: tensor(0.7488)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3594 loss: tensor(0.7487)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3595 loss: tensor(0.7487)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3596 loss: tensor(0.7487)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3597 loss: tensor(0.7487)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3598 loss: tensor(0.7487)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3599 loss: tensor(0.7487)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3600 loss: tensor(0.7487)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3601 loss: tensor(0.7487)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3602 loss: tensor(0.7487)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3603 loss: tensor(0.7486)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3604 loss: tensor(0.7486)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3605 loss: tensor(0.7486)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3606 loss: tensor(0.7486)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3607 loss: tensor(0.7486)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3608 loss: tensor(0.7486)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3609 loss: tensor(0.7486)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3610 loss: tensor(0.7486)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3611 loss: tensor(0.7486)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3612 loss: tensor(0.7485)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3613 loss: tensor(0.7485)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3614 loss: tensor(0.7485)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3615 loss: tensor(0.7485)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3616 loss: tensor(0.7485)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3617 loss: tensor(0.7485)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3618 loss: tensor(0.7485)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3619 loss: tensor(0.7485)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3620 loss: tensor(0.7485)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3621 loss: tensor(0.7484)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3622 loss: tensor(0.7484)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3623 loss: tensor(0.7484)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3624 loss: tensor(0.7484)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3625 loss: tensor(0.7484)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3626 loss: tensor(0.7484)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3627 loss: tensor(0.7484)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3628 loss: tensor(0.7484)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3629 loss: tensor(0.7484)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3630 loss: tensor(0.7483)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3631 loss: tensor(0.7483)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3632 loss: tensor(0.7483)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3633 loss: tensor(0.7483)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3634 loss: tensor(0.7483)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3635 loss: tensor(0.7483)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3636 loss: tensor(0.7483)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3637 loss: tensor(0.7483)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3638 loss: tensor(0.7483)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3639 loss: tensor(0.7483)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3640 loss: tensor(0.7482)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3641 loss: tensor(0.7482)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3642 loss: tensor(0.7482)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3643 loss: tensor(0.7482)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3644 loss: tensor(0.7482)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3645 loss: tensor(0.7482)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3646 loss: tensor(0.7482)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3647 loss: tensor(0.7482)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3648 loss: tensor(0.7482)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3649 loss: tensor(0.7481)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3650 loss: tensor(0.7481)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3651 loss: tensor(0.7481)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3652 loss: tensor(0.7481)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3653 loss: tensor(0.7481)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3654 loss: tensor(0.7481)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3655 loss: tensor(0.7481)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3656 loss: tensor(0.7481)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3657 loss: tensor(0.7481)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3658 loss: tensor(0.7480)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3659 loss: tensor(0.7480)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3660 loss: tensor(0.7480)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3661 loss: tensor(0.7480)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3662 loss: tensor(0.7480)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3663 loss: tensor(0.7480)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3664 loss: tensor(0.7480)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3665 loss: tensor(0.7480)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3666 loss: tensor(0.7480)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3667 loss: tensor(0.7480)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3668 loss: tensor(0.7479)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3669 loss: tensor(0.7479)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3670 loss: tensor(0.7479)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3671 loss: tensor(0.7479)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3672 loss: tensor(0.7479)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3673 loss: tensor(0.7479)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3674 loss: tensor(0.7479)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3675 loss: tensor(0.7479)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3676 loss: tensor(0.7479)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3677 loss: tensor(0.7478)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3678 loss: tensor(0.7478)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3679 loss: tensor(0.7478)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3680 loss: tensor(0.7478)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3681 loss: tensor(0.7478)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3682 loss: tensor(0.7478)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3683 loss: tensor(0.7478)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3684 loss: tensor(0.7478)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3685 loss: tensor(0.7478)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3686 loss: tensor(0.7477)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3687 loss: tensor(0.7477)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3688 loss: tensor(0.7477)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3689 loss: tensor(0.7477)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3690 loss: tensor(0.7477)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3691 loss: tensor(0.7477)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3692 loss: tensor(0.7477)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3693 loss: tensor(0.7477)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3694 loss: tensor(0.7477)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3695 loss: tensor(0.7477)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3696 loss: tensor(0.7476)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3697 loss: tensor(0.7476)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3698 loss: tensor(0.7476)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3699 loss: tensor(0.7476)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3700 loss: tensor(0.7476)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3701 loss: tensor(0.7476)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3702 loss: tensor(0.7476)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3703 loss: tensor(0.7476)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3704 loss: tensor(0.7476)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3705 loss: tensor(0.7475)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3706 loss: tensor(0.7475)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3707 loss: tensor(0.7475)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3708 loss: tensor(0.7475)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3709 loss: tensor(0.7475)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3710 loss: tensor(0.7475)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3711 loss: tensor(0.7475)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3712 loss: tensor(0.7475)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3713 loss: tensor(0.7475)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3714 loss: tensor(0.7475)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3715 loss: tensor(0.7474)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3716 loss: tensor(0.7474)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3717 loss: tensor(0.7474)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3718 loss: tensor(0.7474)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3719 loss: tensor(0.7474)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3720 loss: tensor(0.7474)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3721 loss: tensor(0.7474)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3722 loss: tensor(0.7474)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3723 loss: tensor(0.7474)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3724 loss: tensor(0.7473)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3725 loss: tensor(0.7473)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3726 loss: tensor(0.7473)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3727 loss: tensor(0.7473)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3728 loss: tensor(0.7473)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3729 loss: tensor(0.7473)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3730 loss: tensor(0.7473)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3731 loss: tensor(0.7473)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3732 loss: tensor(0.7473)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3733 loss: tensor(0.7473)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3734 loss: tensor(0.7472)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3735 loss: tensor(0.7472)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3736 loss: tensor(0.7472)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3737 loss: tensor(0.7472)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3738 loss: tensor(0.7472)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3739 loss: tensor(0.7472)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "3740 loss: tensor(0.7472)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3741 loss: tensor(0.7472)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "3742 loss: tensor(0.7472)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3743 loss: tensor(0.7471)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3744 loss: tensor(0.7471)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3745 loss: tensor(0.7471)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3746 loss: tensor(0.7471)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3747 loss: tensor(0.7471)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3748 loss: tensor(0.7471)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3749 loss: tensor(0.7471)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3750 loss: tensor(0.7471)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3751 loss: tensor(0.7471)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3752 loss: tensor(0.7471)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3753 loss: tensor(0.7470)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3754 loss: tensor(0.7470)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3755 loss: tensor(0.7470)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3756 loss: tensor(0.7470)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3757 loss: tensor(0.7470)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3758 loss: tensor(0.7470)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3759 loss: tensor(0.7470)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3760 loss: tensor(0.7470)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3761 loss: tensor(0.7470)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3762 loss: tensor(0.7469)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3763 loss: tensor(0.7469)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3764 loss: tensor(0.7469)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3765 loss: tensor(0.7469)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3766 loss: tensor(0.7469)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3767 loss: tensor(0.7469)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3768 loss: tensor(0.7469)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3769 loss: tensor(0.7469)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3770 loss: tensor(0.7469)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3771 loss: tensor(0.7469)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3772 loss: tensor(0.7468)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3773 loss: tensor(0.7468)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3774 loss: tensor(0.7468)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3775 loss: tensor(0.7468)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3776 loss: tensor(0.7468)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3777 loss: tensor(0.7468)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3778 loss: tensor(0.7468)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3779 loss: tensor(0.7468)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3780 loss: tensor(0.7468)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3781 loss: tensor(0.7467)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3782 loss: tensor(0.7467)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3783 loss: tensor(0.7467)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3784 loss: tensor(0.7467)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3785 loss: tensor(0.7467)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3786 loss: tensor(0.7467)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3787 loss: tensor(0.7467)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3788 loss: tensor(0.7467)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3789 loss: tensor(0.7467)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3790 loss: tensor(0.7467)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3791 loss: tensor(0.7466)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3792 loss: tensor(0.7466)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3793 loss: tensor(0.7466)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3794 loss: tensor(0.7466)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3795 loss: tensor(0.7466)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3796 loss: tensor(0.7466)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3797 loss: tensor(0.7466)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3798 loss: tensor(0.7466)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3799 loss: tensor(0.7466)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3800 loss: tensor(0.7465)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3801 loss: tensor(0.7465)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3802 loss: tensor(0.7465)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3803 loss: tensor(0.7465)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3804 loss: tensor(0.7465)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3805 loss: tensor(0.7465)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3806 loss: tensor(0.7465)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3807 loss: tensor(0.7465)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3808 loss: tensor(0.7465)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3809 loss: tensor(0.7465)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3810 loss: tensor(0.7464)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3811 loss: tensor(0.7464)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3812 loss: tensor(0.7464)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3813 loss: tensor(0.7464)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3814 loss: tensor(0.7464)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3815 loss: tensor(0.7464)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3816 loss: tensor(0.7464)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3817 loss: tensor(0.7464)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3818 loss: tensor(0.7464)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3819 loss: tensor(0.7464)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3820 loss: tensor(0.7463)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3821 loss: tensor(0.7463)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3822 loss: tensor(0.7463)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3823 loss: tensor(0.7463)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3824 loss: tensor(0.7463)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3825 loss: tensor(0.7463)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3826 loss: tensor(0.7463)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3827 loss: tensor(0.7463)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3828 loss: tensor(0.7463)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3829 loss: tensor(0.7462)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3830 loss: tensor(0.7462)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3831 loss: tensor(0.7462)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3832 loss: tensor(0.7462)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3833 loss: tensor(0.7462)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3834 loss: tensor(0.7462)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3835 loss: tensor(0.7462)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3836 loss: tensor(0.7462)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3837 loss: tensor(0.7462)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3838 loss: tensor(0.7462)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3839 loss: tensor(0.7461)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3840 loss: tensor(0.7461)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3841 loss: tensor(0.7461)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3842 loss: tensor(0.7461)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3843 loss: tensor(0.7461)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3844 loss: tensor(0.7461)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3845 loss: tensor(0.7461)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3846 loss: tensor(0.7461)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3847 loss: tensor(0.7461)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3848 loss: tensor(0.7461)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3849 loss: tensor(0.7460)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3850 loss: tensor(0.7460)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3851 loss: tensor(0.7460)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3852 loss: tensor(0.7460)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3853 loss: tensor(0.7460)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3854 loss: tensor(0.7460)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3855 loss: tensor(0.7460)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3856 loss: tensor(0.7460)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3857 loss: tensor(0.7460)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3858 loss: tensor(0.7459)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3859 loss: tensor(0.7459)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3860 loss: tensor(0.7459)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3861 loss: tensor(0.7459)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3862 loss: tensor(0.7459)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3863 loss: tensor(0.7459)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3864 loss: tensor(0.7459)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3865 loss: tensor(0.7459)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3866 loss: tensor(0.7459)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3867 loss: tensor(0.7459)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3868 loss: tensor(0.7458)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3869 loss: tensor(0.7458)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3870 loss: tensor(0.7458)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3871 loss: tensor(0.7458)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3872 loss: tensor(0.7458)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3873 loss: tensor(0.7458)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3874 loss: tensor(0.7458)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3875 loss: tensor(0.7458)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3876 loss: tensor(0.7458)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3877 loss: tensor(0.7458)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3878 loss: tensor(0.7457)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3879 loss: tensor(0.7457)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3880 loss: tensor(0.7457)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3881 loss: tensor(0.7457)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3882 loss: tensor(0.7457)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3883 loss: tensor(0.7457)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3884 loss: tensor(0.7457)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3885 loss: tensor(0.7457)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3886 loss: tensor(0.7457)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3887 loss: tensor(0.7457)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3888 loss: tensor(0.7456)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3889 loss: tensor(0.7456)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3890 loss: tensor(0.7456)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3891 loss: tensor(0.7456)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3892 loss: tensor(0.7456)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3893 loss: tensor(0.7456)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3894 loss: tensor(0.7456)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3895 loss: tensor(0.7456)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3896 loss: tensor(0.7456)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3897 loss: tensor(0.7455)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3898 loss: tensor(0.7455)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3899 loss: tensor(0.7455)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3900 loss: tensor(0.7455)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3901 loss: tensor(0.7455)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3902 loss: tensor(0.7455)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3903 loss: tensor(0.7455)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3904 loss: tensor(0.7455)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3905 loss: tensor(0.7455)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3906 loss: tensor(0.7455)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3907 loss: tensor(0.7454)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3908 loss: tensor(0.7454)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3909 loss: tensor(0.7454)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3910 loss: tensor(0.7454)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3911 loss: tensor(0.7454)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3912 loss: tensor(0.7454)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3913 loss: tensor(0.7454)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3914 loss: tensor(0.7454)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3915 loss: tensor(0.7454)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3916 loss: tensor(0.7454)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3917 loss: tensor(0.7453)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3918 loss: tensor(0.7453)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3919 loss: tensor(0.7453)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3920 loss: tensor(0.7453)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3921 loss: tensor(0.7453)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3922 loss: tensor(0.7453)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3923 loss: tensor(0.7453)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3924 loss: tensor(0.7453)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3925 loss: tensor(0.7453)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3926 loss: tensor(0.7453)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3927 loss: tensor(0.7452)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3928 loss: tensor(0.7452)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3929 loss: tensor(0.7452)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3930 loss: tensor(0.7452)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3931 loss: tensor(0.7452)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3932 loss: tensor(0.7452)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3933 loss: tensor(0.7452)\n",
      "train : 1.0\n",
      "test : 0.5897319400272604\n",
      "3934 loss: tensor(0.7452)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3935 loss: tensor(0.7452)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3936 loss: tensor(0.7452)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3937 loss: tensor(0.7451)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3938 loss: tensor(0.7451)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3939 loss: tensor(0.7451)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3940 loss: tensor(0.7451)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3941 loss: tensor(0.7451)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3942 loss: tensor(0.7451)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3943 loss: tensor(0.7451)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3944 loss: tensor(0.7451)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3945 loss: tensor(0.7451)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3946 loss: tensor(0.7451)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3947 loss: tensor(0.7450)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3948 loss: tensor(0.7450)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3949 loss: tensor(0.7450)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3950 loss: tensor(0.7450)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3951 loss: tensor(0.7450)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3952 loss: tensor(0.7450)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3953 loss: tensor(0.7450)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3954 loss: tensor(0.7450)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3955 loss: tensor(0.7450)\n",
      "train : 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.5900954111767378\n",
      "3956 loss: tensor(0.7450)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "3957 loss: tensor(0.7449)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3958 loss: tensor(0.7449)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3959 loss: tensor(0.7449)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3960 loss: tensor(0.7449)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3961 loss: tensor(0.7449)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3962 loss: tensor(0.7449)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3963 loss: tensor(0.7449)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3964 loss: tensor(0.7449)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3965 loss: tensor(0.7449)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3966 loss: tensor(0.7449)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3967 loss: tensor(0.7448)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3968 loss: tensor(0.7448)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3969 loss: tensor(0.7448)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3970 loss: tensor(0.7448)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3971 loss: tensor(0.7448)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3972 loss: tensor(0.7448)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3973 loss: tensor(0.7448)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3974 loss: tensor(0.7448)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3975 loss: tensor(0.7448)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3976 loss: tensor(0.7447)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3977 loss: tensor(0.7447)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3978 loss: tensor(0.7447)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3979 loss: tensor(0.7447)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3980 loss: tensor(0.7447)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3981 loss: tensor(0.7447)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3982 loss: tensor(0.7447)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3983 loss: tensor(0.7447)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3984 loss: tensor(0.7447)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3985 loss: tensor(0.7447)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3986 loss: tensor(0.7446)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3987 loss: tensor(0.7446)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3988 loss: tensor(0.7446)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3989 loss: tensor(0.7446)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3990 loss: tensor(0.7446)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3991 loss: tensor(0.7446)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3992 loss: tensor(0.7446)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "3993 loss: tensor(0.7446)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3994 loss: tensor(0.7446)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "3995 loss: tensor(0.7446)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3996 loss: tensor(0.7445)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3997 loss: tensor(0.7445)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "3998 loss: tensor(0.7445)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "3999 loss: tensor(0.7445)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "4000 loss: tensor(0.7445)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4001 loss: tensor(0.7445)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4002 loss: tensor(0.7445)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4003 loss: tensor(0.7445)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "4004 loss: tensor(0.7445)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4005 loss: tensor(0.7445)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "4006 loss: tensor(0.7444)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4007 loss: tensor(0.7444)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4008 loss: tensor(0.7444)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4009 loss: tensor(0.7444)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4010 loss: tensor(0.7444)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4011 loss: tensor(0.7444)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4012 loss: tensor(0.7444)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4013 loss: tensor(0.7444)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4014 loss: tensor(0.7444)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4015 loss: tensor(0.7444)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4016 loss: tensor(0.7443)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4017 loss: tensor(0.7443)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4018 loss: tensor(0.7443)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4019 loss: tensor(0.7443)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4020 loss: tensor(0.7443)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4021 loss: tensor(0.7443)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4022 loss: tensor(0.7443)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4023 loss: tensor(0.7443)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4024 loss: tensor(0.7443)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4025 loss: tensor(0.7443)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4026 loss: tensor(0.7442)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4027 loss: tensor(0.7442)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4028 loss: tensor(0.7442)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4029 loss: tensor(0.7442)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4030 loss: tensor(0.7442)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4031 loss: tensor(0.7442)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4032 loss: tensor(0.7442)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4033 loss: tensor(0.7442)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4034 loss: tensor(0.7442)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4035 loss: tensor(0.7442)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4036 loss: tensor(0.7441)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4037 loss: tensor(0.7441)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4038 loss: tensor(0.7441)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4039 loss: tensor(0.7441)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4040 loss: tensor(0.7441)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4041 loss: tensor(0.7441)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4042 loss: tensor(0.7441)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4043 loss: tensor(0.7441)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4044 loss: tensor(0.7441)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4045 loss: tensor(0.7441)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4046 loss: tensor(0.7440)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4047 loss: tensor(0.7440)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4048 loss: tensor(0.7440)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4049 loss: tensor(0.7440)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4050 loss: tensor(0.7440)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4051 loss: tensor(0.7440)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4052 loss: tensor(0.7440)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4053 loss: tensor(0.7440)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4054 loss: tensor(0.7440)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4055 loss: tensor(0.7440)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4056 loss: tensor(0.7439)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4057 loss: tensor(0.7439)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4058 loss: tensor(0.7439)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4059 loss: tensor(0.7439)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4060 loss: tensor(0.7439)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4061 loss: tensor(0.7439)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4062 loss: tensor(0.7439)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4063 loss: tensor(0.7439)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4064 loss: tensor(0.7439)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4065 loss: tensor(0.7439)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "4066 loss: tensor(0.7438)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4067 loss: tensor(0.7438)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "4068 loss: tensor(0.7438)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4069 loss: tensor(0.7438)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "4070 loss: tensor(0.7438)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4071 loss: tensor(0.7438)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "4072 loss: tensor(0.7438)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4073 loss: tensor(0.7438)\n",
      "train : 1.0\n",
      "test : 0.5898228078146297\n",
      "4074 loss: tensor(0.7438)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4075 loss: tensor(0.7438)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "4076 loss: tensor(0.7438)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4077 loss: tensor(0.7437)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "4078 loss: tensor(0.7437)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4079 loss: tensor(0.7437)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4080 loss: tensor(0.7437)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4081 loss: tensor(0.7437)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "4082 loss: tensor(0.7437)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4083 loss: tensor(0.7437)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4084 loss: tensor(0.7437)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4085 loss: tensor(0.7437)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4086 loss: tensor(0.7437)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4087 loss: tensor(0.7436)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4088 loss: tensor(0.7436)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "4089 loss: tensor(0.7436)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4090 loss: tensor(0.7436)\n",
      "train : 1.0\n",
      "test : 0.5899136756019991\n",
      "4091 loss: tensor(0.7436)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4092 loss: tensor(0.7436)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4093 loss: tensor(0.7436)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4094 loss: tensor(0.7436)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4095 loss: tensor(0.7436)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4096 loss: tensor(0.7436)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4097 loss: tensor(0.7435)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4098 loss: tensor(0.7435)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4099 loss: tensor(0.7435)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4100 loss: tensor(0.7435)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4101 loss: tensor(0.7435)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4102 loss: tensor(0.7435)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4103 loss: tensor(0.7435)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4104 loss: tensor(0.7435)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4105 loss: tensor(0.7435)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4106 loss: tensor(0.7435)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4107 loss: tensor(0.7434)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4108 loss: tensor(0.7434)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4109 loss: tensor(0.7434)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4110 loss: tensor(0.7434)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4111 loss: tensor(0.7434)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4112 loss: tensor(0.7434)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4113 loss: tensor(0.7434)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4114 loss: tensor(0.7434)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4115 loss: tensor(0.7434)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4116 loss: tensor(0.7434)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4117 loss: tensor(0.7433)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4118 loss: tensor(0.7433)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4119 loss: tensor(0.7433)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4120 loss: tensor(0.7433)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4121 loss: tensor(0.7433)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4122 loss: tensor(0.7433)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4123 loss: tensor(0.7433)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4124 loss: tensor(0.7433)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4125 loss: tensor(0.7433)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4126 loss: tensor(0.7433)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4127 loss: tensor(0.7432)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4128 loss: tensor(0.7432)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4129 loss: tensor(0.7432)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4130 loss: tensor(0.7432)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4131 loss: tensor(0.7432)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4132 loss: tensor(0.7432)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4133 loss: tensor(0.7432)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4134 loss: tensor(0.7432)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4135 loss: tensor(0.7432)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4136 loss: tensor(0.7432)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4137 loss: tensor(0.7431)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4138 loss: tensor(0.7431)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4139 loss: tensor(0.7431)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4140 loss: tensor(0.7431)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4141 loss: tensor(0.7431)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4142 loss: tensor(0.7431)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4143 loss: tensor(0.7431)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4144 loss: tensor(0.7431)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4145 loss: tensor(0.7431)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4146 loss: tensor(0.7431)\n",
      "train : 1.0\n",
      "test : 0.5900045433893685\n",
      "4147 loss: tensor(0.7431)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4148 loss: tensor(0.7430)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4149 loss: tensor(0.7430)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4150 loss: tensor(0.7430)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4151 loss: tensor(0.7430)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4152 loss: tensor(0.7430)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4153 loss: tensor(0.7430)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4154 loss: tensor(0.7430)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4155 loss: tensor(0.7430)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4156 loss: tensor(0.7430)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4157 loss: tensor(0.7430)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4158 loss: tensor(0.7430)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4159 loss: tensor(0.7429)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4160 loss: tensor(0.7429)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4161 loss: tensor(0.7429)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4162 loss: tensor(0.7429)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4163 loss: tensor(0.7429)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4164 loss: tensor(0.7429)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4165 loss: tensor(0.7429)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4166 loss: tensor(0.7429)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4167 loss: tensor(0.7429)\n",
      "train : 1.0\n",
      "test : 0.5900954111767378\n",
      "4168 loss: tensor(0.7429)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4169 loss: tensor(0.7429)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4170 loss: tensor(0.7428)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4171 loss: tensor(0.7428)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4172 loss: tensor(0.7428)\n",
      "train : 1.0\n",
      "test : 0.5901862789641072\n",
      "4173 loss: tensor(0.7428)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "4174 loss: tensor(0.7428)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "4175 loss: tensor(0.7428)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "4176 loss: tensor(0.7428)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "4177 loss: tensor(0.7428)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "4178 loss: tensor(0.7428)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "4179 loss: tensor(0.7428)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "4180 loss: tensor(0.7428)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "4181 loss: tensor(0.7428)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "4182 loss: tensor(0.7427)\n",
      "train : 1.0\n",
      "test : 0.5902771467514766\n",
      "4183 loss: tensor(0.7427)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "4184 loss: tensor(0.7427)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "4185 loss: tensor(0.7427)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "4186 loss: tensor(0.7427)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "4187 loss: tensor(0.7427)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "4188 loss: tensor(0.7427)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "4189 loss: tensor(0.7427)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "4190 loss: tensor(0.7427)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "4191 loss: tensor(0.7427)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "4192 loss: tensor(0.7427)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "4193 loss: tensor(0.7426)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "4194 loss: tensor(0.7426)\n",
      "train : 1.0\n",
      "test : 0.5905497501135847\n",
      "4195 loss: tensor(0.7426)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "4196 loss: tensor(0.7426)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "4197 loss: tensor(0.7426)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "4198 loss: tensor(0.7426)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "4199 loss: tensor(0.7426)\n",
      "train : 1.0\n",
      "test : 0.5904588823262154\n",
      "4200 loss: tensor(0.7426)\n",
      "train : 1.0\n",
      "test : 0.590368014538846\n",
      "4201 loss: tensor(0.7426)\n",
      "train : 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a73c5d4bb1bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m9981\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m9981\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mpred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-450c4a706a36>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.01)\n",
    " \n",
    "epochs = 10000\n",
    " \n",
    "px = []\n",
    "py = []\n",
    "t1 = []\n",
    "t2 = []\n",
    "for i in range(epochs):\n",
    "    predict = net(x)\n",
    "#     print(predict)\n",
    "    loss = F.nll_loss(predict,y.long())  # 输出层 用了log_softmax 则需要用这个误差函数\n",
    "    lambda1 = torch.tensor(0.01)\n",
    "    l2_reg = torch.tensor(0.)\n",
    "    for param in net.parameters():\n",
    "        l2_reg += torch.norm(param)\n",
    "    loss += lambda1 * l2_reg\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(i,\"loss:\",loss.data[0])\n",
    "    px.append(i)\n",
    "    py.append(loss.data[0])\n",
    "    output = net(x)\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    s = 0\n",
    "    for i in range(9981):\n",
    "        if float(pred[i].item()) == y[i].item():\n",
    "            s+=1\n",
    "    t1.append(s/9981)\n",
    "    print(\"train :\",s/9981)\n",
    "    output1 = net(x_test)\n",
    "    pred1 = output1.data.max(1, keepdim=True)[1]\n",
    "    s = 0\n",
    "    for i in range(len(test)):\n",
    "        if float(pred1[i].item()) == y_test[i].item():\n",
    "            s+=1\n",
    "    t2.append(s/11005)\n",
    "    print(\"test :\",s/11005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "# plt.plot(px,py,label=\"loss\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(t1,'-b',label=\"accuracy of train\")\n",
    "plt.plot(t2,'-r',label=\"accuracy of test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(px,py,label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
